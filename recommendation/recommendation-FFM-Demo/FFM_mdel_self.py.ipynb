{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "input_x_size=20\n",
    "field_size=2\n",
    "\n",
    "##s 此处维度为3 ？ \n",
    "vector_dimension=3\n",
    "\n",
    "total_plan_train_steps=1000\n",
    "# 使用SGD 随机梯度下降法 ，每一个样本进行依次梯度下降，更新参数\n",
    "batch_size=1\n",
    "all_data_size=1000\n",
    "\n",
    "lr=0.01\n",
    "MODEL_SAVE_PATH=\"TFModel_self\"\n",
    "MODEL_NAME= \"FFM\"\n",
    "\n",
    "def createTwoDimensionWeight(input_x_size,field_size,vector_dimension):\n",
    "    weights=tf.truncated_normal([input_x_size,field_size,vector_dimension])\n",
    "    tf_weights=tf.Variable(weights)\n",
    "    return tf_weights\n",
    "\n",
    "def createOneDimensionWeight(input_x_size):\n",
    "    weights=tf.truncated_normal([input_x_size])\n",
    "    tf_weights=tf.Variable(weights)\n",
    "    return tf_weights\n",
    "\n",
    "def createZeroDimensionWeight():\n",
    "    weights=tf.truncated_normal([1])\n",
    "    tf_weights=tf.Variable(weights)\n",
    "    return tf_weights\n",
    "\n",
    "def inference(input_x,input_x_field,zeroWeights,oneDimWeights,thirdWeight):\n",
    "    \"\"\" 计算回归模型输出的值\"\"\"\n",
    "    secondValue=tf.reduce_sum(tf.multiply(oneDimWeights,input_x,name='secondValue'))\n",
    "    firstTwoValue=tf.add(zeroWeights,secondValue,name=\"firstTwoValue\")\n",
    "    \n",
    "    thirdValue=tf.Variable(0.0,dtype=tf.float32)\n",
    "    input_shape=input_x_size\n",
    "    \n",
    "    for i in range(input_shape):\n",
    "        featureIndex1=i\n",
    "        fieldIndex1 = int(input_x_field[i])\n",
    "        for j in range(i+1,input_shape):\n",
    "            featureIndex2=j\n",
    "            fieldIndex2=int(input_x_field[j])\n",
    "            ## 下面这个地方具体是指  ？\n",
    "            vectorLeft=tf.convert_to_tensor([[featureIndex1,fieldIndex2,i] for i in range(vector_dimension)])\n",
    "            weightLeft=tf.gather_nd(thirdWeight,vectorLeft)\n",
    "            #  gather_nd(params, indices, name=None)实现了根据指定的 参数 indices 来提取params 的元素重建出一个tensor,\n",
    "            weightLeftAfterCut=tf.squeeze(weightLeft)\n",
    "            # tf.squeeze() 给定张量输入，此操作返回相同类型的张量，并删除所有尺寸为1的尺寸。 如果不想删除所有尺寸1尺寸，可以通过指定squeeze_dims来删除特定尺寸1尺寸。\n",
    "            \n",
    "            vectorRight=tf.convert_to_tensor([[featureIndex2,fieldIndex1,i] for i in range(vector_dimension)])\n",
    "            weightRight=tf.gather_nd(thirdWeight,vectorRight)\n",
    "            weightRightAfterCut=tf.squeeze(weightRight)\n",
    "            \n",
    "            tempValue=tf.reduce_sum(tf.multiply(weightLeftAfterCut,weightRightAfterCut))\n",
    "            \n",
    "## todo  带请教，上面的vectorLeft、weightLeft、weightRightAfterCut、tempValue 等都是对应FFM公式里的那部分？\n",
    "            \n",
    "            indices2=[i]\n",
    "            indices3=[j]\n",
    "            \n",
    "            xi=tf.squeeze(tf.gather_nd(input_x,indices2))\n",
    "            xj=tf.squeeze(tf.gather_nd(input_x,indices3))\n",
    "            \n",
    "            product=tf.reduce_sum(tf.multiply(xi,xj))\n",
    "            \n",
    "            secondItemVal=tf.multiply(tempValue,product)\n",
    "            \n",
    "            tf.assign(thirdValue,tf.add(thirdValue,secondItemVal))\n",
    "            # tf.assign(A, new_number): 这个函数的功能主要是把A的值变为new_number\n",
    "            \n",
    "    return tf.add(firstTwoValue,thirdValue)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 生成数据\n",
    "def gen_data():\n",
    "    labels=[-1,1]\n",
    "    y=[np.random.choice(labels,1)[0] for _ in range(all_data_size)]\n",
    "    # 上面np.random.choice(a, size=3, replace=False, p=None) 的含义是随机从a只能跟选择出size个，如果P没有指定的时候相当于是一致分布。\n",
    "    x_field=[i//10 for i in range(input_x_size)]\n",
    "    # input_x_size = 20 ,数据涉及20维特征，前10维特征是一个field，后十维是一个field\n",
    "    x =np.random.randint(0,2,size=(all_data_size,input_x_size))\n",
    "    return x,y ,x_field\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [1.1331886 1.1331886 1.1331886 0.7741379 1.1331886 0.7741379 0.7741379\n",
      " 1.1331886 0.7741379 1.1331886 1.1331886 1.1331886 0.7741379 1.1331886\n",
      " 1.1331886 1.1331886 1.1331886 0.7741379 1.1331886 1.1331886]\n",
      " After 0 training  step(s), loss on training batch is [0.64809656 0.64809656 0.64809656 0.77429307 0.64809656 0.77429307\n",
      " 0.77429307 0.77429307 0.77429307 0.64809656 0.64809656 0.64809656\n",
      " 0.64809656 0.64809656 0.64809656 0.64809656 0.77429307 0.64809656\n",
      " 0.77429307 0.77429307]\n",
      " After 0 training  step(s), loss on training batch is [0.33300203 0.7743935  0.7743935  0.33300203 0.7743935  0.7743935\n",
      " 0.7743935  0.7743935  0.7743935  0.33300203 0.7743935  0.33300203\n",
      " 0.7743935  0.33300203 0.33300203 0.7743935  0.7743935  0.33300203\n",
      " 0.7743935  0.7743935 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7743974 0.7743974 0.7743974 1.4744775 0.7743974 0.7743974 0.7743974\n",
      " 0.7743974 1.4744775 0.7743974 0.7743974 0.7743974 0.7743974 0.7743974\n",
      " 1.4744775 1.4744775 0.7743974 0.7743974 0.7743974 1.4744775]\n",
      " After 0 training  step(s), loss on training batch is [0.774322   0.39045316 0.39045316 0.39045316 0.774322   0.39045316\n",
      " 0.774322   0.39045316 0.39045316 0.774322   0.774322   0.774322\n",
      " 0.774322   0.774322   0.39045316 0.774322   0.3904532  0.3904532\n",
      " 0.774322   0.3904532 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7743606 0.7743606 0.6228699 0.6228699 0.7743606 0.6228699 0.7743606\n",
      " 0.7743606 0.7743606 0.6228699 0.7743606 0.6228699 0.7743606 0.6228699\n",
      " 0.6228699 0.6228699 0.6228699 0.6228699 0.6228699 0.7743606]\n",
      " After 0 training  step(s), loss on training batch is [0.77441794 0.77441794 0.36292928 0.36292928 0.77441794 0.77441794\n",
      " 0.36292928 0.36292928 0.77441794 0.77441794 0.77441794 0.36292928\n",
      " 0.77441794 0.77441794 0.36292928 0.77441794 0.36292928 0.77441794\n",
      " 0.36292928 0.77441794]\n",
      " After 0 training  step(s), loss on training batch is [0.8020311 0.8020311 0.8020311 0.7744258 0.8020311 0.8020311 0.7744258\n",
      " 0.8020311 0.7744258 0.7744258 0.8020311 0.8020311 0.7744258 0.7744258\n",
      " 0.7744258 0.8020311 0.7744258 0.7744258 0.7744258 0.7744258]\n",
      " After 0 training  step(s), loss on training batch is [0.11821563 0.11821563 0.11821563 0.7744318  0.11821563 0.11821563\n",
      " 0.11821563 0.7744318  0.11821563 0.7744318  0.7744318  0.7744318\n",
      " 0.11821563 0.7744318  0.7744318  0.7744318  0.11821564 0.11821564\n",
      " 0.7744318  0.11821564]\n",
      " After 0 training  step(s), loss on training batch is [0.3407408 0.3407408 0.7743973 0.3407408 0.7743973 0.7743973 0.7743973\n",
      " 0.7743973 0.7743973 0.3407408 0.7743973 0.7743973 0.3407408 0.7743973\n",
      " 0.3407408 0.3407408 0.3407408 0.7743973 0.7743973 0.7743973]\n",
      " After 0 training  step(s), loss on training batch is [0.7743983 0.5456055 0.7743983 0.7743983 0.7743983 0.7743983 0.7743983\n",
      " 0.7743983 0.7743983 0.7743983 0.7743983 0.7743983 0.5456055 0.7743983\n",
      " 0.7743983 0.7743983 0.5456055 0.7743983 0.7743983 0.7743983]\n",
      " After 0 training  step(s), loss on training batch is [0.30909485 0.30909485 0.77435607 0.77435607 0.77435607 0.30909485\n",
      " 0.30909485 0.30909485 0.30909485 0.77435607 0.30909485 0.30909485\n",
      " 0.77435607 0.30909485 0.30909485 0.30909485 0.77435607 0.30909485\n",
      " 0.30909485 0.30909485]\n",
      " After 0 training  step(s), loss on training batch is [0.77440625 0.24235539 0.24235539 0.77440625 0.77440625 0.77440625\n",
      " 0.24235539 0.77440625 0.24235539 0.77440625 0.77440625 0.77440625\n",
      " 0.24235539 0.77440625 0.77440625 0.77440625 0.24235539 0.24235539\n",
      " 0.24235539 0.24235539]\n",
      " After 0 training  step(s), loss on training batch is [0.48759422 0.77440023 0.48759422 0.77440023 0.77440023 0.77440023\n",
      " 0.48759422 0.48759422 0.48759422 0.77440023 0.77440023 0.77440023\n",
      " 0.77440023 0.77440023 0.48759422 0.48759422 0.77440023 0.4875942\n",
      " 0.77440023 0.4875942 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7744072 0.7744072 0.648086  0.7744072 0.7744072 0.7744072 0.7744072\n",
      " 0.7744072 0.7744072 0.648086  0.648086  0.7744072 0.648086  0.7744072\n",
      " 0.7744072 0.7744072 0.7744072 0.648086  0.648086  0.648086 ]\n",
      " After 0 training  step(s), loss on training batch is [0.09786277 0.7743839  0.09786277 0.09786277 0.09786277 0.09786277\n",
      " 0.09786277 0.09786277 0.09786277 0.09786277 0.09786277 0.09786277\n",
      " 0.09786277 0.7743839  0.09786277 0.09786277 0.09786277 0.09786277\n",
      " 0.09786277 0.7743839 ]\n",
      " After 0 training  step(s), loss on training batch is [0.08789263 0.77434343 0.08789263 0.77434343 0.08789263 0.08789263\n",
      " 0.77434343 0.77434343 0.77434343 0.08789263 0.08789263 0.77434343\n",
      " 0.77434343 0.08789263 0.08789263 0.77434343 0.08789263 0.08789263\n",
      " 0.77434343 0.77434343]\n",
      " After 0 training  step(s), loss on training batch is [0.11146504 0.7742855  0.11146504 0.11146504 0.11146504 0.7742855\n",
      " 0.7742855  0.11146504 0.7742855  0.7742855  0.7742855  0.7742855\n",
      " 0.11146504 0.7742855  0.11146504 0.11146504 0.11146504 0.7742855\n",
      " 0.11146504 0.7742855 ]\n",
      " After 0 training  step(s), loss on training batch is [0.77424294 0.77424294 0.77424294 0.77424294 0.77424294 0.4599731\n",
      " 0.4599731  0.77424294 0.77424294 0.4599731  0.77424294 0.4599731\n",
      " 0.77424294 0.77424294 0.77424294 0.77424294 0.77424294 0.4599731\n",
      " 0.4599731  0.4599731 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7742262 0.7742262 1.8855642 0.7742262 0.7742262 1.8855642 0.7742262\n",
      " 0.7742262 0.7742262 0.7742262 1.8855642 0.7742262 1.8855642 0.7742262\n",
      " 0.7742262 1.8855642 0.7742262 0.7742262 0.7742262 0.7742262]\n",
      " After 0 training  step(s), loss on training batch is [0.7740506 0.7740506 1.4339116 0.7740506 0.7740506 0.7740506 1.4339116\n",
      " 1.4339116 0.7740506 0.7740506 1.4339116 1.4339116 1.4339116 0.7740506\n",
      " 0.7740506 1.4339116 0.7740506 0.7740506 0.7740506 1.4339116]\n",
      " After 0 training  step(s), loss on training batch is [0.77390224 0.77390224 0.14733002 0.14733002 0.77390224 0.14733002\n",
      " 0.14733002 0.14733002 0.77390224 0.77390224 0.14733002 0.14733002\n",
      " 0.14733002 0.14733002 0.77390224 0.77390224 0.14733002 0.77390224\n",
      " 0.14733002 0.77390224]\n",
      " After 0 training  step(s), loss on training batch is [0.77387595 0.10947348 0.10947348 0.10947348 0.10947348 0.10947348\n",
      " 0.10947348 0.10947348 0.77387595 0.77387595 0.10947348 0.10947348\n",
      " 0.10947348 0.10947348 0.77387595 0.10947348 0.77387595 0.77387595\n",
      " 0.10947348 0.10947348]\n",
      " After 0 training  step(s), loss on training batch is [0.1037    0.7738395 0.7738395 0.1037    0.1037    0.7738395 0.7738395\n",
      " 0.7738395 0.7738395 0.7738395 0.7738395 0.1037    0.1037    0.7738395\n",
      " 0.1037    0.7738395 0.7738395 0.7738395 0.7738395 0.7738395]\n",
      " After 0 training  step(s), loss on training batch is [0.7737851  0.22054714 0.22054714 0.7737851  0.7737851  0.7737851\n",
      " 0.22054714 0.22054714 0.7737851  0.7737851  0.22054714 0.22054714\n",
      " 0.22054714 0.7737851  0.7737851  0.22054714 0.22054714 0.22054714\n",
      " 0.22054714 0.7737851 ]\n",
      " After 0 training  step(s), loss on training batch is [0.09822296 0.7737764  0.09822296 0.7737764  0.09822296 0.7737764\n",
      " 0.09822296 0.09822296 0.7737764  0.09822296 0.7737764  0.7737764\n",
      " 0.09822296 0.7737764  0.09822296 0.7737764  0.7737764  0.7737764\n",
      " 0.09822296 0.7737764 ]\n",
      " After 0 training  step(s), loss on training batch is [0.08365272 0.08365272 0.08365272 0.7737244  0.08365272 0.08365272\n",
      " 0.7737244  0.7737244  0.08365272 0.7737244  0.08365272 0.08365272\n",
      " 0.7737244  0.08365272 0.08365272 0.7737244  0.7737244  0.08365272\n",
      " 0.08365272 0.7737244 ]\n",
      " After 0 training  step(s), loss on training batch is [0.15881859 0.15881859 0.7736642  0.7736642  0.7736642  0.7736642\n",
      " 0.15881859 0.7736642  0.15881859 0.7736642  0.7736642  0.15881859\n",
      " 0.7736642  0.7736642  0.15881859 0.15881859 0.7736642  0.15881859\n",
      " 0.7736642  0.15881859]\n",
      " After 0 training  step(s), loss on training batch is [0.09001673 0.77363324 0.09001673 0.77363324 0.09001673 0.09001673\n",
      " 0.09001673 0.77363324 0.77363324 0.77363324 0.09001673 0.77363324\n",
      " 0.77363324 0.09001673 0.09001673 0.09001673 0.77363324 0.09001673\n",
      " 0.77363324 0.09001673]\n",
      " After 0 training  step(s), loss on training batch is [0.09311834 0.09311834 0.09311834 0.09311834 0.09311834 0.7735784\n",
      " 0.09311834 0.09311834 0.09311834 0.7735784  0.7735784  0.09311834\n",
      " 0.09311834 0.09311834 0.7735784  0.09311834 0.7735784  0.7735784\n",
      " 0.09311834 0.7735784 ]\n",
      " After 0 training  step(s), loss on training batch is [0.77352816 0.77352816 0.19826064 0.77352816 0.77352816 0.77352816\n",
      " 0.19826064 0.77352816 0.77352816 0.19826064 0.77352816 0.19826064\n",
      " 0.19826064 0.19826064 0.19826064 0.19826064 0.19826065 0.77352816\n",
      " 0.77352816 0.19826065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.09188844 0.77350944 0.09188844 0.09188844 0.09188844 0.09188844\n",
      " 0.77350944 0.09188844 0.77350944 0.09188844 0.77350944 0.77350944\n",
      " 0.09188844 0.09188844 0.77350944 0.77350944 0.77350944 0.77350944\n",
      " 0.09188844 0.09188844]\n",
      " After 0 training  step(s), loss on training batch is [0.08554959 0.77345616 0.08554959 0.77345616 0.08554959 0.77345616\n",
      " 0.08554959 0.77345616 0.08554959 0.77345616 0.08554959 0.08554959\n",
      " 0.08554959 0.08554959 0.77345616 0.77345616 0.77345616 0.08554959\n",
      " 0.08554959 0.08554959]\n",
      " After 0 training  step(s), loss on training batch is [0.77339834 0.77339834 0.77339834 0.77339834 0.77339834 0.35347486\n",
      " 0.35347486 0.77339834 0.35347486 0.35347486 0.35347486 0.35347486\n",
      " 0.35347486 0.35347486 0.35347486 0.35347486 0.77339834 0.77339834\n",
      " 0.77339834 0.77339834]\n",
      " After 0 training  step(s), loss on training batch is [0.77338994 0.77338994 0.77338994 0.10895887 0.10895887 0.77338994\n",
      " 0.77338994 0.10895887 0.10895887 0.77338994 0.77338994 0.10895887\n",
      " 0.10895887 0.77338994 0.77338994 0.77338994 0.10895887 0.77338994\n",
      " 0.77338994 0.77338994]\n",
      " After 0 training  step(s), loss on training batch is [0.77333915 0.77333915 0.77333915 0.09172361 0.09172361 0.77333915\n",
      " 0.09172361 0.77333915 0.77333915 0.09172361 0.09172361 0.09172361\n",
      " 0.77333915 0.77333915 0.77333915 0.09172361 0.09172361 0.09172361\n",
      " 0.09172361 0.77333915]\n",
      " After 0 training  step(s), loss on training batch is [0.08144709 0.08144709 0.08144709 0.08144709 0.08144709 0.7732849\n",
      " 0.7732849  0.7732849  0.08144709 0.7732849  0.7732849  0.08144709\n",
      " 0.7732849  0.08144709 0.08144709 0.08144709 0.08144709 0.08144709\n",
      " 0.7732849  0.08144709]\n",
      " After 0 training  step(s), loss on training batch is [0.08179989 0.08179989 0.08179989 0.08179989 0.08179989 0.08179989\n",
      " 0.08179989 0.08179989 0.773223   0.08179989 0.08179989 0.773223\n",
      " 0.08179989 0.773223   0.08179989 0.773223   0.773223   0.08179989\n",
      " 0.08179989 0.773223  ]\n",
      " After 0 training  step(s), loss on training batch is [0.77316195 0.77316195 0.77316195 0.10371435 0.10371435 0.77316195\n",
      " 0.77316195 0.10371435 0.77316195 0.10371435 0.77316195 0.10371435\n",
      " 0.77316195 0.77316195 0.10371435 0.10371435 0.77316195 0.10371435\n",
      " 0.10371435 0.77316195]\n",
      " After 0 training  step(s), loss on training batch is [0.23792043 0.773113   0.23792043 0.773113   0.773113   0.23792043\n",
      " 0.773113   0.23792043 0.23792043 0.773113   0.23792043 0.773113\n",
      " 0.773113   0.23792043 0.773113   0.23792043 0.773113   0.23792042\n",
      " 0.773113   0.773113  ]\n",
      " After 0 training  step(s), loss on training batch is [0.77309257 0.77309257 0.09560262 0.09560262 0.09560262 0.09560262\n",
      " 0.77309257 0.09560262 0.77309257 0.77309257 0.77309257 0.77309257\n",
      " 0.77309257 0.77309257 0.09560262 0.77309257 0.77309257 0.77309257\n",
      " 0.09560262 0.09560262]\n",
      " After 0 training  step(s), loss on training batch is [0.08764468 0.77303845 0.77303845 0.77303845 0.08764468 0.08764468\n",
      " 0.08764468 0.08764468 0.77303845 0.77303845 0.08764468 0.77303845\n",
      " 0.08764468 0.08764468 0.77303845 0.77303845 0.08764468 0.77303845\n",
      " 0.77303845 0.77303845]\n",
      " After 0 training  step(s), loss on training batch is [0.11187501 0.77298105 0.77298105 0.77298105 0.11187501 0.11187501\n",
      " 0.11187501 0.11187501 0.77298105 0.11187501 0.77298105 0.11187501\n",
      " 0.77298105 0.11187501 0.77298105 0.11187501 0.77298105 0.77298105\n",
      " 0.11187501 0.77298105]\n",
      " After 0 training  step(s), loss on training batch is [0.08262431 0.7729376  0.7729376  0.08262431 0.08262431 0.08262431\n",
      " 0.7729376  0.7729376  0.08262431 0.7729376  0.08262431 0.08262431\n",
      " 0.08262431 0.08262431 0.7729376  0.7729376  0.08262431 0.7729376\n",
      " 0.08262431 0.08262431]\n",
      " After 0 training  step(s), loss on training batch is [0.77287763 0.77287763 0.77287763 0.77287763 0.19245312 0.19245312\n",
      " 0.77287763 0.19245312 0.19245312 0.19245312 0.77287763 0.19245312\n",
      " 0.77287763 0.77287763 0.19245312 0.19245312 0.77287763 0.77287763\n",
      " 0.77287763 0.77287763]\n",
      " After 0 training  step(s), loss on training batch is [0.14834741 0.7728472  0.14834741 0.7728472  0.7728472  0.7728472\n",
      " 0.14834741 0.7728472  0.14834741 0.7728472  0.14834741 0.14834741\n",
      " 0.7728472  0.14834741 0.7728472  0.14834741 0.14834741 0.7728472\n",
      " 0.7728472  0.7728472 ]\n",
      " After 0 training  step(s), loss on training batch is [0.09462853 0.09462853 0.77281266 0.09462853 0.77281266 0.77281266\n",
      " 0.77281266 0.77281266 0.09462853 0.77281266 0.09462853 0.09462853\n",
      " 0.77281266 0.09462853 0.09462853 0.09462853 0.09462853 0.77281266\n",
      " 0.77281266 0.77281266]\n",
      " After 0 training  step(s), loss on training batch is [0.7727607  0.7727607  0.08839586 0.7727607  0.08839586 0.08839586\n",
      " 0.7727607  0.7727607  0.7727607  0.7727607  0.7727607  0.08839586\n",
      " 0.08839586 0.08839586 0.08839586 0.7727607  0.7727607  0.08839586\n",
      " 0.08839586 0.7727607 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7727041  0.19238001 0.19238001 0.7727041  0.7727041  0.7727041\n",
      " 0.19238001 0.7727041  0.7727041  0.7727041  0.19238001 0.19238001\n",
      " 0.19238001 0.19238001 0.7727041  0.19238001 0.7727041  0.19238001\n",
      " 0.7727041  0.7727041 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7726775  0.17482132 0.17482132 0.7726775  0.7726775  0.17482132\n",
      " 0.7726775  0.17482132 0.7726775  0.7726775  0.7726775  0.7726775\n",
      " 0.17482132 0.7726775  0.7726775  0.17482132 0.17482132 0.17482132\n",
      " 0.17482132 0.17482132]\n",
      " After 0 training  step(s), loss on training batch is [0.77265185 0.77265185 0.17241597 0.77265185 0.77265185 0.77265185\n",
      " 0.17241597 0.17241597 0.77265185 0.17241597 0.17241597 0.17241597\n",
      " 0.77265185 0.17241597 0.17241597 0.17241597 0.77265185 0.77265185\n",
      " 0.17241597 0.77265185]\n",
      " After 0 training  step(s), loss on training batch is [0.77262545 0.13269146 0.13269146 0.13269146 0.77262545 0.77262545\n",
      " 0.77262545 0.77262545 0.13269146 0.77262545 0.77262545 0.13269146\n",
      " 0.13269146 0.77262545 0.77262545 0.13269146 0.13269146 0.13269146\n",
      " 0.77262545 0.13269146]\n",
      " After 0 training  step(s), loss on training batch is [0.7725892  0.7725892  0.7725892  0.7725892  0.7725892  0.12930499\n",
      " 0.12930499 0.7725892  0.12930499 0.7725892  0.7725892  0.7725892\n",
      " 0.7725892  0.7725892  0.7725892  0.7725892  0.12930499 0.12930499\n",
      " 0.12930499 0.7725892 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7725413 0.1008074 0.7725413 0.7725413 0.1008074 0.7725413 0.1008074\n",
      " 0.1008074 0.7725413 0.7725413 0.1008074 0.7725413 0.7725413 0.1008074\n",
      " 0.7725413 0.1008074 0.7725413 0.7725413 0.1008074 0.7725413]\n",
      " After 0 training  step(s), loss on training batch is [0.7724897 0.0938423 0.7724897 0.0938423 0.0938423 0.7724897 0.7724897\n",
      " 0.0938423 0.7724897 0.7724897 0.0938423 0.7724897 0.7724897 0.7724897\n",
      " 0.7724897 0.0938423 0.0938423 0.7724897 0.0938423 0.7724897]\n",
      " After 0 training  step(s), loss on training batch is [0.08011112 0.08011112 0.08011112 0.77243525 0.08011112 0.08011112\n",
      " 0.77243525 0.77243525 0.08011112 0.77243525 0.08011112 0.77243525\n",
      " 0.77243525 0.08011112 0.08011112 0.08011112 0.08011112 0.08011112\n",
      " 0.08011112 0.08011112]\n",
      " After 0 training  step(s), loss on training batch is [0.77237344 0.10976597 0.10976597 0.10976597 0.77237344 0.77237344\n",
      " 0.77237344 0.10976597 0.10976597 0.10976597 0.77237344 0.10976597\n",
      " 0.10976597 0.77237344 0.77237344 0.77237344 0.10976597 0.77237344\n",
      " 0.10976597 0.10976597]\n",
      " After 0 training  step(s), loss on training batch is [0.77233106 0.22361678 0.22361678 0.77233106 0.77233106 0.22361678\n",
      " 0.77233106 0.22361678 0.22361678 0.77233106 0.77233106 0.22361678\n",
      " 0.77233106 0.77233106 0.77233106 0.22361678 0.77233106 0.22361678\n",
      " 0.22361678 0.22361678]\n",
      " After 0 training  step(s), loss on training batch is [0.77231085 0.77231085 0.11039184 0.77231085 0.77231085 0.11039184\n",
      " 0.77231085 0.11039184 0.11039184 0.77231085 0.11039184 0.11039184\n",
      " 0.11039184 0.11039184 0.11039184 0.11039184 0.11039184 0.11039184\n",
      " 0.77231085 0.77231085]\n",
      " After 0 training  step(s), loss on training batch is [0.77227056 0.77227056 0.77227056 0.77227056 0.77227056 0.77227056\n",
      " 0.18901592 0.18901592 0.18901592 0.18901592 0.77227056 0.77227056\n",
      " 0.18901592 0.77227056 0.18901592 0.77227056 0.77227056 0.77227056\n",
      " 0.77227056 0.18901592]\n",
      " After 0 training  step(s), loss on training batch is [0.7722338  0.08745362 0.08745362 0.08745362 0.7722338  0.08745362\n",
      " 0.7722338  0.7722338  0.08745362 0.7722338  0.08745362 0.7722338\n",
      " 0.08745362 0.08745362 0.08745362 0.7722338  0.7722338  0.08745362\n",
      " 0.7722338  0.08745362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.08034176 0.08034176 0.08034176 0.08034176 0.08034176 0.08034176\n",
      " 0.08034176 0.77217865 0.77217865 0.08034176 0.77217865 0.08034176\n",
      " 0.77217865 0.77217865 0.08034176 0.08034176 0.08034176 0.77217865\n",
      " 0.08034176 0.08034176]\n",
      " After 0 training  step(s), loss on training batch is [0.77211773 0.77211773 0.77211773 0.77211773 0.77211773 0.77211773\n",
      " 0.77211773 0.21612698 0.21612698 0.21612698 0.21612698 0.77211773\n",
      " 0.21612698 0.77211773 0.21612698 0.21612698 0.21612698 0.77211773\n",
      " 0.77211773 0.77211773]\n",
      " After 0 training  step(s), loss on training batch is [0.08522926 0.08522926 0.772087   0.08522926 0.772087   0.08522926\n",
      " 0.08522926 0.08522926 0.08522926 0.08522926 0.08522926 0.08522926\n",
      " 0.08522926 0.08522926 0.08522926 0.08522926 0.772087   0.08522926\n",
      " 0.772087   0.772087  ]\n",
      " After 0 training  step(s), loss on training batch is [0.7720327  0.10260367 0.7720327  0.10260367 0.7720327  0.10260367\n",
      " 0.7720327  0.10260367 0.10260367 0.7720327  0.10260367 0.7720327\n",
      " 0.10260367 0.7720327  0.10260367 0.7720327  0.7720327  0.10260367\n",
      " 0.7720327  0.7720327 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7719836  0.07944926 0.7719836  0.7719836  0.07944926 0.7719836\n",
      " 0.07944926 0.07944926 0.7719836  0.07944926 0.07944926 0.7719836\n",
      " 0.07944926 0.07944926 0.07944926 0.7719836  0.07944926 0.07944926\n",
      " 0.07944926 0.7719836 ]\n",
      " After 0 training  step(s), loss on training batch is [0.77192163 0.08233984 0.77192163 0.08233984 0.08233984 0.77192163\n",
      " 0.08233984 0.77192163 0.08233984 0.77192163 0.08233984 0.08233984\n",
      " 0.77192163 0.08233984 0.77192163 0.77192163 0.77192163 0.77192163\n",
      " 0.08233984 0.77192163]\n",
      " After 0 training  step(s), loss on training batch is [0.771862   0.771862   0.14623523 0.14623523 0.771862   0.14623523\n",
      " 0.771862   0.771862   0.771862   0.14623523 0.771862   0.14623523\n",
      " 0.14623523 0.771862   0.14623523 0.14623523 0.14623521 0.771862\n",
      " 0.771862   0.14623521]\n",
      " After 0 training  step(s), loss on training batch is [0.7718289  0.7718289  0.11395051 0.7718289  0.11395051 0.11395051\n",
      " 0.11395051 0.7718289  0.7718289  0.11395051 0.11395051 0.7718289\n",
      " 0.11395051 0.7718289  0.11395051 0.11395051 0.7718289  0.7718289\n",
      " 0.7718289  0.7718289 ]\n",
      " After 0 training  step(s), loss on training batch is [0.77178425 0.12026766 0.12026766 0.77178425 0.77178425 0.77178425\n",
      " 0.12026766 0.12026766 0.12026766 0.77178425 0.77178425 0.12026766\n",
      " 0.77178425 0.77178425 0.77178425 0.77178425 0.12026766 0.77178425\n",
      " 0.77178425 0.12026766]\n",
      " After 0 training  step(s), loss on training batch is [0.77173924 0.77173924 0.19365248 0.77173924 0.77173924 0.77173924\n",
      " 0.19365248 0.19365248 0.19365248 0.19365248 0.19365248 0.77173924\n",
      " 0.77173924 0.77173924 0.19365248 0.19365248 0.77173924 0.77173924\n",
      " 0.77173924 0.19365248]\n",
      " After 0 training  step(s), loss on training batch is [0.10499775 0.77171004 0.77171004 0.77171004 0.77171004 0.77171004\n",
      " 0.77171004 0.77171004 0.77171004 0.10499775 0.10499775 0.10499775\n",
      " 0.77171004 0.10499775 0.10499775 0.77171004 0.77171004 0.77171004\n",
      " 0.10499775 0.77171004]\n",
      " After 0 training  step(s), loss on training batch is [0.08043382 0.7716588  0.7716588  0.08043382 0.7716588  0.08043382\n",
      " 0.7716588  0.08043382 0.7716588  0.7716588  0.08043382 0.7716588\n",
      " 0.7716588  0.08043382 0.08043382 0.7716588  0.08043382 0.08043382\n",
      " 0.08043382 0.08043382]\n",
      " After 0 training  step(s), loss on training batch is [0.77159846 0.08051518 0.77159846 0.08051518 0.08051518 0.08051518\n",
      " 0.77159846 0.08051518 0.77159846 0.08051518 0.77159846 0.08051518\n",
      " 0.77159846 0.08051518 0.77159846 0.77159846 0.08051518 0.77159846\n",
      " 0.08051518 0.08051518]\n",
      " After 0 training  step(s), loss on training batch is [0.08224335 0.08224335 0.7715383  0.08224335 0.7715383  0.08224335\n",
      " 0.7715383  0.08224335 0.08224335 0.7715383  0.08224335 0.7715383\n",
      " 0.7715383  0.08224335 0.08224335 0.7715383  0.08224335 0.7715383\n",
      " 0.7715383  0.7715383 ]\n",
      " After 0 training  step(s), loss on training batch is [0.77147955 0.77147955 0.77147955 0.11607857 0.77147955 0.11607857\n",
      " 0.77147955 0.11607857 0.11607857 0.11607857 0.77147955 0.77147955\n",
      " 0.11607857 0.77147955 0.77147955 0.11607857 0.11607857 0.11607857\n",
      " 0.11607857 0.11607857]\n",
      " After 0 training  step(s), loss on training batch is [0.09187941 0.7714399  0.7714399  0.09187941 0.7714399  0.7714399\n",
      " 0.7714399  0.7714399  0.7714399  0.7714399  0.7714399  0.09187941\n",
      " 0.7714399  0.7714399  0.09187941 0.7714399  0.7714399  0.09187941\n",
      " 0.09187941 0.09187941]\n",
      " After 0 training  step(s), loss on training batch is [0.08976567 0.08976567 0.08976567 0.77138454 0.08976567 0.77138454\n",
      " 0.77138454 0.08976567 0.77138454 0.08976567 0.77138454 0.08976567\n",
      " 0.08976567 0.77138454 0.08976567 0.08976567 0.77138454 0.77138454\n",
      " 0.77138454 0.08976567]\n",
      " After 0 training  step(s), loss on training batch is [0.18153551 0.18153551 0.18153551 0.771332   0.771332   0.18153551\n",
      " 0.771332   0.771332   0.18153551 0.771332   0.18153551 0.18153551\n",
      " 0.771332   0.771332   0.771332   0.18153551 0.771332   0.771332\n",
      " 0.1815355  0.771332  ]\n",
      " After 0 training  step(s), loss on training batch is [0.08929026 0.77130115 0.77130115 0.77130115 0.08929026 0.77130115\n",
      " 0.77130115 0.77130115 0.77130115 0.77130115 0.77130115 0.08929026\n",
      " 0.77130115 0.77130115 0.77130115 0.77130115 0.08929026 0.77130115\n",
      " 0.08929026 0.77130115]\n",
      " After 0 training  step(s), loss on training batch is [0.09073733 0.09073733 0.09073733 0.09073733 0.7712431  0.7712431\n",
      " 0.7712431  0.7712431  0.7712431  0.7712431  0.09073733 0.7712431\n",
      " 0.7712431  0.7712431  0.09073733 0.09073733 0.7712431  0.09073733\n",
      " 0.09073733 0.7712431 ]\n",
      " After 0 training  step(s), loss on training batch is [0.09094658 0.09094658 0.09094658 0.09094658 0.7711894  0.09094658\n",
      " 0.09094658 0.09094658 0.09094658 0.7711894  0.7711894  0.7711894\n",
      " 0.09094658 0.09094658 0.7711894  0.09094658 0.7711894  0.09094658\n",
      " 0.7711894  0.7711894 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07922297 0.07922297 0.07922297 0.07922297 0.7711389  0.07922297\n",
      " 0.07922297 0.07922297 0.7711389  0.07922297 0.7711389  0.7711389\n",
      " 0.7711389  0.07922297 0.07922297 0.7711389  0.07922297 0.7711389\n",
      " 0.07922297 0.07922297]\n",
      " After 0 training  step(s), loss on training batch is [0.07925806 0.07925806 0.77107847 0.77107847 0.07925806 0.77107847\n",
      " 0.07925806 0.77107847 0.07925806 0.77107847 0.77107847 0.77107847\n",
      " 0.07925806 0.77107847 0.07925806 0.77107847 0.07925806 0.77107847\n",
      " 0.07925806 0.77107847]\n",
      " After 0 training  step(s), loss on training batch is [0.07825553 0.7710176  0.07825553 0.07825553 0.07825553 0.7710176\n",
      " 0.07825553 0.07825553 0.07825553 0.7710176  0.7710176  0.7710176\n",
      " 0.7710176  0.07825553 0.07825553 0.07825553 0.07825553 0.07825553\n",
      " 0.07825553 0.7710176 ]\n",
      " After 0 training  step(s), loss on training batch is [0.08430303 0.7709561  0.7709561  0.7709561  0.08430303 0.08430303\n",
      " 0.7709561  0.08430303 0.7709561  0.7709561  0.08430303 0.7709561\n",
      " 0.08430303 0.7709561  0.08430303 0.7709561  0.7709561  0.08430303\n",
      " 0.7709561  0.7709561 ]\n",
      " After 0 training  step(s), loss on training batch is [0.08249001 0.08249001 0.08249001 0.08249001 0.08249001 0.7708985\n",
      " 0.7708985  0.7708985  0.7708985  0.7708985  0.7708985  0.7708985\n",
      " 0.08249001 0.08249001 0.7708985  0.08249001 0.7708985  0.7708985\n",
      " 0.08249001 0.08249001]\n",
      " After 0 training  step(s), loss on training batch is [0.77084094 0.09694798 0.77084094 0.09694798 0.77084094 0.09694798\n",
      " 0.09694798 0.09694798 0.77084094 0.77084094 0.77084094 0.09694798\n",
      " 0.09694798 0.09694798 0.77084094 0.77084094 0.77084094 0.09694798\n",
      " 0.77084094 0.77084094]\n",
      " After 0 training  step(s), loss on training batch is [0.1105334 0.7707907 0.7707907 0.7707907 0.7707907 0.1105334 0.1105334\n",
      " 0.7707907 0.7707907 0.7707907 0.7707907 0.1105334 0.7707907 0.7707907\n",
      " 0.1105334 0.7707907 0.7707907 0.7707907 0.1105334 0.1105334]\n",
      " After 0 training  step(s), loss on training batch is [0.77074176 0.09314688 0.77074176 0.09314688 0.77074176 0.09314688\n",
      " 0.77074176 0.09314688 0.77074176 0.77074176 0.09314688 0.77074176\n",
      " 0.77074176 0.77074176 0.77074176 0.77074176 0.09314688 0.09314688\n",
      " 0.77074176 0.09314688]\n",
      " After 0 training  step(s), loss on training batch is [0.07871529 0.07871529 0.7706887  0.07871529 0.7706887  0.7706887\n",
      " 0.07871529 0.7706887  0.07871529 0.07871529 0.7706887  0.07871529\n",
      " 0.07871529 0.7706887  0.07871529 0.7706887  0.07871528 0.07871528\n",
      " 0.07871528 0.07871528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.7706286 0.3645135 0.3645135 0.7706286 0.7706286 0.3645135 0.7706286\n",
      " 0.3645135 0.3645135 0.3645135 0.7706286 0.3645135 0.3645135 0.7706286\n",
      " 0.7706286 0.3645135 0.7706286 0.7706286 0.7706286 0.7706286]\n",
      " After 0 training  step(s), loss on training batch is [0.12195626 0.12195626 0.12195626 0.770598   0.770598   0.12195626\n",
      " 0.770598   0.770598   0.770598   0.770598   0.770598   0.770598\n",
      " 0.770598   0.770598   0.770598   0.770598   0.12195626 0.770598\n",
      " 0.770598   0.770598  ]\n",
      " After 0 training  step(s), loss on training batch is [0.08857373 0.08857373 0.08857373 0.08857373 0.77054715 0.08857373\n",
      " 0.77054715 0.08857373 0.08857373 0.08857373 0.77054715 0.08857373\n",
      " 0.08857373 0.08857373 0.77054715 0.77054715 0.77054715 0.77054715\n",
      " 0.77054715 0.08857373]\n",
      " After 0 training  step(s), loss on training batch is [0.07863318 0.7704958  0.7704958  0.07863318 0.07863318 0.07863318\n",
      " 0.07863318 0.7704958  0.7704958  0.07863318 0.07863318 0.7704958\n",
      " 0.7704958  0.07863318 0.07863318 0.07863318 0.07863318 0.7704958\n",
      " 0.7704958  0.7704958 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07920705 0.77043563 0.77043563 0.77043563 0.07920705 0.77043563\n",
      " 0.07920705 0.77043563 0.07920705 0.77043563 0.77043563 0.77043563\n",
      " 0.07920705 0.07920705 0.07920705 0.77043563 0.77043563 0.77043563\n",
      " 0.07920705 0.07920705]\n",
      " After 0 training  step(s), loss on training batch is [0.08783256 0.7703757  0.08783256 0.08783256 0.7703757  0.7703757\n",
      " 0.7703757  0.7703757  0.08783256 0.08783256 0.7703757  0.7703757\n",
      " 0.08783256 0.7703757  0.08783256 0.7703757  0.7703757  0.08783256\n",
      " 0.7703757  0.08783256]\n",
      " After 0 training  step(s), loss on training batch is [0.77032155 0.08309463 0.77032155 0.77032155 0.08309463 0.08309463\n",
      " 0.08309463 0.08309463 0.77032155 0.08309463 0.77032155 0.08309463\n",
      " 0.77032155 0.77032155 0.08309463 0.08309463 0.77032155 0.08309463\n",
      " 0.77032155 0.08309463]\n",
      " After 0 training  step(s), loss on training batch is [0.77026576 0.77026576 0.10390021 0.77026576 0.10390021 0.77026576\n",
      " 0.77026576 0.77026576 0.77026576 0.77026576 0.10390021 0.77026576\n",
      " 0.77026576 0.77026576 0.77026576 0.10390021 0.10390021 0.77026576\n",
      " 0.10390021 0.77026576]\n",
      " After 0 training  step(s), loss on training batch is [0.07747157 0.07747157 0.7702138  0.07747157 0.07747157 0.07747157\n",
      " 0.07747157 0.7702138  0.7702138  0.07747157 0.07747157 0.7702138\n",
      " 0.7702138  0.7702138  0.7702138  0.7702138  0.07747157 0.07747157\n",
      " 0.07747157 0.07747157]\n",
      " After 0 training  step(s), loss on training batch is [0.16203994 0.7701528  0.16203994 0.7701528  0.7701528  0.7701528\n",
      " 0.16203994 0.16203994 0.7701528  0.16203994 0.16203994 0.16203994\n",
      " 0.16203994 0.7701528  0.7701528  0.16203994 0.7701528  0.7701528\n",
      " 0.16203994 0.7701528 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07934532 0.07934532 0.77012223 0.07934532 0.77012223 0.07934532\n",
      " 0.77012223 0.77012223 0.07934532 0.77012223 0.07934532 0.07934532\n",
      " 0.07934532 0.77012223 0.07934532 0.07934532 0.07934532 0.07934532\n",
      " 0.07934532 0.07934532]\n",
      " After 0 training  step(s), loss on training batch is [0.08320819 0.77006423 0.08320819 0.08320819 0.77006423 0.77006423\n",
      " 0.08320819 0.08320819 0.08320819 0.08320819 0.08320819 0.77006423\n",
      " 0.08320819 0.08320819 0.77006423 0.77006423 0.77006423 0.77006423\n",
      " 0.08320819 0.08320819]\n",
      " After 0 training  step(s), loss on training batch is [0.7700095  0.07884661 0.07884661 0.07884661 0.07884661 0.07884661\n",
      " 0.07884661 0.07884661 0.07884661 0.7700095  0.07884661 0.7700095\n",
      " 0.07884661 0.7700095  0.7700095  0.07884661 0.07884661 0.7700095\n",
      " 0.07884661 0.07884661]\n",
      " After 0 training  step(s), loss on training batch is [0.08206987 0.08206987 0.76995116 0.08206987 0.76995116 0.08206987\n",
      " 0.76995116 0.08206987 0.08206987 0.76995116 0.76995116 0.08206987\n",
      " 0.76995116 0.76995116 0.08206987 0.76995116 0.76995116 0.08206987\n",
      " 0.08206987 0.76995116]\n",
      " After 0 training  step(s), loss on training batch is [0.7698946  0.09376638 0.7698946  0.09376638 0.7698946  0.09376638\n",
      " 0.7698946  0.7698946  0.7698946  0.09376638 0.09376638 0.09376638\n",
      " 0.7698946  0.7698946  0.09376638 0.7698946  0.7698946  0.7698946\n",
      " 0.09376638 0.09376638]\n",
      " After 0 training  step(s), loss on training batch is [0.08688194 0.7698438  0.7698438  0.7698438  0.08688194 0.7698438\n",
      " 0.7698438  0.7698438  0.08688194 0.7698438  0.7698438  0.7698438\n",
      " 0.7698438  0.7698438  0.08688194 0.08688194 0.08688194 0.7698438\n",
      " 0.7698438  0.7698438 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7697873 0.0797152 0.0797152 0.7697873 0.7697873 0.0797152 0.7697873\n",
      " 0.7697873 0.0797152 0.0797152 0.0797152 0.7697873 0.7697873 0.0797152\n",
      " 0.0797152 0.7697873 0.0797152 0.0797152 0.7697873 0.0797152]\n",
      " After 0 training  step(s), loss on training batch is [0.07723828 0.07723828 0.07723828 0.76972955 0.07723828 0.07723828\n",
      " 0.07723828 0.76972955 0.07723828 0.07723828 0.07723828 0.07723828\n",
      " 0.07723828 0.76972955 0.76972955 0.76972955 0.07723828 0.07723828\n",
      " 0.76972955 0.07723828]\n",
      " After 0 training  step(s), loss on training batch is [0.76966953 0.08153136 0.76966953 0.08153136 0.76966953 0.08153136\n",
      " 0.76966953 0.08153136 0.08153136 0.08153136 0.08153136 0.08153136\n",
      " 0.76966953 0.08153136 0.76966953 0.76966953 0.08153136 0.08153136\n",
      " 0.76966953 0.76966953]\n",
      " After 0 training  step(s), loss on training batch is [0.08724286 0.7696135  0.08724286 0.7696135  0.7696135  0.08724286\n",
      " 0.7696135  0.7696135  0.08724286 0.08724286 0.08724286 0.7696135\n",
      " 0.7696135  0.7696135  0.08724286 0.08724286 0.08724286 0.08724286\n",
      " 0.7696135  0.08724286]\n",
      " After 0 training  step(s), loss on training batch is [0.0783727  0.76956165 0.76956165 0.0783727  0.0783727  0.0783727\n",
      " 0.76956165 0.0783727  0.76956165 0.0783727  0.76956165 0.0783727\n",
      " 0.76956165 0.0783727  0.76956165 0.76956165 0.0783727  0.76956165\n",
      " 0.76956165 0.0783727 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07768456 0.07768456 0.07768456 0.07768456 0.07768456 0.07768456\n",
      " 0.76950276 0.07768456 0.76950276 0.76950276 0.07768456 0.76950276\n",
      " 0.07768456 0.07768456 0.07768456 0.07768456 0.76950276 0.76950276\n",
      " 0.07768456 0.76950276]\n",
      " After 0 training  step(s), loss on training batch is [0.76944375 0.76944375 0.09640989 0.76944375 0.76944375 0.09640989\n",
      " 0.09640989 0.09640989 0.09640989 0.76944375 0.09640989 0.76944375\n",
      " 0.09640989 0.09640989 0.76944375 0.09640989 0.09640989 0.09640989\n",
      " 0.76944375 0.76944375]\n",
      " After 0 training  step(s), loss on training batch is [0.07840519 0.76939726 0.76939726 0.76939726 0.76939726 0.07840519\n",
      " 0.07840519 0.07840519 0.76939726 0.76939726 0.07840519 0.76939726\n",
      " 0.07840519 0.07840519 0.07840519 0.76939726 0.07840519 0.07840519\n",
      " 0.07840519 0.76939726]\n",
      " After 0 training  step(s), loss on training batch is [0.76933885 0.08049375 0.76933885 0.08049375 0.08049375 0.08049375\n",
      " 0.76933885 0.08049375 0.76933885 0.76933885 0.76933885 0.08049375\n",
      " 0.76933885 0.08049375 0.08049375 0.08049375 0.76933885 0.76933885\n",
      " 0.08049375 0.76933885]\n",
      " After 0 training  step(s), loss on training batch is [0.09056725 0.7692821  0.7692821  0.7692821  0.09056725 0.7692821\n",
      " 0.7692821  0.7692821  0.7692821  0.7692821  0.7692821  0.7692821\n",
      " 0.7692821  0.7692821  0.7692821  0.09056725 0.09056725 0.7692821\n",
      " 0.09056725 0.09056725]\n",
      " After 0 training  step(s), loss on training batch is [0.76922745 0.76922745 0.76922745 0.76922745 0.07750858 0.07750858\n",
      " 0.07750858 0.76922745 0.76922745 0.07750858 0.07750858 0.07750858\n",
      " 0.07750858 0.76922745 0.07750858 0.76922745 0.07750858 0.07750858\n",
      " 0.07750858 0.76922745]\n",
      " After 0 training  step(s), loss on training batch is [0.08484782 0.7691685  0.08484782 0.08484782 0.7691685  0.7691685\n",
      " 0.08484782 0.08484782 0.08484782 0.7691685  0.08484782 0.08484782\n",
      " 0.7691685  0.08484782 0.7691685  0.08484782 0.7691685  0.08484782\n",
      " 0.7691685  0.08484782]\n",
      " After 0 training  step(s), loss on training batch is [0.7691164  0.08023565 0.7691164  0.08023565 0.08023565 0.7691164\n",
      " 0.08023565 0.08023565 0.08023565 0.08023565 0.7691164  0.08023565\n",
      " 0.7691164  0.7691164  0.7691164  0.08023565 0.08023565 0.7691164\n",
      " 0.7691164  0.08023565]\n",
      " After 0 training  step(s), loss on training batch is [0.76906013 0.07772927 0.76906013 0.07772927 0.07772927 0.07772927\n",
      " 0.76906013 0.07772927 0.07772927 0.07772927 0.07772927 0.76906013\n",
      " 0.07772927 0.76906013 0.76906013 0.76906013 0.07772927 0.76906013\n",
      " 0.07772927 0.07772927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.11521181 0.11521181 0.76900196 0.11521181 0.76900196 0.76900196\n",
      " 0.76900196 0.76900196 0.76900196 0.11521181 0.11521181 0.76900196\n",
      " 0.76900196 0.11521181 0.76900196 0.11521181 0.76900196 0.76900196\n",
      " 0.76900196 0.76900196]\n",
      " After 0 training  step(s), loss on training batch is [0.7689554  0.7689554  0.7689554  0.7689554  0.7689554  0.7689554\n",
      " 0.7689554  0.7689554  0.7689554  0.7689554  0.7689554  0.7689554\n",
      " 0.12783967 0.7689554  0.12783967 0.7689554  0.12783967 0.7689554\n",
      " 0.12783967 0.7689554 ]\n",
      " After 0 training  step(s), loss on training batch is [0.0809743 0.0809743 0.0809743 0.7689042 0.0809743 0.0809743 0.7689042\n",
      " 0.7689042 0.7689042 0.7689042 0.7689042 0.0809743 0.7689042 0.7689042\n",
      " 0.7689042 0.0809743 0.0809743 0.0809743 0.7689042 0.7689042]\n",
      " After 0 training  step(s), loss on training batch is [0.76884794 0.07887875 0.76884794 0.07887875 0.07887875 0.76884794\n",
      " 0.07887875 0.07887875 0.07887875 0.76884794 0.76884794 0.76884794\n",
      " 0.07887875 0.76884794 0.76884794 0.07887875 0.07887875 0.76884794\n",
      " 0.07887875 0.07887875]\n",
      " After 0 training  step(s), loss on training batch is [0.10790788 0.768791   0.10790788 0.768791   0.768791   0.768791\n",
      " 0.768791   0.10790788 0.10790788 0.10790788 0.768791   0.10790788\n",
      " 0.10790788 0.768791   0.10790788 0.768791   0.768791   0.768791\n",
      " 0.768791   0.10790788]\n",
      " After 0 training  step(s), loss on training batch is [0.7687466  0.7687466  0.10212731 0.10212731 0.7687466  0.10212731\n",
      " 0.10212731 0.10212731 0.7687466  0.10212731 0.10212731 0.10212731\n",
      " 0.7687466  0.7687466  0.10212731 0.7687466  0.7687466  0.7687466\n",
      " 0.7687466  0.7687466 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07569544 0.07569544 0.07569544 0.7687003  0.07569544 0.07569544\n",
      " 0.07569544 0.07569544 0.07569544 0.7687003  0.07569544 0.07569544\n",
      " 0.7687003  0.07569544 0.07569544 0.07569544 0.07569544 0.07569544\n",
      " 0.07569544 0.07569544]\n",
      " After 0 training  step(s), loss on training batch is [0.7686403  0.09066939 0.09066939 0.7686403  0.09066939 0.09066939\n",
      " 0.09066939 0.7686403  0.09066939 0.09066939 0.7686403  0.09066939\n",
      " 0.7686403  0.7686403  0.7686403  0.09066939 0.7686403  0.7686403\n",
      " 0.09066939 0.7686403 ]\n",
      " After 0 training  step(s), loss on training batch is [0.08282828 0.7685906  0.7685906  0.7685906  0.08282828 0.08282828\n",
      " 0.7685906  0.7685906  0.08282828 0.7685906  0.08282828 0.08282828\n",
      " 0.7685906  0.7685906  0.08282828 0.08282828 0.7685906  0.7685906\n",
      " 0.08282828 0.08282828]\n",
      " After 0 training  step(s), loss on training batch is [0.08748624 0.76853657 0.08748624 0.76853657 0.08748624 0.76853657\n",
      " 0.76853657 0.08748624 0.76853657 0.76853657 0.76853657 0.08748624\n",
      " 0.76853657 0.76853657 0.76853657 0.76853657 0.76853657 0.76853657\n",
      " 0.08748624 0.08748624]\n",
      " After 0 training  step(s), loss on training batch is [0.7684826  0.7684826  0.09958553 0.7684826  0.09958553 0.7684826\n",
      " 0.7684826  0.09958553 0.7684826  0.09958553 0.7684826  0.7684826\n",
      " 0.7684826  0.7684826  0.7684826  0.7684826  0.7684826  0.7684826\n",
      " 0.09958553 0.09958553]\n",
      " After 0 training  step(s), loss on training batch is [0.07758737 0.76843125 0.07758737 0.76843125 0.07758737 0.76843125\n",
      " 0.76843125 0.07758737 0.07758737 0.07758737 0.76843125 0.07758737\n",
      " 0.76843125 0.07758737 0.76843125 0.76843125 0.07758737 0.76843125\n",
      " 0.76843125 0.07758737]\n",
      " After 0 training  step(s), loss on training batch is [0.11209698 0.7683735  0.7683735  0.11209698 0.7683735  0.7683735\n",
      " 0.7683735  0.7683735  0.11209698 0.11209698 0.11209698 0.7683735\n",
      " 0.11209698 0.7683735  0.7683735  0.11209698 0.7683735  0.11209698\n",
      " 0.7683735  0.11209698]\n",
      " After 0 training  step(s), loss on training batch is [0.7683307  0.07592937 0.7683307  0.7683307  0.07592937 0.07592937\n",
      " 0.07592937 0.07592937 0.7683307  0.7683307  0.7683307  0.07592937\n",
      " 0.7683307  0.07592937 0.07592937 0.7683307  0.07592937 0.7683307\n",
      " 0.07592937 0.07592937]\n",
      " After 0 training  step(s), loss on training batch is [0.07581384 0.07581384 0.7682716  0.7682716  0.07581384 0.07581384\n",
      " 0.07581384 0.07581384 0.07581384 0.07581384 0.07581384 0.07581384\n",
      " 0.7682716  0.7682716  0.07581384 0.07581384 0.07581384 0.07581384\n",
      " 0.7682716  0.7682716 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07992661 0.7682128  0.07992661 0.7682128  0.7682128  0.07992661\n",
      " 0.07992661 0.7682128  0.07992661 0.07992661 0.07992661 0.7682128\n",
      " 0.7682128  0.07992661 0.07992661 0.07992661 0.07992661 0.7682128\n",
      " 0.07992661 0.7682128 ]\n",
      " After 0 training  step(s), loss on training batch is [0.0756068  0.0756068  0.0756068  0.76815826 0.0756068  0.76815826\n",
      " 0.0756068  0.0756068  0.76815826 0.76815826 0.76815826 0.0756068\n",
      " 0.76815826 0.0756068  0.76815826 0.0756068  0.0756068  0.0756068\n",
      " 0.0756068  0.0756068 ]\n",
      " After 0 training  step(s), loss on training batch is [0.0753061 0.7680993 0.7680993 0.0753061 0.0753061 0.7680993 0.0753061\n",
      " 0.0753061 0.7680993 0.0753061 0.0753061 0.0753061 0.7680993 0.0753061\n",
      " 0.0753061 0.7680993 0.0753061 0.7680993 0.7680993 0.0753061]\n",
      " After 0 training  step(s), loss on training batch is [0.10075398 0.10075398 0.76803994 0.76803994 0.76803994 0.76803994\n",
      " 0.10075398 0.10075398 0.10075398 0.76803994 0.76803994 0.76803994\n",
      " 0.76803994 0.76803994 0.10075398 0.10075398 0.76803994 0.76803994\n",
      " 0.10075398 0.76803994]\n",
      " After 0 training  step(s), loss on training batch is [0.7679923 0.7679923 0.0785583 0.7679923 0.0785583 0.7679923 0.0785583\n",
      " 0.7679923 0.7679923 0.7679923 0.0785583 0.0785583 0.7679923 0.7679923\n",
      " 0.7679923 0.7679923 0.0785583 0.0785583 0.7679923 0.0785583]\n",
      " After 0 training  step(s), loss on training batch is [0.7679354  0.08533631 0.08533631 0.08533631 0.7679354  0.08533631\n",
      " 0.7679354  0.7679354  0.08533631 0.08533631 0.7679354  0.7679354\n",
      " 0.7679354  0.08533631 0.7679354  0.08533631 0.08533631 0.08533631\n",
      " 0.7679354  0.7679354 ]\n",
      " After 0 training  step(s), loss on training batch is [0.16576856 0.16576856 0.7678838  0.7678838  0.7678838  0.16576856\n",
      " 0.7678838  0.16576856 0.7678838  0.7678838  0.16576856 0.16576856\n",
      " 0.16576856 0.7678838  0.7678838  0.16576856 0.7678838  0.7678838\n",
      " 0.7678838  0.16576856]\n",
      " After 0 training  step(s), loss on training batch is [0.76785123 0.76785123 0.76785123 0.07893153 0.07893153 0.07893153\n",
      " 0.76785123 0.07893153 0.07893153 0.76785123 0.07893153 0.76785123\n",
      " 0.76785123 0.76785123 0.07893153 0.07893153 0.07893153 0.76785123\n",
      " 0.76785123 0.07893153]\n",
      " After 0 training  step(s), loss on training batch is [0.07659383 0.07659383 0.7677955  0.7677955  0.07659383 0.7677955\n",
      " 0.7677955  0.07659383 0.7677955  0.07659383 0.07659383 0.07659383\n",
      " 0.7677955  0.07659383 0.7677955  0.7677955  0.07659383 0.7677955\n",
      " 0.7677955  0.7677955 ]\n",
      " After 0 training  step(s), loss on training batch is [0.76773775 0.76773775 0.07722519 0.07722519 0.07722519 0.76773775\n",
      " 0.07722519 0.07722519 0.76773775 0.07722519 0.76773775 0.76773775\n",
      " 0.07722519 0.76773775 0.07722519 0.07722519 0.07722519 0.76773775\n",
      " 0.07722519 0.76773775]\n",
      " After 0 training  step(s), loss on training batch is [0.07852592 0.07852592 0.07852592 0.07852592 0.7676811  0.07852592\n",
      " 0.07852592 0.07852592 0.7676811  0.07852592 0.7676811  0.07852592\n",
      " 0.07852592 0.07852592 0.07852592 0.07852592 0.7676811  0.7676811\n",
      " 0.07852592 0.7676811 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07529327 0.07529327 0.76762694 0.07529327 0.07529327 0.07529327\n",
      " 0.07529327 0.07529327 0.07529327 0.76762694 0.07529327 0.07529327\n",
      " 0.76762694 0.76762694 0.07529327 0.76762694 0.76762694 0.76762694\n",
      " 0.76762694 0.07529327]\n",
      " After 0 training  step(s), loss on training batch is [0.76756865 0.76756865 0.76756865 0.76756865 0.10572251 0.76756865\n",
      " 0.76756865 0.76756865 0.76756865 0.76756865 0.76756865 0.76756865\n",
      " 0.10572251 0.10572251 0.76756865 0.10572251 0.76756865 0.10572251\n",
      " 0.76756865 0.76756865]\n",
      " After 0 training  step(s), loss on training batch is [0.7675177  0.7675177  0.07840046 0.7675177  0.07840046 0.07840046\n",
      " 0.7675177  0.7675177  0.7675177  0.07840046 0.7675177  0.07840046\n",
      " 0.07840046 0.7675177  0.07840046 0.7675177  0.7675177  0.07840046\n",
      " 0.07840046 0.07840046]\n",
      " After 0 training  step(s), loss on training batch is [0.07509042 0.7674621  0.7674621  0.7674621  0.07509042 0.07509042\n",
      " 0.07509042 0.7674621  0.7674621  0.07509042 0.7674621  0.7674621\n",
      " 0.7674621  0.07509042 0.07509042 0.07509042 0.07509042 0.07509042\n",
      " 0.07509042 0.7674621 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.07969581 0.7674037  0.07969581 0.07969581 0.07969581 0.7674037\n",
      " 0.07969581 0.07969581 0.07969581 0.07969581 0.7674037  0.07969581\n",
      " 0.7674037  0.7674037  0.7674037  0.07969581 0.7674037  0.7674037\n",
      " 0.07969581 0.7674037 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07904707 0.7673498  0.7673498  0.07904707 0.7673498  0.07904707\n",
      " 0.7673498  0.7673498  0.07904707 0.7673498  0.07904707 0.07904707\n",
      " 0.7673498  0.07904707 0.07904707 0.07904707 0.07904707 0.7673498\n",
      " 0.07904707 0.7673498 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07565565 0.76729536 0.76729536 0.76729536 0.07565565 0.76729536\n",
      " 0.07565565 0.76729536 0.07565565 0.07565565 0.76729536 0.07565565\n",
      " 0.07565565 0.76729536 0.76729536 0.76729536 0.07565565 0.07565565\n",
      " 0.07565565 0.76729536]\n",
      " After 0 training  step(s), loss on training batch is [0.0777868  0.76723784 0.0777868  0.0777868  0.76723784 0.0777868\n",
      " 0.76723784 0.0777868  0.0777868  0.0777868  0.76723784 0.0777868\n",
      " 0.76723784 0.0777868  0.76723784 0.76723784 0.0777868  0.76723784\n",
      " 0.0777868  0.0777868 ]\n",
      " After 0 training  step(s), loss on training batch is [0.76718295 0.76718295 0.10193266 0.10193266 0.76718295 0.76718295\n",
      " 0.10193266 0.76718295 0.76718295 0.10193266 0.76718295 0.76718295\n",
      " 0.10193266 0.76718295 0.76718295 0.76718295 0.76718295 0.10193266\n",
      " 0.10193266 0.76718295]\n",
      " After 0 training  step(s), loss on training batch is [0.09079412 0.76713485 0.09079412 0.09079412 0.76713485 0.76713485\n",
      " 0.09079412 0.76713485 0.09079412 0.09079412 0.76713485 0.76713485\n",
      " 0.76713485 0.76713485 0.76713485 0.76713485 0.76713485 0.09079412\n",
      " 0.76713485 0.76713485]\n",
      " After 0 training  step(s), loss on training batch is [0.76708364 0.76708364 0.76708364 0.76708364 0.76708364 0.76708364\n",
      " 0.76708364 0.76708364 0.76708364 0.76708364 0.0817381  0.76708364\n",
      " 0.0817381  0.0817381  0.0817381  0.76708364 0.0817381  0.0817381\n",
      " 0.0817381  0.76708364]\n",
      " After 0 training  step(s), loss on training batch is [0.07415336 0.07415336 0.07415336 0.07415336 0.07415336 0.07415336\n",
      " 0.07415336 0.76702905 0.76702905 0.76702905 0.76702905 0.07415336\n",
      " 0.07415336 0.07415336 0.76702905 0.07415336 0.07415336 0.07415336\n",
      " 0.07415336 0.07415336]\n",
      " After 0 training  step(s), loss on training batch is [0.7669705 0.0819558 0.0819558 0.7669705 0.7669705 0.7669705 0.0819558\n",
      " 0.7669705 0.7669705 0.7669705 0.0819558 0.7669705 0.0819558 0.0819558\n",
      " 0.0819558 0.7669705 0.7669705 0.0819558 0.7669705 0.7669705]\n",
      " After 0 training  step(s), loss on training batch is [0.7669169  0.7669169  0.7669169  0.07430346 0.07430346 0.7669169\n",
      " 0.7669169  0.07430346 0.07430346 0.7669169  0.07430346 0.07430346\n",
      " 0.7669169  0.07430346 0.07430346 0.07430346 0.07430346 0.07430346\n",
      " 0.07430346 0.07430346]\n",
      " After 0 training  step(s), loss on training batch is [0.7668588  0.7668588  0.08740975 0.08740975 0.7668588  0.08740975\n",
      " 0.08740975 0.7668588  0.08740975 0.08740975 0.7668588  0.7668588\n",
      " 0.08740975 0.7668588  0.7668588  0.08740975 0.08740975 0.08740975\n",
      " 0.08740975 0.7668588 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07397679 0.07397679 0.76681066 0.07397679 0.07397679 0.76681066\n",
      " 0.07397679 0.07397679 0.76681066 0.07397679 0.07397679 0.07397679\n",
      " 0.76681066 0.07397679 0.07397679 0.07397679 0.07397679 0.76681066\n",
      " 0.76681066 0.07397679]\n",
      " After 0 training  step(s), loss on training batch is [0.08747098 0.76675236 0.08747098 0.76675236 0.08747098 0.08747098\n",
      " 0.76675236 0.76675236 0.08747098 0.76675236 0.76675236 0.08747098\n",
      " 0.08747098 0.76675236 0.08747098 0.08747098 0.76675236 0.76675236\n",
      " 0.76675236 0.76675236]\n",
      " After 0 training  step(s), loss on training batch is [0.76670235 0.76670235 0.76670235 0.76670235 0.09476955 0.76670235\n",
      " 0.76670235 0.76670235 0.09476955 0.09476955 0.09476955 0.76670235\n",
      " 0.76670235 0.09476955 0.76670235 0.09476955 0.76670235 0.76670235\n",
      " 0.09476955 0.76670235]\n",
      " After 0 training  step(s), loss on training batch is [0.7666528  0.7666528  0.7666528  0.7666528  0.08230392 0.08230392\n",
      " 0.08230392 0.7666528  0.08230392 0.08230392 0.7666528  0.08230392\n",
      " 0.7666528  0.7666528  0.7666528  0.08230392 0.08230392 0.08230392\n",
      " 0.7666528  0.7666528 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07352188 0.07352188 0.7666004  0.07352188 0.07352188 0.7666004\n",
      " 0.07352188 0.7666004  0.07352188 0.7666004  0.7666004  0.07352188\n",
      " 0.7666004  0.07352188 0.07352188 0.7666004  0.07352188 0.07352188\n",
      " 0.07352188 0.07352188]\n",
      " After 0 training  step(s), loss on training batch is [0.7665418  0.12624052 0.7665418  0.7665418  0.7665418  0.12624052\n",
      " 0.12624052 0.12624052 0.7665418  0.7665418  0.12624052 0.12624052\n",
      " 0.7665418  0.7665418  0.7665418  0.7665418  0.7665418  0.7665418\n",
      " 0.12624052 0.7665418 ]\n",
      " After 0 training  step(s), loss on training batch is [0.08844247 0.76649916 0.08844247 0.08844247 0.76649916 0.08844247\n",
      " 0.08844247 0.76649916 0.08844247 0.08844247 0.76649916 0.76649916\n",
      " 0.76649916 0.76649916 0.08844247 0.08844247 0.76649916 0.76649916\n",
      " 0.08844247 0.76649916]\n",
      " After 0 training  step(s), loss on training batch is [0.07370758 0.07370758 0.07370758 0.07370758 0.07370758 0.76645094\n",
      " 0.07370758 0.07370758 0.07370758 0.07370758 0.76645094 0.76645094\n",
      " 0.07370758 0.07370758 0.07370758 0.07370758 0.76645094 0.76645094\n",
      " 0.07370758 0.07370758]\n",
      " After 0 training  step(s), loss on training batch is [0.07597291 0.07597291 0.7663932  0.7663932  0.07597291 0.7663932\n",
      " 0.07597291 0.7663932  0.7663932  0.7663932  0.7663932  0.7663932\n",
      " 0.7663932  0.07597291 0.7663932  0.7663932  0.7663932  0.07597291\n",
      " 0.7663932  0.7663932 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07335029 0.07335029 0.07335029 0.7663363  0.07335029 0.07335029\n",
      " 0.07335029 0.07335029 0.7663363  0.07335029 0.07335029 0.7663363\n",
      " 0.07335029 0.7663363  0.07335029 0.7663363  0.07335029 0.07335029\n",
      " 0.07335029 0.07335029]\n",
      " After 0 training  step(s), loss on training batch is [0.76627815 0.07513372 0.07513372 0.76627815 0.07513372 0.07513372\n",
      " 0.76627815 0.76627815 0.76627815 0.76627815 0.76627815 0.07513372\n",
      " 0.07513372 0.76627815 0.07513372 0.76627815 0.07513372 0.76627815\n",
      " 0.07513372 0.07513372]\n",
      " After 0 training  step(s), loss on training batch is [0.11879471 0.11879471 0.11879471 0.11879471 0.7662219  0.7662219\n",
      " 0.7662219  0.11879471 0.11879471 0.7662219  0.7662219  0.11879471\n",
      " 0.11879471 0.7662219  0.7662219  0.11879471 0.7662219  0.7662219\n",
      " 0.7662219  0.7662219 ]\n",
      " After 0 training  step(s), loss on training batch is [0.76618254 0.76618254 0.76618254 0.76618254 0.07519403 0.76618254\n",
      " 0.76618254 0.07519403 0.76618254 0.07519403 0.07519403 0.76618254\n",
      " 0.76618254 0.07519403 0.07519403 0.76618254 0.07519403 0.76618254\n",
      " 0.07519403 0.76618254]\n",
      " After 0 training  step(s), loss on training batch is [0.766126   0.13862428 0.13862428 0.766126   0.766126   0.766126\n",
      " 0.766126   0.766126   0.766126   0.766126   0.13862428 0.13862428\n",
      " 0.766126   0.766126   0.766126   0.13862428 0.766126   0.13862428\n",
      " 0.13862428 0.766126  ]\n",
      " After 0 training  step(s), loss on training batch is [0.08081207 0.7660853  0.08081207 0.08081207 0.7660853  0.08081207\n",
      " 0.08081207 0.7660853  0.08081207 0.7660853  0.08081207 0.7660853\n",
      " 0.7660853  0.08081207 0.7660853  0.7660853  0.7660853  0.7660853\n",
      " 0.08081207 0.7660853 ]\n",
      " After 0 training  step(s), loss on training batch is [0.08034822 0.08034822 0.7660328  0.7660328  0.7660328  0.08034822\n",
      " 0.7660328  0.7660328  0.08034822 0.08034822 0.08034822 0.08034822\n",
      " 0.7660328  0.08034822 0.08034822 0.08034822 0.7660328  0.08034822\n",
      " 0.7660328  0.7660328 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7659813  0.7659813  0.08833423 0.7659813  0.7659813  0.7659813\n",
      " 0.08833423 0.7659813  0.08833423 0.7659813  0.08833423 0.7659813\n",
      " 0.08833423 0.7659813  0.08833423 0.08833423 0.08833423 0.08833423\n",
      " 0.7659813  0.7659813 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7659326  0.7659326  0.09510799 0.09510799 0.7659326  0.7659326\n",
      " 0.7659326  0.7659326  0.09510799 0.7659326  0.09510799 0.7659326\n",
      " 0.09510799 0.09510799 0.7659326  0.09510799 0.09510799 0.7659326\n",
      " 0.7659326  0.09510799]\n",
      " After 0 training  step(s), loss on training batch is [0.08126208 0.08126208 0.08126208 0.08126208 0.08126208 0.08126208\n",
      " 0.76588655 0.76588655 0.76588655 0.76588655 0.76588655 0.76588655\n",
      " 0.08126208 0.76588655 0.76588655 0.08126208 0.76588655 0.76588655\n",
      " 0.76588655 0.08126208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.76583457 0.09069591 0.09069591 0.76583457 0.09069591 0.09069591\n",
      " 0.76583457 0.09069591 0.76583457 0.09069591 0.09069591 0.76583457\n",
      " 0.76583457 0.76583457 0.76583457 0.09069591 0.76583457 0.76583457\n",
      " 0.09069591 0.76583457]\n",
      " After 0 training  step(s), loss on training batch is [0.76578695 0.08679288 0.76578695 0.76578695 0.76578695 0.08679288\n",
      " 0.08679288 0.76578695 0.08679288 0.08679288 0.76578695 0.76578695\n",
      " 0.76578695 0.08679288 0.76578695 0.76578695 0.76578695 0.08679288\n",
      " 0.76578695 0.08679288]\n",
      " After 0 training  step(s), loss on training batch is [0.07503752 0.07503752 0.76573676 0.07503752 0.76573676 0.76573676\n",
      " 0.07503752 0.07503752 0.07503752 0.76573676 0.76573676 0.07503752\n",
      " 0.07503752 0.76573676 0.07503752 0.07503752 0.07503752 0.07503752\n",
      " 0.76573676 0.76573676]\n",
      " After 0 training  step(s), loss on training batch is [0.07538501 0.7656818  0.7656818  0.07538501 0.7656818  0.07538501\n",
      " 0.07538501 0.7656818  0.07538501 0.07538501 0.07538501 0.07538501\n",
      " 0.07538501 0.07538501 0.07538501 0.7656818  0.7656818  0.7656818\n",
      " 0.07538501 0.7656818 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07446546 0.07446546 0.76562726 0.07446546 0.07446546 0.07446546\n",
      " 0.76562726 0.76562726 0.07446546 0.07446546 0.07446546 0.76562726\n",
      " 0.76562726 0.76562726 0.76562726 0.76562726 0.76562726 0.07446546\n",
      " 0.76562726 0.76562726]\n",
      " After 0 training  step(s), loss on training batch is [0.10809761 0.76557124 0.76557124 0.76557124 0.76557124 0.10809761\n",
      " 0.10809761 0.10809761 0.10809761 0.10809761 0.76557124 0.76557124\n",
      " 0.76557124 0.76557124 0.10809761 0.10809761 0.76557124 0.76557124\n",
      " 0.76557124 0.10809761]\n",
      " After 0 training  step(s), loss on training batch is [0.76552963 0.76552963 0.08744922 0.08744922 0.08744922 0.76552963\n",
      " 0.76552963 0.08744922 0.76552963 0.76552963 0.76552963 0.76552963\n",
      " 0.08744922 0.76552963 0.76552963 0.76552963 0.76552963 0.76552963\n",
      " 0.08744922 0.76552963]\n",
      " After 0 training  step(s), loss on training batch is [0.15078227 0.76547796 0.15078227 0.76547796 0.76547796 0.76547796\n",
      " 0.76547796 0.15078227 0.15078227 0.76547796 0.76547796 0.76547796\n",
      " 0.15078227 0.76547796 0.76547796 0.15078227 0.76547796 0.15078227\n",
      " 0.76547796 0.76547796]\n",
      " After 0 training  step(s), loss on training batch is [0.07878601 0.07878601 0.07878601 0.07878601 0.7654388  0.7654388\n",
      " 0.07878601 0.7654388  0.7654388  0.7654388  0.07878601 0.7654388\n",
      " 0.7654388  0.07878601 0.07878601 0.07878601 0.7654388  0.7654388\n",
      " 0.7654388  0.7654388 ]\n",
      " After 0 training  step(s), loss on training batch is [0.09914786 0.765386   0.09914786 0.765386   0.765386   0.765386\n",
      " 0.09914786 0.765386   0.09914786 0.09914786 0.09914786 0.09914786\n",
      " 0.09914786 0.765386   0.765386   0.765386   0.765386   0.765386\n",
      " 0.765386   0.09914786]\n",
      " After 0 training  step(s), loss on training batch is [0.7653418  0.7653418  0.10644618 0.7653418  0.7653418  0.10644618\n",
      " 0.7653418  0.10644618 0.10644618 0.10644618 0.10644618 0.7653418\n",
      " 0.7653418  0.10644618 0.7653418  0.10644618 0.7653418  0.10644619\n",
      " 0.7653418  0.10644619]\n",
      " After 0 training  step(s), loss on training batch is [0.76530164 0.76530164 0.08950268 0.08950268 0.76530164 0.76530164\n",
      " 0.08950268 0.76530164 0.08950268 0.76530164 0.08950268 0.76530164\n",
      " 0.08950268 0.08950268 0.08950268 0.08950268 0.76530164 0.76530164\n",
      " 0.76530164 0.76530164]\n",
      " After 0 training  step(s), loss on training batch is [0.08277553 0.08277553 0.7652541  0.7652541  0.7652541  0.08277553\n",
      " 0.7652541  0.08277553 0.08277553 0.08277553 0.08277553 0.08277553\n",
      " 0.08277553 0.7652541  0.08277553 0.08277553 0.7652541  0.08277553\n",
      " 0.7652541  0.7652541 ]\n",
      " After 0 training  step(s), loss on training batch is [0.765206   0.07378896 0.07378896 0.765206   0.07378896 0.07378896\n",
      " 0.07378896 0.07378896 0.07378896 0.07378896 0.07378896 0.765206\n",
      " 0.07378896 0.765206   0.07378896 0.07378896 0.07378896 0.765206\n",
      " 0.765206   0.765206  ]\n",
      " After 0 training  step(s), loss on training batch is [0.7651509  0.7651509  0.07346556 0.07346556 0.07346556 0.07346556\n",
      " 0.7651509  0.7651509  0.7651509  0.7651509  0.7651509  0.7651509\n",
      " 0.07346556 0.07346556 0.07346556 0.07346556 0.07346556 0.7651509\n",
      " 0.07346556 0.07346556]\n",
      " After 0 training  step(s), loss on training batch is [0.7650951  0.07878454 0.7650951  0.7650951  0.07878454 0.07878454\n",
      " 0.7650951  0.07878454 0.7650951  0.7650951  0.07878454 0.7650951\n",
      " 0.07878454 0.07878454 0.7650951  0.07878454 0.07878454 0.7650951\n",
      " 0.7650951  0.7650951 ]\n",
      " After 0 training  step(s), loss on training batch is [0.08459678 0.7650427  0.7650427  0.7650427  0.08459678 0.08459678\n",
      " 0.7650427  0.7650427  0.08459678 0.7650427  0.7650427  0.7650427\n",
      " 0.7650427  0.7650427  0.08459678 0.08459678 0.7650427  0.7650427\n",
      " 0.7650427  0.08459678]\n",
      " After 0 training  step(s), loss on training batch is [0.7649916 0.7649916 0.7649916 0.0733441 0.0733441 0.7649916 0.0733441\n",
      " 0.0733441 0.0733441 0.7649916 0.7649916 0.0733441 0.0733441 0.7649916\n",
      " 0.0733441 0.0733441 0.0733441 0.0733441 0.7649916 0.7649916]\n",
      " After 0 training  step(s), loss on training batch is [0.764936   0.07421249 0.07421249 0.07421249 0.764936   0.07421249\n",
      " 0.764936   0.07421249 0.07421249 0.764936   0.764936   0.07421249\n",
      " 0.07421249 0.764936   0.07421249 0.07421249 0.07421249 0.07421249\n",
      " 0.07421249 0.07421249]\n",
      " After 0 training  step(s), loss on training batch is [0.76488215 0.07560556 0.07560556 0.07560556 0.07560556 0.76488215\n",
      " 0.07560556 0.07560556 0.07560556 0.07560556 0.07560556 0.07560556\n",
      " 0.76488215 0.76488215 0.76488215 0.07560556 0.76488215 0.76488215\n",
      " 0.76488215 0.07560556]\n",
      " After 0 training  step(s), loss on training batch is [0.0727843  0.0727843  0.76482916 0.0727843  0.76482916 0.76482916\n",
      " 0.76482916 0.0727843  0.76482916 0.0727843  0.0727843  0.0727843\n",
      " 0.0727843  0.0727843  0.0727843  0.76482916 0.0727843  0.76482916\n",
      " 0.0727843  0.76482916]\n",
      " After 0 training  step(s), loss on training batch is [0.07187119 0.7647734  0.7647734  0.07187119 0.07187119 0.7647734\n",
      " 0.7647734  0.7647734  0.7647734  0.07187119 0.07187119 0.07187119\n",
      " 0.7647734  0.07187119 0.07187119 0.7647734  0.07187119 0.07187119\n",
      " 0.7647734  0.07187119]\n",
      " After 0 training  step(s), loss on training batch is [0.07960366 0.07960366 0.76471657 0.07960366 0.76471657 0.76471657\n",
      " 0.76471657 0.07960366 0.07960366 0.07960366 0.76471657 0.76471657\n",
      " 0.07960366 0.76471657 0.76471657 0.07960366 0.07960366 0.76471657\n",
      " 0.07960366 0.07960366]\n",
      " After 0 training  step(s), loss on training batch is [0.76466644 0.76466644 0.07283003 0.07283003 0.07283003 0.07283003\n",
      " 0.07283003 0.07283003 0.07283003 0.76466644 0.76466644 0.76466644\n",
      " 0.76466644 0.76466644 0.76466644 0.76466644 0.07283003 0.07283003\n",
      " 0.76466644 0.07283003]\n",
      " After 0 training  step(s), loss on training batch is [0.76461077 0.76461077 0.76461077 0.0827829  0.76461077 0.0827829\n",
      " 0.76461077 0.0827829  0.0827829  0.0827829  0.0827829  0.0827829\n",
      " 0.76461077 0.76461077 0.0827829  0.76461077 0.0827829  0.76461077\n",
      " 0.76461077 0.76461077]\n",
      " After 0 training  step(s), loss on training batch is [0.08551457 0.08551457 0.76456106 0.08551457 0.76456106 0.08551457\n",
      " 0.08551457 0.08551457 0.08551457 0.08551457 0.08551457 0.76456106\n",
      " 0.76456106 0.76456106 0.76456106 0.08551457 0.76456106 0.76456106\n",
      " 0.76456106 0.08551457]\n",
      " After 0 training  step(s), loss on training batch is [0.07287611 0.7645146  0.7645146  0.7645146  0.07287611 0.7645146\n",
      " 0.07287611 0.07287611 0.07287611 0.07287611 0.7645146  0.7645146\n",
      " 0.07287611 0.7645146  0.07287611 0.07287611 0.7645146  0.07287611\n",
      " 0.07287611 0.07287611]\n",
      " After 0 training  step(s), loss on training batch is [0.7644596  0.07345282 0.07345282 0.7644596  0.07345282 0.07345282\n",
      " 0.07345282 0.7644596  0.7644596  0.07345282 0.07345282 0.7644596\n",
      " 0.7644596  0.7644596  0.7644596  0.7644596  0.7644596  0.07345282\n",
      " 0.07345282 0.07345282]\n",
      " After 0 training  step(s), loss on training batch is [0.7644049  0.07630739 0.07630739 0.7644049  0.7644049  0.7644049\n",
      " 0.07630739 0.07630739 0.07630739 0.07630739 0.07630739 0.07630739\n",
      " 0.7644049  0.07630739 0.7644049  0.07630739 0.07630739 0.7644049\n",
      " 0.07630739 0.07630739]\n",
      " After 0 training  step(s), loss on training batch is [0.76435375 0.07471637 0.07471637 0.76435375 0.76435375 0.07471637\n",
      " 0.07471637 0.76435375 0.07471637 0.76435375 0.76435375 0.76435375\n",
      " 0.07471637 0.07471637 0.76435375 0.76435375 0.07471637 0.07471637\n",
      " 0.07471637 0.76435375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.07237764 0.07237764 0.07237764 0.07237764 0.07237764 0.7643002\n",
      " 0.7643002  0.07237764 0.7643002  0.07237764 0.07237764 0.7643002\n",
      " 0.7643002  0.7643002  0.7643002  0.7643002  0.7643002  0.07237764\n",
      " 0.7643002  0.07237764]\n",
      " After 0 training  step(s), loss on training batch is [0.76424474 0.76424474 0.07176301 0.76424474 0.07176301 0.07176301\n",
      " 0.76424474 0.76424474 0.76424474 0.07176301 0.07176301 0.76424474\n",
      " 0.07176301 0.07176301 0.07176301 0.76424474 0.07176301 0.07176301\n",
      " 0.07176301 0.76424474]\n",
      " After 0 training  step(s), loss on training batch is [0.07168566 0.7641888  0.7641888  0.07168566 0.07168566 0.7641888\n",
      " 0.7641888  0.7641888  0.7641888  0.07168566 0.7641888  0.7641888\n",
      " 0.07168566 0.7641888  0.07168566 0.7641888  0.07168566 0.07168566\n",
      " 0.07168566 0.7641888 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07147983 0.07147983 0.7641328  0.07147983 0.07147983 0.07147983\n",
      " 0.7641328  0.7641328  0.7641328  0.07147983 0.7641328  0.07147983\n",
      " 0.07147983 0.07147983 0.7641328  0.7641328  0.07147983 0.07147983\n",
      " 0.7641328  0.7641328 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07184587 0.76407677 0.76407677 0.07184587 0.07184587 0.76407677\n",
      " 0.07184587 0.76407677 0.07184587 0.76407677 0.76407677 0.07184587\n",
      " 0.76407677 0.07184587 0.76407677 0.76407677 0.07184587 0.76407677\n",
      " 0.76407677 0.07184587]\n",
      " After 0 training  step(s), loss on training batch is [0.09307739 0.09307739 0.76402104 0.76402104 0.76402104 0.76402104\n",
      " 0.76402104 0.76402104 0.76402104 0.76402104 0.09307739 0.76402104\n",
      " 0.09307739 0.76402104 0.09307739 0.76402104 0.76402104 0.76402104\n",
      " 0.76402104 0.09307739]\n",
      " After 0 training  step(s), loss on training batch is [0.07617682 0.07617682 0.7639723  0.7639723  0.07617682 0.7639723\n",
      " 0.7639723  0.7639723  0.7639723  0.07617682 0.7639723  0.7639723\n",
      " 0.7639723  0.07617682 0.07617682 0.07617682 0.7639723  0.7639723\n",
      " 0.7639723  0.7639723 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07091824 0.07091824 0.07091824 0.07091824 0.07091824 0.07091824\n",
      " 0.07091824 0.07091824 0.07091824 0.76391894 0.76391894 0.76391894\n",
      " 0.07091824 0.07091824 0.07091824 0.76391894 0.76391894 0.07091824\n",
      " 0.76391894 0.07091824]\n",
      " After 0 training  step(s), loss on training batch is [0.76386267 0.07317019 0.07317019 0.07317019 0.76386267 0.76386267\n",
      " 0.76386267 0.07317019 0.76386267 0.76386267 0.76386267 0.07317019\n",
      " 0.07317019 0.07317019 0.07317019 0.76386267 0.76386267 0.07317019\n",
      " 0.07317019 0.07317019]\n",
      " After 0 training  step(s), loss on training batch is [0.07392462 0.7638089  0.07392462 0.07392462 0.07392462 0.07392462\n",
      " 0.07392462 0.07392462 0.07392462 0.07392462 0.7638089  0.07392462\n",
      " 0.07392462 0.7638089  0.7638089  0.07392462 0.7638089  0.7638089\n",
      " 0.07392462 0.7638089 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07188087 0.07188087 0.7637565  0.7637565  0.07188087 0.7637565\n",
      " 0.07188087 0.07188087 0.7637565  0.7637565  0.07188087 0.7637565\n",
      " 0.07188087 0.07188087 0.07188087 0.7637565  0.7637565  0.7637565\n",
      " 0.7637565  0.7637565 ]\n",
      " After 0 training  step(s), loss on training batch is [0.76370144 0.76370144 0.07820235 0.07820235 0.76370144 0.76370144\n",
      " 0.76370144 0.76370144 0.76370144 0.07820235 0.07820235 0.07820235\n",
      " 0.76370144 0.07820235 0.76370144 0.76370144 0.07820235 0.07820235\n",
      " 0.76370144 0.76370144]\n",
      " After 0 training  step(s), loss on training batch is [0.76364994 0.76364994 0.0976904  0.76364994 0.76364994 0.0976904\n",
      " 0.76364994 0.76364994 0.76364994 0.0976904  0.0976904  0.0976904\n",
      " 0.0976904  0.76364994 0.76364994 0.76364994 0.0976904  0.0976904\n",
      " 0.76364994 0.76364994]\n",
      " After 0 training  step(s), loss on training batch is [0.07251631 0.76360554 0.07251631 0.07251631 0.07251631 0.76360554\n",
      " 0.76360554 0.07251631 0.76360554 0.76360554 0.76360554 0.07251631\n",
      " 0.07251631 0.07251631 0.76360554 0.07251631 0.07251631 0.76360554\n",
      " 0.76360554 0.07251631]\n",
      " After 0 training  step(s), loss on training batch is [0.07242658 0.76355165 0.76355165 0.76355165 0.76355165 0.76355165\n",
      " 0.07242658 0.07242658 0.76355165 0.76355165 0.76355165 0.07242658\n",
      " 0.76355165 0.07242658 0.07242658 0.76355165 0.07242658 0.07242658\n",
      " 0.07242658 0.76355165]\n",
      " After 0 training  step(s), loss on training batch is [0.7634973  0.08070955 0.7634973  0.08070955 0.08070955 0.7634973\n",
      " 0.7634973  0.7634973  0.08070955 0.7634973  0.7634973  0.08070955\n",
      " 0.7634973  0.08070955 0.7634973  0.08070955 0.7634973  0.7634973\n",
      " 0.7634973  0.7634973 ]\n",
      " After 0 training  step(s), loss on training batch is [0.76344645 0.76344645 0.07336384 0.76344645 0.07336384 0.76344645\n",
      " 0.07336384 0.76344645 0.07336384 0.76344645 0.07336384 0.76344645\n",
      " 0.76344645 0.07336384 0.76344645 0.07336384 0.76344645 0.07336384\n",
      " 0.07336384 0.07336384]\n",
      " After 0 training  step(s), loss on training batch is [0.7633933  0.7633933  0.7633933  0.7633933  0.07600278 0.07600278\n",
      " 0.07600278 0.7633933  0.07600278 0.7633933  0.7633933  0.7633933\n",
      " 0.7633933  0.07600278 0.7633933  0.7633933  0.07600278 0.7633933\n",
      " 0.7633933  0.7633933 ]\n",
      " After 0 training  step(s), loss on training batch is [0.76334006 0.76334006 0.08021142 0.76334006 0.08021142 0.08021142\n",
      " 0.08021142 0.76334006 0.76334006 0.76334006 0.76334006 0.08021142\n",
      " 0.08021142 0.76334006 0.76334006 0.76334006 0.08021142 0.76334006\n",
      " 0.76334006 0.76334006]\n",
      " After 0 training  step(s), loss on training batch is [0.07647295 0.7632892  0.7632892  0.07647295 0.7632892  0.07647295\n",
      " 0.7632892  0.7632892  0.7632892  0.07647295 0.7632892  0.7632892\n",
      " 0.7632892  0.07647295 0.07647295 0.07647295 0.07647295 0.7632892\n",
      " 0.07647295 0.7632892 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07186135 0.07186135 0.76323795 0.76323795 0.07186135 0.07186135\n",
      " 0.07186135 0.07186135 0.76323795 0.76323795 0.76323795 0.76323795\n",
      " 0.07186135 0.76323795 0.76323795 0.07186135 0.07186135 0.07186135\n",
      " 0.76323795 0.07186135]\n",
      " After 0 training  step(s), loss on training batch is [0.10493914 0.763184   0.763184   0.10493914 0.763184   0.763184\n",
      " 0.10493914 0.10493914 0.763184   0.763184   0.763184   0.763184\n",
      " 0.10493914 0.763184   0.763184   0.10493914 0.763184   0.763184\n",
      " 0.10493913 0.10493913]\n",
      " After 0 training  step(s), loss on training batch is [0.76314193 0.76314193 0.07067866 0.07067866 0.07067866 0.76314193\n",
      " 0.76314193 0.07067866 0.07067866 0.76314193 0.07067866 0.07067866\n",
      " 0.76314193 0.07067866 0.07067866 0.07067866 0.07067866 0.07067866\n",
      " 0.76314193 0.76314193]\n",
      " After 0 training  step(s), loss on training batch is [0.07477694 0.07477694 0.76308703 0.07477694 0.76308703 0.07477694\n",
      " 0.76308703 0.07477694 0.07477694 0.76308703 0.76308703 0.76308703\n",
      " 0.07477694 0.76308703 0.76308703 0.76308703 0.07477694 0.76308703\n",
      " 0.07477694 0.07477694]\n",
      " After 0 training  step(s), loss on training batch is [0.7630354  0.07486676 0.07486676 0.07486676 0.7630354  0.7630354\n",
      " 0.07486676 0.07486676 0.7630354  0.07486676 0.7630354  0.07486676\n",
      " 0.7630354  0.07486676 0.7630354  0.7630354  0.7630354  0.07486676\n",
      " 0.7630354  0.07486676]\n",
      " After 0 training  step(s), loss on training batch is [0.07011833 0.07011833 0.07011833 0.762984   0.07011833 0.07011833\n",
      " 0.762984   0.07011833 0.07011833 0.07011833 0.07011833 0.762984\n",
      " 0.762984   0.762984   0.07011833 0.07011833 0.07011833 0.07011833\n",
      " 0.07011833 0.07011833]\n",
      " After 0 training  step(s), loss on training batch is [0.07619455 0.07619455 0.7629287  0.07619455 0.7629287  0.7629287\n",
      " 0.7629287  0.7629287  0.7629287  0.7629287  0.7629287  0.7629287\n",
      " 0.7629287  0.7629287  0.07619455 0.7629287  0.7629287  0.07619455\n",
      " 0.07619455 0.7629287 ]\n",
      " After 0 training  step(s), loss on training batch is [0.08081339 0.08081339 0.08081339 0.08081339 0.76287615 0.08081339\n",
      " 0.76287615 0.76287615 0.76287615 0.76287615 0.76287615 0.76287615\n",
      " 0.08081339 0.76287615 0.76287615 0.76287615 0.76287615 0.08081339\n",
      " 0.76287615 0.08081339]\n",
      " After 0 training  step(s), loss on training batch is [0.7628269  0.07457555 0.07457555 0.7628269  0.7628269  0.07457555\n",
      " 0.07457555 0.07457555 0.07457555 0.7628269  0.07457555 0.07457555\n",
      " 0.7628269  0.7628269  0.7628269  0.7628269  0.07457555 0.07457555\n",
      " 0.7628269  0.07457555]\n",
      " After 0 training  step(s), loss on training batch is [0.07011494 0.07011494 0.07011494 0.07011494 0.07011494 0.762776\n",
      " 0.07011494 0.07011494 0.762776   0.07011494 0.07011494 0.07011494\n",
      " 0.07011494 0.762776   0.762776   0.762776   0.762776   0.07011494\n",
      " 0.07011494 0.762776  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.07033316 0.7627212  0.07033316 0.07033316 0.07033316 0.07033316\n",
      " 0.7627212  0.07033316 0.07033316 0.07033316 0.7627212  0.07033316\n",
      " 0.7627212  0.07033316 0.7627212  0.07033316 0.07033316 0.7627212\n",
      " 0.07033316 0.07033316]\n",
      " After 0 training  step(s), loss on training batch is [0.7626669  0.7626669  0.08004357 0.7626669  0.7626669  0.7626669\n",
      " 0.08004357 0.7626669  0.7626669  0.08004357 0.7626669  0.7626669\n",
      " 0.7626669  0.08004357 0.7626669  0.08004357 0.08004357 0.08004357\n",
      " 0.08004357 0.08004357]\n",
      " After 0 training  step(s), loss on training batch is [0.07159565 0.7626183  0.07159565 0.07159565 0.07159565 0.07159565\n",
      " 0.7626183  0.07159565 0.7626183  0.7626183  0.7626183  0.07159565\n",
      " 0.7626183  0.07159565 0.07159565 0.07159565 0.7626183  0.7626183\n",
      " 0.7626183  0.07159565]\n",
      " After 0 training  step(s), loss on training batch is [0.06988592 0.06988592 0.7625652  0.06988592 0.06988592 0.06988592\n",
      " 0.7625652  0.7625652  0.06988592 0.06988592 0.06988592 0.06988592\n",
      " 0.06988592 0.06988592 0.7625652  0.7625652  0.7625652  0.06988592\n",
      " 0.7625652  0.06988592]\n",
      " After 0 training  step(s), loss on training batch is [0.76251054 0.76251054 0.07313882 0.76251054 0.07313882 0.76251054\n",
      " 0.07313882 0.76251054 0.07313882 0.07313882 0.76251054 0.07313882\n",
      " 0.07313882 0.07313882 0.76251054 0.07313882 0.07313882 0.76251054\n",
      " 0.76251054 0.07313882]\n",
      " After 0 training  step(s), loss on training batch is [0.07985066 0.762459   0.762459   0.07985066 0.762459   0.07985066\n",
      " 0.762459   0.07985066 0.762459   0.07985066 0.762459   0.07985066\n",
      " 0.07985066 0.07985066 0.762459   0.762459   0.762459   0.762459\n",
      " 0.07985066 0.07985066]\n",
      " After 0 training  step(s), loss on training batch is [0.06949016 0.06949016 0.76241136 0.06949016 0.06949016 0.06949016\n",
      " 0.76241136 0.06949016 0.76241136 0.76241136 0.06949016 0.06949016\n",
      " 0.06949016 0.76241136 0.06949016 0.76241136 0.06949016 0.06949016\n",
      " 0.76241136 0.06949016]\n",
      " After 0 training  step(s), loss on training batch is [0.7623564  0.7623564  0.07470061 0.7623564  0.07470061 0.07470061\n",
      " 0.7623564  0.07470061 0.7623564  0.7623564  0.7623564  0.7623564\n",
      " 0.7623564  0.7623564  0.07470061 0.07470061 0.07470061 0.7623564\n",
      " 0.07470061 0.07470061]\n",
      " After 0 training  step(s), loss on training batch is [0.06934625 0.06934625 0.7623053  0.06934625 0.06934625 0.06934625\n",
      " 0.06934625 0.06934625 0.06934625 0.7623053  0.06934625 0.7623053\n",
      " 0.7623053  0.06934625 0.7623053  0.7623053  0.06934625 0.7623053\n",
      " 0.06934625 0.06934625]\n",
      " After 0 training  step(s), loss on training batch is [0.13405052 0.7622504  0.7622504  0.7622504  0.7622504  0.7622504\n",
      " 0.7622504  0.7622504  0.13405052 0.7622504  0.13405052 0.7622504\n",
      " 0.7622504  0.7622504  0.7622504  0.7622504  0.7622504  0.7622504\n",
      " 0.13405052 0.7622504 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07013696 0.07013696 0.07013696 0.07013696 0.07013696 0.7622046\n",
      " 0.7622046  0.7622046  0.07013696 0.7622046  0.07013696 0.07013696\n",
      " 0.07013696 0.07013696 0.7622046  0.07013696 0.7622046  0.07013696\n",
      " 0.7622046  0.7622046 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06944384 0.7621509  0.7621509  0.06944384 0.06944384 0.7621509\n",
      " 0.7621509  0.06944384 0.7621509  0.06944384 0.06944384 0.06944384\n",
      " 0.7621509  0.06944384 0.06944384 0.7621509  0.7621509  0.06944384\n",
      " 0.06944384 0.7621509 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07861182 0.76209635 0.76209635 0.76209635 0.76209635 0.07861182\n",
      " 0.07861182 0.07861182 0.07861182 0.07861182 0.76209635 0.76209635\n",
      " 0.76209635 0.76209635 0.07861182 0.07861182 0.07861182 0.76209635\n",
      " 0.07861182 0.76209635]\n",
      " After 0 training  step(s), loss on training batch is [0.76204854 0.76204854 0.76204854 0.07925218 0.76204854 0.76204854\n",
      " 0.76204854 0.76204854 0.07925218 0.07925218 0.76204854 0.76204854\n",
      " 0.76204854 0.76204854 0.07925218 0.07925218 0.07925218 0.07925218\n",
      " 0.07925218 0.76204854]\n",
      " After 0 training  step(s), loss on training batch is [0.76199955 0.06983454 0.76199955 0.06983454 0.06983454 0.76199955\n",
      " 0.76199955 0.06983454 0.06983454 0.76199955 0.76199955 0.76199955\n",
      " 0.76199955 0.76199955 0.06983454 0.06983454 0.06983454 0.06983454\n",
      " 0.06983454 0.76199955]\n",
      " After 0 training  step(s), loss on training batch is [0.07470407 0.07470407 0.07470407 0.76194566 0.76194566 0.07470407\n",
      " 0.76194566 0.76194566 0.76194566 0.07470407 0.76194566 0.07470407\n",
      " 0.07470407 0.76194566 0.07470407 0.07470407 0.07470407 0.76194566\n",
      " 0.07470407 0.07470407]\n",
      " After 0 training  step(s), loss on training batch is [0.7618967  0.07839932 0.07839932 0.7618967  0.7618967  0.07839932\n",
      " 0.7618967  0.07839932 0.7618967  0.07839932 0.07839932 0.7618967\n",
      " 0.07839932 0.07839932 0.7618967  0.7618967  0.07839932 0.7618967\n",
      " 0.7618967  0.07839932]\n",
      " After 0 training  step(s), loss on training batch is [0.07052064 0.76184905 0.76184905 0.07052064 0.07052064 0.76184905\n",
      " 0.07052064 0.07052064 0.76184905 0.76184905 0.07052064 0.07052064\n",
      " 0.07052064 0.07052064 0.76184905 0.07052064 0.76184905 0.07052064\n",
      " 0.76184905 0.76184905]\n",
      " After 0 training  step(s), loss on training batch is [0.76179624 0.76179624 0.09682303 0.09682303 0.76179624 0.76179624\n",
      " 0.76179624 0.76179624 0.09682303 0.09682303 0.09682303 0.09682303\n",
      " 0.76179624 0.76179624 0.76179624 0.76179624 0.76179624 0.09682303\n",
      " 0.76179624 0.76179624]\n",
      " After 0 training  step(s), loss on training batch is [0.76175195 0.07954979 0.07954979 0.07954979 0.76175195 0.76175195\n",
      " 0.76175195 0.07954979 0.76175195 0.07954979 0.07954979 0.76175195\n",
      " 0.07954979 0.76175195 0.76175195 0.76175195 0.07954979 0.76175195\n",
      " 0.76175195 0.07954979]\n",
      " After 0 training  step(s), loss on training batch is [0.7617042  0.07066139 0.07066139 0.07066139 0.7617042  0.07066139\n",
      " 0.7617042  0.07066139 0.07066139 0.07066139 0.07066139 0.07066139\n",
      " 0.7617042  0.07066139 0.7617042  0.7617042  0.07066139 0.07066139\n",
      " 0.7617042  0.7617042 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06902273 0.06902273 0.7616521  0.06902273 0.06902273 0.7616521\n",
      " 0.7616521  0.06902273 0.06902273 0.06902273 0.7616521  0.7616521\n",
      " 0.7616521  0.06902273 0.06902273 0.06902273 0.7616521  0.06902273\n",
      " 0.7616521  0.06902273]\n",
      " After 0 training  step(s), loss on training batch is [0.07097713 0.07097713 0.7615981  0.07097713 0.07097713 0.7615981\n",
      " 0.07097713 0.7615981  0.7615981  0.7615981  0.7615981  0.7615981\n",
      " 0.07097713 0.7615981  0.07097713 0.07097713 0.7615981  0.7615981\n",
      " 0.7615981  0.07097713]\n",
      " After 0 training  step(s), loss on training batch is [0.7615457  0.07193315 0.07193315 0.7615457  0.07193315 0.07193315\n",
      " 0.07193315 0.7615457  0.07193315 0.7615457  0.7615457  0.07193315\n",
      " 0.7615457  0.07193315 0.7615457  0.7615457  0.7615457  0.7615457\n",
      " 0.7615457  0.07193315]\n",
      " After 0 training  step(s), loss on training batch is [0.76149404 0.76149404 0.76149404 0.76149404 0.76149404 0.76149404\n",
      " 0.76149404 0.10763481 0.10763481 0.76149404 0.10763481 0.76149404\n",
      " 0.10763481 0.76149404 0.10763481 0.10763481 0.76149404 0.10763481\n",
      " 0.76149404 0.10763481]\n",
      " After 0 training  step(s), loss on training batch is [0.76145405 0.76145405 0.07014302 0.07014302 0.07014302 0.07014302\n",
      " 0.76145405 0.07014302 0.76145405 0.76145405 0.76145405 0.76145405\n",
      " 0.76145405 0.07014302 0.76145405 0.76145405 0.07014302 0.07014302\n",
      " 0.76145405 0.76145405]\n",
      " After 0 training  step(s), loss on training batch is [0.761401   0.07257807 0.761401   0.761401   0.07257807 0.761401\n",
      " 0.761401   0.07257807 0.761401   0.761401   0.07257807 0.761401\n",
      " 0.07257807 0.761401   0.07257807 0.07257807 0.07257807 0.761401\n",
      " 0.761401   0.07257807]\n",
      " After 0 training  step(s), loss on training batch is [0.76135    0.76135    0.07228107 0.76135    0.07228107 0.76135\n",
      " 0.07228107 0.07228107 0.76135    0.07228107 0.76135    0.07228107\n",
      " 0.07228107 0.07228107 0.76135    0.07228107 0.07228107 0.76135\n",
      " 0.76135    0.07228107]\n",
      " After 0 training  step(s), loss on training batch is [0.10399777 0.76129955 0.76129955 0.76129955 0.76129955 0.76129955\n",
      " 0.10399777 0.76129955 0.76129955 0.10399777 0.76129955 0.10399777\n",
      " 0.76129955 0.76129955 0.10399777 0.10399777 0.76129955 0.76129955\n",
      " 0.10399777 0.76129955]\n",
      " After 0 training  step(s), loss on training batch is [0.7612572  0.07601503 0.7612572  0.7612572  0.07601503 0.7612572\n",
      " 0.7612572  0.07601503 0.7612572  0.7612572  0.07601503 0.7612572\n",
      " 0.7612572  0.07601503 0.07601503 0.07601503 0.7612572  0.7612572\n",
      " 0.7612572  0.7612572 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.7612071  0.07307876 0.07307876 0.7612071  0.07307876 0.07307876\n",
      " 0.7612071  0.07307876 0.7612071  0.07307876 0.7612071  0.07307876\n",
      " 0.07307876 0.07307876 0.7612071  0.07307876 0.7612071  0.7612071\n",
      " 0.07307876 0.07307876]\n",
      " After 0 training  step(s), loss on training batch is [0.06855758 0.06855758 0.761158   0.06855758 0.06855758 0.761158\n",
      " 0.761158   0.06855758 0.06855758 0.06855758 0.06855758 0.06855758\n",
      " 0.761158   0.06855758 0.761158   0.06855758 0.06855758 0.761158\n",
      " 0.06855758 0.761158  ]\n",
      " After 0 training  step(s), loss on training batch is [0.07563858 0.7611045  0.7611045  0.7611045  0.7611045  0.7611045\n",
      " 0.07563858 0.7611045  0.7611045  0.07563858 0.07563858 0.07563858\n",
      " 0.07563858 0.07563858 0.7611045  0.07563858 0.07563858 0.07563858\n",
      " 0.7611045  0.7611045 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06863686 0.06863686 0.7610563  0.06863686 0.06863686 0.7610563\n",
      " 0.06863686 0.06863686 0.7610563  0.7610563  0.06863686 0.06863686\n",
      " 0.06863686 0.7610563  0.7610563  0.7610563  0.7610563  0.06863686\n",
      " 0.06863686 0.7610563 ]\n",
      " After 0 training  step(s), loss on training batch is [0.761003   0.07870747 0.761003   0.07870747 0.761003   0.761003\n",
      " 0.761003   0.761003   0.07870747 0.761003   0.07870747 0.761003\n",
      " 0.761003   0.07870747 0.761003   0.761003   0.07870747 0.761003\n",
      " 0.761003   0.761003  ]\n",
      " After 0 training  step(s), loss on training batch is [0.06885035 0.7609534  0.7609534  0.7609534  0.06885035 0.06885035\n",
      " 0.7609534  0.06885035 0.06885035 0.7609534  0.7609534  0.7609534\n",
      " 0.06885035 0.06885035 0.06885035 0.06885035 0.06885035 0.06885035\n",
      " 0.7609534  0.7609534 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7609006  0.07081763 0.07081763 0.7609006  0.7609006  0.07081763\n",
      " 0.07081763 0.07081763 0.07081763 0.07081763 0.07081763 0.07081763\n",
      " 0.07081763 0.7609006  0.07081763 0.7609006  0.07081763 0.7609006\n",
      " 0.7609006  0.07081763]\n",
      " After 0 training  step(s), loss on training batch is [0.7608503  0.07151223 0.07151223 0.07151223 0.7608503  0.07151223\n",
      " 0.07151223 0.07151223 0.07151223 0.07151223 0.07151223 0.07151223\n",
      " 0.07151223 0.7608503  0.07151223 0.7608503  0.7608503  0.7608503\n",
      " 0.7608503  0.07151223]\n",
      " After 0 training  step(s), loss on training batch is [0.07058176 0.76080084 0.76080084 0.76080084 0.07058176 0.76080084\n",
      " 0.76080084 0.07058176 0.07058176 0.07058176 0.76080084 0.07058176\n",
      " 0.07058176 0.76080084 0.76080084 0.07058176 0.07058176 0.07058176\n",
      " 0.76080084 0.07058176]\n",
      " After 0 training  step(s), loss on training batch is [0.7607499  0.07914445 0.7607499  0.7607499  0.7607499  0.7607499\n",
      " 0.7607499  0.07914445 0.07914445 0.07914445 0.07914445 0.7607499\n",
      " 0.7607499  0.07914445 0.07914445 0.07914445 0.7607499  0.7607499\n",
      " 0.07914445 0.07914445]\n",
      " After 0 training  step(s), loss on training batch is [0.07054705 0.76070404 0.76070404 0.76070404 0.07054705 0.76070404\n",
      " 0.07054705 0.76070404 0.76070404 0.07054705 0.76070404 0.07054705\n",
      " 0.76070404 0.76070404 0.76070404 0.76070404 0.76070404 0.07054705\n",
      " 0.07054705 0.07054705]\n",
      " After 0 training  step(s), loss on training batch is [0.07056859 0.07056859 0.76065236 0.76065236 0.76065236 0.07056859\n",
      " 0.07056859 0.76065236 0.76065236 0.07056859 0.07056859 0.76065236\n",
      " 0.07056859 0.76065236 0.07056859 0.76065236 0.07056859 0.76065236\n",
      " 0.07056859 0.76065236]\n",
      " After 0 training  step(s), loss on training batch is [0.07401857 0.7606014  0.07401857 0.7606014  0.7606014  0.7606014\n",
      " 0.07401857 0.7606014  0.07401857 0.7606014  0.07401857 0.07401857\n",
      " 0.07401857 0.7606014  0.7606014  0.07401857 0.07401857 0.07401857\n",
      " 0.07401857 0.7606014 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07961638 0.7605534  0.7605534  0.07961638 0.7605534  0.7605534\n",
      " 0.7605534  0.07961638 0.7605534  0.07961638 0.07961638 0.07961638\n",
      " 0.7605534  0.7605534  0.07961638 0.7605534  0.7605534  0.7605534\n",
      " 0.7605534  0.07961638]\n",
      " After 0 training  step(s), loss on training batch is [0.06897457 0.06897457 0.06897457 0.06897457 0.06897457 0.76050633\n",
      " 0.06897457 0.06897457 0.06897457 0.76050633 0.06897457 0.76050633\n",
      " 0.06897457 0.76050633 0.76050633 0.76050633 0.76050633 0.76050633\n",
      " 0.76050633 0.76050633]\n",
      " After 0 training  step(s), loss on training batch is [0.07235759 0.76045424 0.07235759 0.76045424 0.07235759 0.07235759\n",
      " 0.07235759 0.76045424 0.76045424 0.76045424 0.76045424 0.07235759\n",
      " 0.07235759 0.76045424 0.76045424 0.76045424 0.76045424 0.07235759\n",
      " 0.76045424 0.76045424]\n",
      " After 0 training  step(s), loss on training batch is [0.760404   0.08410731 0.760404   0.08410731 0.760404   0.760404\n",
      " 0.760404   0.760404   0.08410731 0.08410731 0.760404   0.08410731\n",
      " 0.760404   0.760404   0.760404   0.08410731 0.08410731 0.08410731\n",
      " 0.760404   0.760404  ]\n",
      " After 0 training  step(s), loss on training batch is [0.76035875 0.06840543 0.76035875 0.06840543 0.06840543 0.06840543\n",
      " 0.76035875 0.06840543 0.06840543 0.06840543 0.06840543 0.06840543\n",
      " 0.76035875 0.06840543 0.76035875 0.76035875 0.76035875 0.06840543\n",
      " 0.76035875 0.76035875]\n",
      " After 0 training  step(s), loss on training batch is [0.06914482 0.06914482 0.06914482 0.06914482 0.76030654 0.76030654\n",
      " 0.06914482 0.76030654 0.06914482 0.76030654 0.06914482 0.76030654\n",
      " 0.06914482 0.76030654 0.06914482 0.76030654 0.76030654 0.76030654\n",
      " 0.06914482 0.06914482]\n",
      " After 0 training  step(s), loss on training batch is [0.7602551 0.0714018 0.0714018 0.0714018 0.7602551 0.7602551 0.7602551\n",
      " 0.0714018 0.0714018 0.7602551 0.0714018 0.7602551 0.7602551 0.7602551\n",
      " 0.0714018 0.7602551 0.7602551 0.0714018 0.0714018 0.7602551]\n",
      " After 0 training  step(s), loss on training batch is [0.06854942 0.760205   0.760205   0.760205   0.06854942 0.06854942\n",
      " 0.760205   0.760205   0.760205   0.760205   0.06854942 0.06854942\n",
      " 0.06854942 0.760205   0.06854942 0.760205   0.760205   0.06854942\n",
      " 0.06854942 0.06854942]\n",
      " After 0 training  step(s), loss on training batch is [0.11984302 0.760153   0.760153   0.760153   0.760153   0.760153\n",
      " 0.760153   0.11984302 0.11984302 0.760153   0.760153   0.760153\n",
      " 0.760153   0.760153   0.760153   0.11984302 0.760153   0.11984302\n",
      " 0.11984302 0.760153  ]\n",
      " After 0 training  step(s), loss on training batch is [0.07054459 0.07054459 0.760112   0.760112   0.07054459 0.07054459\n",
      " 0.07054459 0.760112   0.760112   0.07054459 0.760112   0.760112\n",
      " 0.07054459 0.760112   0.07054459 0.07054459 0.760112   0.760112\n",
      " 0.760112   0.07054459]\n",
      " After 0 training  step(s), loss on training batch is [0.06772614 0.06772614 0.06772614 0.76006186 0.06772614 0.76006186\n",
      " 0.06772614 0.76006186 0.06772614 0.06772614 0.76006186 0.76006186\n",
      " 0.06772614 0.76006186 0.76006186 0.76006186 0.06772614 0.06772614\n",
      " 0.76006186 0.76006186]\n",
      " After 0 training  step(s), loss on training batch is [0.7600093  0.09426866 0.09426866 0.7600093  0.7600093  0.7600093\n",
      " 0.7600093  0.7600093  0.09426866 0.09426866 0.7600093  0.7600093\n",
      " 0.7600093  0.7600093  0.7600093  0.09426866 0.09426866 0.7600093\n",
      " 0.09426866 0.09426866]\n",
      " After 0 training  step(s), loss on training batch is [0.7599675  0.7599675  0.07413916 0.7599675  0.07413916 0.07413916\n",
      " 0.07413916 0.7599675  0.07413916 0.7599675  0.07413916 0.07413916\n",
      " 0.7599675  0.07413916 0.7599675  0.07413916 0.7599675  0.7599675\n",
      " 0.7599675  0.07413916]\n",
      " After 0 training  step(s), loss on training batch is [0.06695037 0.06695037 0.06695037 0.06695037 0.06695037 0.75991994\n",
      " 0.75991994 0.75991994 0.06695037 0.06695037 0.06695037 0.75991994\n",
      " 0.75991994 0.06695037 0.75991994 0.06695037 0.06695037 0.06695037\n",
      " 0.06695037 0.06695037]\n",
      " After 0 training  step(s), loss on training batch is [0.06896812 0.75986695 0.06896812 0.75986695 0.06896812 0.75986695\n",
      " 0.75986695 0.06896812 0.06896812 0.75986695 0.75986695 0.06896812\n",
      " 0.06896812 0.06896812 0.75986695 0.06896812 0.06896812 0.75986695\n",
      " 0.06896812 0.75986695]\n",
      " After 0 training  step(s), loss on training batch is [0.06800201 0.7598161  0.06800201 0.06800201 0.06800201 0.7598161\n",
      " 0.06800201 0.7598161  0.7598161  0.7598161  0.06800201 0.06800201\n",
      " 0.7598161  0.7598161  0.7598161  0.7598161  0.7598161  0.06800201\n",
      " 0.06800201 0.7598161 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06688529 0.06688529 0.06688529 0.7597642  0.06688529 0.06688529\n",
      " 0.7597642  0.7597642  0.06688529 0.06688529 0.7597642  0.7597642\n",
      " 0.06688529 0.06688529 0.06688529 0.7597642  0.7597642  0.06688529\n",
      " 0.06688529 0.06688529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.06800228 0.06800228 0.06800228 0.7597114  0.7597114  0.06800228\n",
      " 0.06800228 0.06800228 0.7597114  0.06800228 0.06800228 0.7597114\n",
      " 0.06800228 0.06800228 0.06800228 0.7597114  0.06800228 0.7597114\n",
      " 0.7597114  0.7597114 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06766358 0.06766358 0.06766358 0.7596601  0.7596601  0.06766358\n",
      " 0.06766358 0.06766358 0.06766358 0.06766358 0.7596601  0.7596601\n",
      " 0.7596601  0.06766358 0.06766358 0.7596601  0.7596601  0.06766358\n",
      " 0.06766358 0.7596601 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7596085  0.07816592 0.07816592 0.7596085  0.7596085  0.7596085\n",
      " 0.7596085  0.07816592 0.7596085  0.7596085  0.7596085  0.07816592\n",
      " 0.07816592 0.7596085  0.7596085  0.7596085  0.07816592 0.07816592\n",
      " 0.07816592 0.7596085 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07948352 0.75956196 0.75956196 0.75956196 0.07948352 0.07948352\n",
      " 0.75956196 0.07948352 0.75956196 0.75956196 0.07948352 0.75956196\n",
      " 0.75956196 0.75956196 0.75956196 0.75956196 0.75956196 0.75956196\n",
      " 0.75956196 0.07948352]\n",
      " After 0 training  step(s), loss on training batch is [0.7595142  0.7595142  0.07209895 0.7595142  0.07209895 0.7595142\n",
      " 0.7595142  0.07209895 0.07209895 0.07209895 0.07209895 0.7595142\n",
      " 0.07209895 0.7595142  0.7595142  0.07209895 0.7595142  0.07209895\n",
      " 0.07209895 0.07209895]\n",
      " After 0 training  step(s), loss on training batch is [0.75946647 0.75946647 0.06836307 0.06836307 0.06836307 0.06836307\n",
      " 0.75946647 0.75946647 0.75946647 0.75946647 0.75946647 0.06836307\n",
      " 0.06836307 0.06836307 0.06836307 0.06836307 0.75946647 0.06836307\n",
      " 0.75946647 0.06836307]\n",
      " After 0 training  step(s), loss on training batch is [0.7594158  0.7594158  0.7594158  0.7594158  0.06923075 0.7594158\n",
      " 0.06923075 0.06923075 0.06923075 0.7594158  0.7594158  0.06923075\n",
      " 0.7594158  0.7594158  0.7594158  0.7594158  0.06923075 0.06923075\n",
      " 0.7594158  0.06923075]\n",
      " After 0 training  step(s), loss on training batch is [0.0698268  0.75936514 0.75936514 0.75936514 0.0698268  0.0698268\n",
      " 0.75936514 0.0698268  0.0698268  0.75936514 0.0698268  0.75936514\n",
      " 0.0698268  0.75936514 0.0698268  0.75936514 0.75936514 0.75936514\n",
      " 0.75936514 0.0698268 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07755916 0.07755916 0.7593152  0.07755916 0.7593152  0.07755916\n",
      " 0.07755916 0.7593152  0.07755916 0.07755916 0.07755916 0.7593152\n",
      " 0.7593152  0.7593152  0.7593152  0.7593152  0.7593152  0.7593152\n",
      " 0.7593152  0.7593152 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06660652 0.75926876 0.06660652 0.75926876 0.06660652 0.75926876\n",
      " 0.75926876 0.06660652 0.06660652 0.06660652 0.75926876 0.75926876\n",
      " 0.06660652 0.06660652 0.06660652 0.06660652 0.06660652 0.06660652\n",
      " 0.75926876 0.06660652]\n",
      " After 0 training  step(s), loss on training batch is [0.7592167  0.7592167  0.07526336 0.7592167  0.07526336 0.07526336\n",
      " 0.07526336 0.7592167  0.7592167  0.07526336 0.7592167  0.7592167\n",
      " 0.07526336 0.7592167  0.07526336 0.07526336 0.7592167  0.7592167\n",
      " 0.07526336 0.7592167 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7591701  0.7591701  0.06941118 0.7591701  0.7591701  0.06941118\n",
      " 0.06941118 0.06941118 0.7591701  0.7591701  0.06941118 0.7591701\n",
      " 0.7591701  0.06941118 0.06941118 0.7591701  0.06941118 0.7591701\n",
      " 0.06941118 0.06941118]\n",
      " After 0 training  step(s), loss on training batch is [0.7591205  0.7591205  0.7591205  0.06665702 0.06665702 0.7591205\n",
      " 0.7591205  0.06665702 0.7591205  0.7591205  0.06665702 0.7591205\n",
      " 0.06665702 0.06665702 0.06665702 0.7591205  0.06665702 0.7591205\n",
      " 0.06665702 0.06665702]\n",
      " After 0 training  step(s), loss on training batch is [0.7590686  0.06687181 0.06687181 0.06687181 0.06687181 0.7590686\n",
      " 0.7590686  0.7590686  0.7590686  0.06687181 0.7590686  0.06687181\n",
      " 0.7590686  0.06687181 0.06687181 0.06687181 0.7590686  0.06687181\n",
      " 0.06687181 0.7590686 ]\n",
      " After 0 training  step(s), loss on training batch is [0.75901717 0.07270955 0.75901717 0.75901717 0.75901717 0.07270955\n",
      " 0.07270955 0.07270955 0.75901717 0.75901717 0.07270955 0.07270955\n",
      " 0.07270955 0.07270955 0.07270955 0.07270955 0.07270955 0.75901717\n",
      " 0.75901717 0.75901717]\n",
      " After 0 training  step(s), loss on training batch is [0.7589706  0.7589706  0.06967811 0.06967811 0.06967811 0.7589706\n",
      " 0.7589706  0.06967811 0.06967811 0.06967811 0.06967811 0.7589706\n",
      " 0.7589706  0.7589706  0.7589706  0.7589706  0.06967811 0.7589706\n",
      " 0.7589706  0.7589706 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07237008 0.07237008 0.7589208  0.07237008 0.07237008 0.07237008\n",
      " 0.7589208  0.07237008 0.7589208  0.7589208  0.7589208  0.07237008\n",
      " 0.7589208  0.7589208  0.7589208  0.07237008 0.7589208  0.7589208\n",
      " 0.7589208  0.07237008]\n",
      " After 0 training  step(s), loss on training batch is [0.08946119 0.08946119 0.75887305 0.75887305 0.75887305 0.75887305\n",
      " 0.08946119 0.75887305 0.75887305 0.08946119 0.75887305 0.08946119\n",
      " 0.08946119 0.75887305 0.75887305 0.08946119 0.08946119 0.75887305\n",
      " 0.75887305 0.75887305]\n",
      " After 0 training  step(s), loss on training batch is [0.06745521 0.06745521 0.7588311  0.06745521 0.06745521 0.7588311\n",
      " 0.06745521 0.7588311  0.06745521 0.7588311  0.06745521 0.06745521\n",
      " 0.7588311  0.7588311  0.7588311  0.06745521 0.7588311  0.06745521\n",
      " 0.7588311  0.7588311 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07720083 0.07720083 0.07720083 0.7587805  0.7587805  0.07720083\n",
      " 0.7587805  0.07720083 0.7587805  0.7587805  0.07720083 0.7587805\n",
      " 0.07720083 0.7587805  0.7587805  0.7587805  0.7587805  0.07720083\n",
      " 0.7587805  0.07720083]\n",
      " After 0 training  step(s), loss on training batch is [0.7587353  0.06677331 0.7587353  0.7587353  0.06677331 0.7587353\n",
      " 0.7587353  0.06677331 0.06677331 0.7587353  0.06677331 0.7587353\n",
      " 0.06677331 0.06677331 0.06677331 0.06677331 0.06677331 0.7587353\n",
      " 0.06677331 0.7587353 ]\n",
      " After 0 training  step(s), loss on training batch is [0.75868434 0.75868434 0.75868434 0.75868434 0.75868434 0.08705761\n",
      " 0.08705761 0.75868434 0.75868434 0.08705761 0.08705761 0.75868434\n",
      " 0.75868434 0.75868434 0.75868434 0.75868434 0.08705761 0.75868434\n",
      " 0.08705761 0.08705761]\n",
      " After 0 training  step(s), loss on training batch is [0.75864065 0.75864065 0.75864065 0.75864065 0.75864065 0.08122847\n",
      " 0.08122847 0.75864065 0.75864065 0.08122847 0.08122847 0.08122847\n",
      " 0.08122847 0.75864065 0.08122847 0.08122847 0.08122847 0.75864065\n",
      " 0.08122847 0.75864065]\n",
      " After 0 training  step(s), loss on training batch is [0.06675205 0.7585983  0.06675205 0.06675205 0.7585983  0.06675205\n",
      " 0.06675205 0.7585983  0.06675205 0.06675205 0.7585983  0.06675205\n",
      " 0.06675205 0.7585983  0.06675205 0.7585983  0.06675205 0.06675205\n",
      " 0.7585983  0.7585983 ]\n",
      " After 0 training  step(s), loss on training batch is [0.065697  0.7585478 0.7585478 0.7585478 0.065697  0.065697  0.065697\n",
      " 0.065697  0.7585478 0.065697  0.065697  0.7585478 0.7585478 0.7585478\n",
      " 0.065697  0.7585478 0.065697  0.065697  0.065697  0.065697 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06629628 0.7584959  0.7584959  0.06629628 0.06629628 0.7584959\n",
      " 0.06629628 0.7584959  0.06629628 0.06629628 0.06629628 0.06629628\n",
      " 0.06629628 0.06629628 0.7584959  0.7584959  0.7584959  0.7584959\n",
      " 0.06629628 0.7584959 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06590888 0.06590888 0.06590888 0.06590888 0.75844496 0.75844496\n",
      " 0.06590888 0.06590888 0.06590888 0.06590888 0.06590888 0.75844496\n",
      " 0.75844496 0.75844496 0.75844496 0.75844496 0.06590888 0.06590888\n",
      " 0.06590888 0.06590888]\n",
      " After 0 training  step(s), loss on training batch is [0.75839376 0.75839376 0.10357827 0.75839376 0.75839376 0.75839376\n",
      " 0.75839376 0.75839376 0.75839376 0.10357827 0.10357827 0.10357827\n",
      " 0.10357827 0.75839376 0.75839376 0.10357827 0.10357827 0.10357827\n",
      " 0.75839376 0.75839376]\n",
      " After 0 training  step(s), loss on training batch is [0.07623821 0.75835556 0.75835556 0.75835556 0.75835556 0.07623821\n",
      " 0.75835556 0.75835556 0.75835556 0.75835556 0.07623821 0.07623821\n",
      " 0.07623821 0.75835556 0.07623821 0.07623821 0.07623821 0.07623821\n",
      " 0.75835556 0.75835556]\n",
      " After 0 training  step(s), loss on training batch is [0.06879672 0.75831044 0.06879672 0.06879672 0.06879672 0.06879672\n",
      " 0.75831044 0.06879672 0.06879672 0.75831044 0.06879672 0.06879672\n",
      " 0.06879672 0.75831044 0.75831044 0.75831044 0.75831044 0.75831044\n",
      " 0.75831044 0.75831044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.75826174 0.06603216 0.06603216 0.06603216 0.06603216 0.75826174\n",
      " 0.75826174 0.06603216 0.06603216 0.75826174 0.75826174 0.06603216\n",
      " 0.06603216 0.06603216 0.06603216 0.06603216 0.75826174 0.75826174\n",
      " 0.06603216 0.06603216]\n",
      " After 0 training  step(s), loss on training batch is [0.75821114 0.75821114 0.75821114 0.75821114 0.07818125 0.07818125\n",
      " 0.75821114 0.75821114 0.75821114 0.75821114 0.75821114 0.75821114\n",
      " 0.75821114 0.07818125 0.07818125 0.75821114 0.75821114 0.75821114\n",
      " 0.75821114 0.75821114]\n",
      " After 0 training  step(s), loss on training batch is [0.07826628 0.7581626  0.7581626  0.07826628 0.7581626  0.7581626\n",
      " 0.7581626  0.07826628 0.07826628 0.07826628 0.7581626  0.07826628\n",
      " 0.7581626  0.07826628 0.7581626  0.07826628 0.7581626  0.07826628\n",
      " 0.7581626  0.7581626 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06761879 0.7581186  0.7581186  0.7581186  0.06761879 0.7581186\n",
      " 0.7581186  0.7581186  0.7581186  0.7581186  0.06761879 0.7581186\n",
      " 0.06761879 0.7581186  0.06761879 0.06761879 0.06761879 0.06761879\n",
      " 0.7581186  0.7581186 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07017194 0.07017194 0.75806874 0.07017194 0.75806874 0.07017194\n",
      " 0.75806874 0.07017194 0.07017194 0.75806874 0.75806874 0.07017194\n",
      " 0.07017194 0.75806874 0.07017194 0.07017194 0.07017194 0.75806874\n",
      " 0.75806874 0.75806874]\n",
      " After 0 training  step(s), loss on training batch is [0.06866365 0.06866365 0.75802183 0.06866365 0.75802183 0.75802183\n",
      " 0.06866365 0.75802183 0.75802183 0.06866365 0.06866365 0.06866365\n",
      " 0.06866365 0.75802183 0.06866365 0.75802183 0.75802183 0.75802183\n",
      " 0.75802183 0.06866365]\n",
      " After 0 training  step(s), loss on training batch is [0.06496821 0.06496821 0.75797343 0.06496821 0.06496821 0.75797343\n",
      " 0.75797343 0.75797343 0.06496821 0.75797343 0.75797343 0.06496821\n",
      " 0.75797343 0.06496821 0.06496821 0.06496821 0.06496821 0.06496821\n",
      " 0.06496821 0.06496821]\n",
      " After 0 training  step(s), loss on training batch is [0.7579219  0.7579219  0.06941734 0.7579219  0.06941734 0.06941734\n",
      " 0.06941734 0.06941734 0.7579219  0.06941734 0.06941734 0.06941734\n",
      " 0.7579219  0.7579219  0.7579219  0.7579219  0.06941734 0.7579219\n",
      " 0.7579219  0.7579219 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7578738  0.7578738  0.7578738  0.07193486 0.7578738  0.7578738\n",
      " 0.07193486 0.07193486 0.7578738  0.07193486 0.07193486 0.07193486\n",
      " 0.07193486 0.7578738  0.7578738  0.7578738  0.07193486 0.7578738\n",
      " 0.07193486 0.07193486]\n",
      " After 0 training  step(s), loss on training batch is [0.06544508 0.06544508 0.06544508 0.7578277  0.7578277  0.06544508\n",
      " 0.06544508 0.06544508 0.06544508 0.06544508 0.06544508 0.06544508\n",
      " 0.06544508 0.7578277  0.06544508 0.06544508 0.06544508 0.06544508\n",
      " 0.06544508 0.7578277 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06653026 0.7577775  0.7577775  0.06653026 0.06653026 0.06653026\n",
      " 0.06653026 0.7577775  0.7577775  0.7577775  0.06653026 0.06653026\n",
      " 0.06653026 0.7577775  0.7577775  0.7577775  0.06653026 0.7577775\n",
      " 0.7577775  0.7577775 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06503595 0.06503595 0.7577276  0.06503595 0.06503595 0.06503595\n",
      " 0.7577276  0.06503595 0.7577276  0.06503595 0.06503595 0.7577276\n",
      " 0.06503595 0.06503595 0.7577276  0.7577276  0.06503595 0.7577276\n",
      " 0.7577276  0.06503595]\n",
      " After 0 training  step(s), loss on training batch is [0.7576767  0.7576767  0.11433615 0.7576767  0.7576767  0.7576767\n",
      " 0.7576767  0.11433615 0.7576767  0.11433615 0.7576767  0.7576767\n",
      " 0.11433615 0.7576767  0.11433615 0.11433615 0.7576767  0.7576767\n",
      " 0.11433615 0.11433615]\n",
      " After 0 training  step(s), loss on training batch is [0.06528815 0.06528815 0.757641   0.06528815 0.06528815 0.757641\n",
      " 0.06528815 0.757641   0.06528815 0.757641   0.757641   0.757641\n",
      " 0.757641   0.06528815 0.757641   0.757641   0.757641   0.757641\n",
      " 0.06528815 0.06528815]\n",
      " After 0 training  step(s), loss on training batch is [0.07842837 0.07842837 0.07842837 0.7575903  0.7575903  0.7575903\n",
      " 0.7575903  0.7575903  0.7575903  0.7575903  0.7575903  0.7575903\n",
      " 0.7575903  0.07842837 0.07842837 0.07842837 0.7575903  0.7575903\n",
      " 0.07842837 0.7575903 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7575452  0.06802766 0.7575452  0.7575452  0.7575452  0.7575452\n",
      " 0.06802766 0.06802766 0.06802766 0.06802766 0.06802766 0.06802766\n",
      " 0.06802766 0.7575452  0.06802766 0.06802766 0.06802766 0.7575452\n",
      " 0.06802766 0.06802766]\n",
      " After 0 training  step(s), loss on training batch is [0.06476114 0.7574981  0.06476114 0.7574981  0.06476114 0.06476114\n",
      " 0.06476114 0.06476114 0.06476114 0.06476114 0.06476114 0.7574981\n",
      " 0.7574981  0.06476114 0.06476114 0.06476114 0.06476114 0.7574981\n",
      " 0.7574981  0.06476114]\n",
      " After 0 training  step(s), loss on training batch is [0.75744736 0.75744736 0.08847256 0.75744736 0.75744736 0.08847256\n",
      " 0.75744736 0.08847256 0.75744736 0.75744736 0.08847256 0.75744736\n",
      " 0.75744736 0.75744736 0.08847256 0.08847256 0.08847256 0.75744736\n",
      " 0.08847256 0.75744736]\n",
      " After 0 training  step(s), loss on training batch is [0.75740653 0.75740653 0.75740653 0.75740653 0.75740653 0.75740653\n",
      " 0.07407026 0.07407026 0.07407026 0.07407026 0.75740653 0.07407026\n",
      " 0.07407026 0.75740653 0.07407026 0.75740653 0.07407026 0.75740653\n",
      " 0.75740653 0.07407026]\n",
      " After 0 training  step(s), loss on training batch is [0.06617298 0.7573616  0.7573616  0.06617298 0.7573616  0.06617298\n",
      " 0.06617298 0.7573616  0.7573616  0.06617298 0.06617298 0.06617298\n",
      " 0.7573616  0.06617298 0.7573616  0.06617298 0.06617298 0.06617298\n",
      " 0.06617298 0.7573616 ]\n",
      " After 0 training  step(s), loss on training batch is [0.75731266 0.08853751 0.08853751 0.75731266 0.75731266 0.08853751\n",
      " 0.75731266 0.75731266 0.75731266 0.75731266 0.75731266 0.75731266\n",
      " 0.08853751 0.08853751 0.75731266 0.75731266 0.75731266 0.75731266\n",
      " 0.08853751 0.08853751]\n",
      " After 0 training  step(s), loss on training batch is [0.07224914 0.75727063 0.75727063 0.75727063 0.07224914 0.75727063\n",
      " 0.07224914 0.07224914 0.75727063 0.07224914 0.75727063 0.75727063\n",
      " 0.07224914 0.75727063 0.75727063 0.75727063 0.75727063 0.75727063\n",
      " 0.75727063 0.07224914]\n",
      " After 0 training  step(s), loss on training batch is [0.7572237  0.06572572 0.7572237  0.06572572 0.06572572 0.7572237\n",
      " 0.06572572 0.06572572 0.06572572 0.06572572 0.06572572 0.7572237\n",
      " 0.7572237  0.7572237  0.7572237  0.7572237  0.7572237  0.7572237\n",
      " 0.06572572 0.06572572]\n",
      " After 0 training  step(s), loss on training batch is [0.06487439 0.06487439 0.06487439 0.75717425 0.75717425 0.06487439\n",
      " 0.06487439 0.75717425 0.06487439 0.06487439 0.06487439 0.75717425\n",
      " 0.75717425 0.06487439 0.06487439 0.06487439 0.06487439 0.75717425\n",
      " 0.06487439 0.06487439]\n",
      " After 0 training  step(s), loss on training batch is [0.06705399 0.06705399 0.06705399 0.06705399 0.7571245  0.7571245\n",
      " 0.7571245  0.06705399 0.7571245  0.06705399 0.7571245  0.7571245\n",
      " 0.06705399 0.06705399 0.7571245  0.06705399 0.7571245  0.06705399\n",
      " 0.06705399 0.06705399]\n",
      " After 0 training  step(s), loss on training batch is [0.07870657 0.07870657 0.07870657 0.75707686 0.75707686 0.07870657\n",
      " 0.75707686 0.75707686 0.75707686 0.75707686 0.75707686 0.07870657\n",
      " 0.75707686 0.75707686 0.75707686 0.75707686 0.07870657 0.75707686\n",
      " 0.75707686 0.07870657]\n",
      " After 0 training  step(s), loss on training batch is [0.06860569 0.7570324  0.7570324  0.06860569 0.7570324  0.06860569\n",
      " 0.7570324  0.06860569 0.06860569 0.7570324  0.7570324  0.06860569\n",
      " 0.7570324  0.7570324  0.06860569 0.7570324  0.06860569 0.7570324\n",
      " 0.7570324  0.06860569]\n",
      " After 0 training  step(s), loss on training batch is [0.756985   0.06469981 0.06469981 0.06469981 0.756985   0.06469981\n",
      " 0.756985   0.06469981 0.06469981 0.756985   0.06469981 0.756985\n",
      " 0.06469981 0.06469981 0.06469981 0.756985   0.06469981 0.756985\n",
      " 0.06469981 0.06469981]\n",
      " After 0 training  step(s), loss on training batch is [0.7569353 0.0655029 0.0655029 0.0655029 0.7569353 0.7569353 0.0655029\n",
      " 0.0655029 0.0655029 0.7569353 0.0655029 0.7569353 0.0655029 0.7569353\n",
      " 0.0655029 0.7569353 0.0655029 0.7569353 0.0655029 0.7569353]\n",
      " After 0 training  step(s), loss on training batch is [0.7568863  0.06685672 0.06685672 0.06685672 0.7568863  0.06685672\n",
      " 0.7568863  0.06685672 0.06685672 0.7568863  0.7568863  0.7568863\n",
      " 0.7568863  0.7568863  0.06685672 0.7568863  0.06685672 0.06685672\n",
      " 0.7568863  0.7568863 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.09341788 0.756838   0.756838   0.09341788 0.756838   0.756838\n",
      " 0.756838   0.756838   0.756838   0.756838   0.09341788 0.756838\n",
      " 0.756838   0.09341788 0.756838   0.756838   0.756838   0.756838\n",
      " 0.756838   0.756838  ]\n",
      " After 0 training  step(s), loss on training batch is [0.06397301 0.06397301 0.06397301 0.756793   0.06397301 0.756793\n",
      " 0.06397301 0.06397301 0.06397301 0.756793   0.06397301 0.06397301\n",
      " 0.06397301 0.756793   0.06397301 0.06397301 0.756793   0.06397301\n",
      " 0.06397301 0.06397301]\n",
      " After 0 training  step(s), loss on training batch is [0.75674284 0.75674284 0.75674284 0.75674284 0.0721884  0.0721884\n",
      " 0.75674284 0.75674284 0.0721884  0.75674284 0.0721884  0.0721884\n",
      " 0.75674284 0.75674284 0.75674284 0.0721884  0.75674284 0.0721884\n",
      " 0.0721884  0.0721884 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7566978  0.7566978  0.7566978  0.07105234 0.07105234 0.7566978\n",
      " 0.07105234 0.07105234 0.7566978  0.7566978  0.7566978  0.07105234\n",
      " 0.7566978  0.7566978  0.7566978  0.07105234 0.7566978  0.07105234\n",
      " 0.7566978  0.07105234]\n",
      " After 0 training  step(s), loss on training batch is [0.75665164 0.75665164 0.75665164 0.75665164 0.75665164 0.75665164\n",
      " 0.75665164 0.10899553 0.10899553 0.10899553 0.75665164 0.75665164\n",
      " 0.10899553 0.10899553 0.75665164 0.75665164 0.75665164 0.75665164\n",
      " 0.10899553 0.75665164]\n",
      " After 0 training  step(s), loss on training batch is [0.756612   0.756612   0.756612   0.06995637 0.756612   0.756612\n",
      " 0.756612   0.06995637 0.756612   0.06995637 0.06995637 0.756612\n",
      " 0.06995637 0.06995637 0.756612   0.06995637 0.06995637 0.06995637\n",
      " 0.06995637 0.756612  ]\n",
      " After 0 training  step(s), loss on training batch is [0.7565665  0.7565665  0.7565665  0.7565665  0.7565665  0.07476555\n",
      " 0.07476555 0.7565665  0.7565665  0.7565665  0.7565665  0.07476555\n",
      " 0.07476555 0.07476555 0.07476555 0.07476555 0.07476555 0.7565665\n",
      " 0.07476555 0.7565665 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06396525 0.7565229  0.7565229  0.7565229  0.06396525 0.7565229\n",
      " 0.06396525 0.7565229  0.7565229  0.7565229  0.7565229  0.06396525\n",
      " 0.06396525 0.06396525 0.06396525 0.7565229  0.7565229  0.06396525\n",
      " 0.06396525 0.06396525]\n",
      " After 0 training  step(s), loss on training batch is [0.07607089 0.07607089 0.07607089 0.07607089 0.75647295 0.07607089\n",
      " 0.75647295 0.75647295 0.07607089 0.75647295 0.07607089 0.07607089\n",
      " 0.07607089 0.75647295 0.75647295 0.07607089 0.75647295 0.75647295\n",
      " 0.75647295 0.07607089]\n",
      " After 0 training  step(s), loss on training batch is [0.08716047 0.7564317  0.7564317  0.08716047 0.7564317  0.08716047\n",
      " 0.7564317  0.7564317  0.08716047 0.7564317  0.08716047 0.08716047\n",
      " 0.7564317  0.7564317  0.7564317  0.08716047 0.7564317  0.08716047\n",
      " 0.7564317  0.7564317 ]\n",
      " After 0 training  step(s), loss on training batch is [0.0896873  0.75639147 0.0896873  0.75639147 0.75639147 0.75639147\n",
      " 0.75639147 0.0896873  0.75639147 0.0896873  0.75639147 0.75639147\n",
      " 0.75639147 0.0896873  0.0896873  0.0896873  0.75639147 0.75639147\n",
      " 0.75639147 0.75639147]\n",
      " After 0 training  step(s), loss on training batch is [0.7563505  0.7563505  0.07570417 0.07570417 0.7563505  0.7563505\n",
      " 0.7563505  0.07570417 0.7563505  0.7563505  0.07570417 0.7563505\n",
      " 0.07570417 0.7563505  0.07570417 0.7563505  0.7563505  0.7563505\n",
      " 0.07570417 0.07570417]\n",
      " After 0 training  step(s), loss on training batch is [0.06719657 0.06719657 0.06719657 0.75630665 0.75630665 0.75630665\n",
      " 0.06719657 0.06719657 0.75630665 0.75630665 0.75630665 0.75630665\n",
      " 0.75630665 0.75630665 0.06719657 0.06719657 0.06719657 0.06719657\n",
      " 0.75630665 0.75630665]\n",
      " After 0 training  step(s), loss on training batch is [0.75625944 0.06383319 0.75625944 0.06383319 0.06383319 0.75625944\n",
      " 0.06383319 0.06383319 0.75625944 0.06383319 0.06383319 0.06383319\n",
      " 0.06383319 0.06383319 0.75625944 0.75625944 0.06383319 0.75625944\n",
      " 0.75625944 0.75625944]\n",
      " After 0 training  step(s), loss on training batch is [0.06646705 0.7562099  0.7562099  0.06646705 0.7562099  0.06646705\n",
      " 0.7562099  0.06646705 0.06646705 0.7562099  0.7562099  0.7562099\n",
      " 0.7562099  0.06646705 0.06646705 0.06646705 0.06646705 0.7562099\n",
      " 0.06646705 0.7562099 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06420556 0.06420556 0.06420556 0.75616264 0.06420556 0.75616264\n",
      " 0.75616264 0.75616264 0.06420556 0.06420556 0.06420556 0.75616264\n",
      " 0.06420556 0.75616264 0.06420556 0.75616264 0.75616264 0.75616264\n",
      " 0.06420556 0.75616264]\n",
      " After 0 training  step(s), loss on training batch is [0.75611365 0.06863781 0.06863781 0.06863781 0.06863781 0.06863781\n",
      " 0.75611365 0.75611365 0.06863781 0.75611365 0.06863781 0.06863781\n",
      " 0.75611365 0.75611365 0.75611365 0.06863781 0.75611365 0.75611365\n",
      " 0.75611365 0.06863781]\n",
      " After 0 training  step(s), loss on training batch is [0.06747685 0.756068   0.756068   0.06747685 0.756068   0.756068\n",
      " 0.06747685 0.756068   0.756068   0.06747685 0.06747685 0.06747685\n",
      " 0.756068   0.756068   0.756068   0.756068   0.06747685 0.06747685\n",
      " 0.756068   0.756068  ]\n",
      " After 0 training  step(s), loss on training batch is [0.06568746 0.75602084 0.06568746 0.75602084 0.06568746 0.75602084\n",
      " 0.75602084 0.06568746 0.06568746 0.75602084 0.06568746 0.06568746\n",
      " 0.75602084 0.06568746 0.75602084 0.75602084 0.75602084 0.75602084\n",
      " 0.75602084 0.06568746]\n",
      " After 0 training  step(s), loss on training batch is [0.75597304 0.75597304 0.06666619 0.06666619 0.06666619 0.06666619\n",
      " 0.75597304 0.06666619 0.75597304 0.75597304 0.75597304 0.06666619\n",
      " 0.06666619 0.75597304 0.06666619 0.06666619 0.75597304 0.75597304\n",
      " 0.06666619 0.06666619]\n",
      " After 0 training  step(s), loss on training batch is [0.7559266  0.7559266  0.06381901 0.06381901 0.7559266  0.7559266\n",
      " 0.7559266  0.06381901 0.06381901 0.7559266  0.06381901 0.06381901\n",
      " 0.06381901 0.06381901 0.06381901 0.06381901 0.06381901 0.06381901\n",
      " 0.06381901 0.06381901]\n",
      " After 0 training  step(s), loss on training batch is [0.7558781  0.7558781  0.7558781  0.07647283 0.7558781  0.07647283\n",
      " 0.07647283 0.07647283 0.07647283 0.7558781  0.07647283 0.7558781\n",
      " 0.7558781  0.07647283 0.7558781  0.07647283 0.7558781  0.07647283\n",
      " 0.7558781  0.7558781 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07258721 0.07258721 0.07258721 0.755836   0.755836   0.755836\n",
      " 0.755836   0.755836   0.07258721 0.755836   0.755836   0.07258721\n",
      " 0.755836   0.755836   0.755836   0.755836   0.07258721 0.755836\n",
      " 0.07258721 0.755836  ]\n",
      " After 0 training  step(s), loss on training batch is [0.0649596 0.0649596 0.0649596 0.0649596 0.7557908 0.7557908 0.0649596\n",
      " 0.0649596 0.0649596 0.0649596 0.7557908 0.7557908 0.0649596 0.7557908\n",
      " 0.0649596 0.0649596 0.7557908 0.7557908 0.0649596 0.0649596]\n",
      " After 0 training  step(s), loss on training batch is [0.0628599 0.0628599 0.0628599 0.0628599 0.0628599 0.7557438 0.0628599\n",
      " 0.7557438 0.0628599 0.7557438 0.0628599 0.0628599 0.0628599 0.7557438\n",
      " 0.0628599 0.0628599 0.0628599 0.7557438 0.7557438 0.0628599]\n",
      " After 0 training  step(s), loss on training batch is [0.75569427 0.06334736 0.75569427 0.75569427 0.06334736 0.06334736\n",
      " 0.06334736 0.06334736 0.75569427 0.06334736 0.75569427 0.06334736\n",
      " 0.06334736 0.75569427 0.06334736 0.75569427 0.75569427 0.06334736\n",
      " 0.06334736 0.06334736]\n",
      " After 0 training  step(s), loss on training batch is [0.7556454  0.7556454  0.09102026 0.7556454  0.7556454  0.09102026\n",
      " 0.7556454  0.09102026 0.7556454  0.09102026 0.7556454  0.7556454\n",
      " 0.7556454  0.7556454  0.09102026 0.09102026 0.09102026 0.7556454\n",
      " 0.7556454  0.09102026]\n",
      " After 0 training  step(s), loss on training batch is [0.06334894 0.7556069  0.7556069  0.7556069  0.06334894 0.06334894\n",
      " 0.7556069  0.06334894 0.06334894 0.7556069  0.7556069  0.7556069\n",
      " 0.7556069  0.06334894 0.06334894 0.06334894 0.06334894 0.7556069\n",
      " 0.06334894 0.06334894]\n",
      " After 0 training  step(s), loss on training batch is [0.06291133 0.06291133 0.06291133 0.06291133 0.06291133 0.7555581\n",
      " 0.7555581  0.7555581  0.06291133 0.7555581  0.06291133 0.06291133\n",
      " 0.7555581  0.7555581  0.06291133 0.06291133 0.06291133 0.7555581\n",
      " 0.7555581  0.06291133]\n",
      " After 0 training  step(s), loss on training batch is [0.06418353 0.75550896 0.06418353 0.75550896 0.75550896 0.06418353\n",
      " 0.75550896 0.06418353 0.06418353 0.75550896 0.06418353 0.06418353\n",
      " 0.06418353 0.06418353 0.75550896 0.75550896 0.06418353 0.06418353\n",
      " 0.06418353 0.75550896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.06250313 0.06250313 0.7554614  0.06250313 0.06250313 0.7554614\n",
      " 0.7554614  0.06250313 0.7554614  0.7554614  0.06250313 0.7554614\n",
      " 0.7554614  0.06250313 0.7554614  0.7554614  0.06250313 0.06250313\n",
      " 0.06250313 0.06250313]\n",
      " After 0 training  step(s), loss on training batch is [0.06450164 0.75541186 0.75541186 0.06450164 0.75541186 0.06450164\n",
      " 0.06450164 0.75541186 0.06450164 0.06450164 0.06450164 0.75541186\n",
      " 0.75541186 0.75541186 0.06450164 0.06450164 0.06450164 0.06450164\n",
      " 0.75541186 0.75541186]\n",
      " After 0 training  step(s), loss on training batch is [0.06532272 0.06532272 0.06532272 0.06532272 0.75536454 0.06532272\n",
      " 0.75536454 0.06532272 0.06532272 0.75536454 0.75536454 0.75536454\n",
      " 0.06532272 0.06532272 0.75536454 0.06532272 0.06532272 0.75536454\n",
      " 0.75536454 0.06532272]\n",
      " After 0 training  step(s), loss on training batch is [0.75531834 0.08164541 0.75531834 0.08164541 0.75531834 0.08164541\n",
      " 0.75531834 0.08164541 0.75531834 0.75531834 0.75531834 0.75531834\n",
      " 0.75531834 0.75531834 0.75531834 0.75531834 0.75531834 0.08164541\n",
      " 0.08164541 0.75531834]\n",
      " After 0 training  step(s), loss on training batch is [0.06228708 0.06228708 0.75527537 0.75527537 0.06228708 0.06228708\n",
      " 0.06228708 0.06228708 0.06228708 0.75527537 0.75527537 0.06228708\n",
      " 0.06228708 0.06228708 0.06228708 0.75527537 0.75527537 0.06228708\n",
      " 0.06228708 0.06228708]\n",
      " After 0 training  step(s), loss on training batch is [0.06257224 0.06257224 0.755226   0.755226   0.06257224 0.06257224\n",
      " 0.06257224 0.06257224 0.755226   0.06257224 0.06257224 0.755226\n",
      " 0.06257224 0.06257224 0.755226   0.755226   0.06257224 0.755226\n",
      " 0.755226   0.06257224]\n",
      " After 0 training  step(s), loss on training batch is [0.06607857 0.75517714 0.75517714 0.06607857 0.75517714 0.06607857\n",
      " 0.75517714 0.06607857 0.75517714 0.06607857 0.75517714 0.75517714\n",
      " 0.75517714 0.06607857 0.75517714 0.06607857 0.06607857 0.06607857\n",
      " 0.06607857 0.75517714]\n",
      " After 0 training  step(s), loss on training batch is [0.06306641 0.06306641 0.06306641 0.7551312  0.7551312  0.06306641\n",
      " 0.7551312  0.06306641 0.7551312  0.06306641 0.06306641 0.06306641\n",
      " 0.06306641 0.06306641 0.7551312  0.7551312  0.06306641 0.06306641\n",
      " 0.06306641 0.7551312 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7550832  0.7550832  0.06871888 0.7550832  0.06871888 0.7550832\n",
      " 0.06871888 0.7550832  0.7550832  0.7550832  0.06871888 0.7550832\n",
      " 0.7550832  0.06871888 0.7550832  0.06871888 0.7550832  0.7550832\n",
      " 0.06871888 0.06871888]\n",
      " After 0 training  step(s), loss on training batch is [0.75503796 0.0812769  0.75503796 0.75503796 0.75503796 0.0812769\n",
      " 0.75503796 0.75503796 0.0812769  0.0812769  0.75503796 0.75503796\n",
      " 0.75503796 0.75503796 0.75503796 0.75503796 0.75503796 0.0812769\n",
      " 0.0812769  0.0812769 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7549963  0.7549963  0.7549963  0.07507823 0.7549963  0.7549963\n",
      " 0.7549963  0.07507823 0.07507823 0.07507823 0.07507823 0.7549963\n",
      " 0.7549963  0.07507823 0.7549963  0.7549963  0.7549963  0.07507823\n",
      " 0.7549963  0.7549963 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07809217 0.7549529  0.7549529  0.7549529  0.7549529  0.07809217\n",
      " 0.7549529  0.7549529  0.07809217 0.07809217 0.7549529  0.7549529\n",
      " 0.07809217 0.07809217 0.07809217 0.07809217 0.7549529  0.7549529\n",
      " 0.7549529  0.07809217]\n",
      " After 0 training  step(s), loss on training batch is [0.06891276 0.7549125  0.06891276 0.7549125  0.7549125  0.06891276\n",
      " 0.7549125  0.06891276 0.7549125  0.7549125  0.7549125  0.7549125\n",
      " 0.7549125  0.06891276 0.7549125  0.7549125  0.06891276 0.7549125\n",
      " 0.06891276 0.06891276]\n",
      " After 0 training  step(s), loss on training batch is [0.06296353 0.06296353 0.75486755 0.75486755 0.06296353 0.75486755\n",
      " 0.06296353 0.75486755 0.75486755 0.06296353 0.06296353 0.75486755\n",
      " 0.75486755 0.75486755 0.06296353 0.06296353 0.75486755 0.06296353\n",
      " 0.75486755 0.06296353]\n",
      " After 0 training  step(s), loss on training batch is [0.06207406 0.06207406 0.06207406 0.06207406 0.7548196  0.06207406\n",
      " 0.06207406 0.06207406 0.06207406 0.06207406 0.7548196  0.7548196\n",
      " 0.7548196  0.06207406 0.06207406 0.7548196  0.06207406 0.7548196\n",
      " 0.06207406 0.7548196 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06249721 0.06249721 0.06249721 0.06249721 0.06249721 0.06249721\n",
      " 0.754771   0.06249721 0.06249721 0.06249721 0.06249721 0.06249721\n",
      " 0.06249721 0.754771   0.754771   0.06249721 0.754771   0.754771\n",
      " 0.06249721 0.06249721]\n",
      " After 0 training  step(s), loss on training batch is [0.06249535 0.75472325 0.75472325 0.75472325 0.06249535 0.06249535\n",
      " 0.06249535 0.06249535 0.75472325 0.06249535 0.75472325 0.06249535\n",
      " 0.06249535 0.75472325 0.75472325 0.75472325 0.06249535 0.06249535\n",
      " 0.06249535 0.75472325]\n",
      " After 0 training  step(s), loss on training batch is [0.06848904 0.06848904 0.7546752  0.06848904 0.7546752  0.06848904\n",
      " 0.06848904 0.7546752  0.06848904 0.06848904 0.7546752  0.06848904\n",
      " 0.7546752  0.7546752  0.7546752  0.06848904 0.06848904 0.7546752\n",
      " 0.7546752  0.7546752 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07326108 0.75463146 0.07326108 0.75463146 0.75463146 0.07326108\n",
      " 0.75463146 0.75463146 0.75463146 0.07326108 0.75463146 0.75463146\n",
      " 0.07326108 0.07326108 0.75463146 0.75463146 0.75463146 0.07326108\n",
      " 0.75463146 0.07326108]\n",
      " After 0 training  step(s), loss on training batch is [0.75458866 0.06331442 0.75458866 0.75458866 0.75458866 0.75458866\n",
      " 0.06331442 0.06331442 0.75458866 0.06331442 0.06331442 0.06331442\n",
      " 0.06331442 0.75458866 0.06331442 0.75458866 0.06331442 0.06331442\n",
      " 0.06331442 0.75458866]\n",
      " After 0 training  step(s), loss on training batch is [0.06167568 0.06167568 0.06167568 0.06167568 0.06167568 0.7545417\n",
      " 0.06167568 0.7545417  0.06167568 0.7545417  0.06167568 0.06167568\n",
      " 0.06167568 0.7545417  0.7545417  0.7545417  0.7545417  0.06167568\n",
      " 0.06167568 0.06167568]\n",
      " After 0 training  step(s), loss on training batch is [0.7544931  0.7544931  0.7544931  0.7544931  0.06391039 0.06391039\n",
      " 0.7544931  0.7544931  0.06391039 0.7544931  0.06391039 0.7544931\n",
      " 0.06391039 0.06391039 0.06391039 0.7544931  0.7544931  0.06391039\n",
      " 0.7544931  0.7544931 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06355938 0.754446   0.754446   0.06355938 0.754446   0.754446\n",
      " 0.06355938 0.754446   0.754446   0.754446   0.06355938 0.754446\n",
      " 0.06355938 0.754446   0.06355938 0.754446   0.06355938 0.754446\n",
      " 0.06355938 0.06355938]\n",
      " After 0 training  step(s), loss on training batch is [0.75439906 0.75439906 0.06516027 0.06516027 0.06516027 0.06516027\n",
      " 0.75439906 0.75439906 0.06516027 0.75439906 0.75439906 0.75439906\n",
      " 0.75439906 0.75439906 0.75439906 0.75439906 0.06516027 0.75439906\n",
      " 0.06516027 0.75439906]\n",
      " After 0 training  step(s), loss on training batch is [0.06233579 0.06233579 0.75435257 0.06233579 0.06233579 0.06233579\n",
      " 0.75435257 0.06233579 0.75435257 0.75435257 0.06233579 0.75435257\n",
      " 0.06233579 0.75435257 0.06233579 0.75435257 0.75435257 0.75435257\n",
      " 0.75435257 0.06233579]\n",
      " After 0 training  step(s), loss on training batch is [0.7543049  0.06791593 0.7543049  0.06791593 0.7543049  0.7543049\n",
      " 0.7543049  0.7543049  0.06791593 0.06791593 0.06791593 0.7543049\n",
      " 0.06791593 0.7543049  0.06791593 0.7543049  0.7543049  0.06791593\n",
      " 0.7543049  0.7543049 ]\n",
      " After 0 training  step(s), loss on training batch is [0.75426024 0.06252673 0.06252673 0.06252673 0.75426024 0.75426024\n",
      " 0.06252673 0.06252673 0.75426024 0.06252673 0.06252673 0.75426024\n",
      " 0.75426024 0.75426024 0.06252673 0.75426024 0.06252673 0.75426024\n",
      " 0.06252673 0.06252673]\n",
      " After 0 training  step(s), loss on training batch is [0.06417945 0.06417945 0.7542131  0.7542131  0.06417945 0.06417945\n",
      " 0.06417945 0.7542131  0.06417945 0.06417945 0.7542131  0.06417945\n",
      " 0.7542131  0.7542131  0.7542131  0.06417945 0.7542131  0.06417945\n",
      " 0.7542131  0.7542131 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07066884 0.7541672  0.7541672  0.07066884 0.7541672  0.7541672\n",
      " 0.7541672  0.7541672  0.07066884 0.07066884 0.07066884 0.7541672\n",
      " 0.7541672  0.7541672  0.7541672  0.07066884 0.07066884 0.07066884\n",
      " 0.7541672  0.7541672 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06610862 0.06610862 0.06610862 0.7541239  0.7541239  0.06610862\n",
      " 0.06610862 0.06610862 0.7541239  0.7541239  0.06610862 0.06610862\n",
      " 0.7541239  0.06610862 0.7541239  0.06610862 0.7541239  0.7541239\n",
      " 0.06610862 0.06610862]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.75408036 0.0612812  0.0612812  0.75408036 0.0612812  0.0612812\n",
      " 0.0612812  0.0612812  0.75408036 0.75408036 0.75408036 0.0612812\n",
      " 0.75408036 0.0612812  0.0612812  0.0612812  0.0612812  0.0612812\n",
      " 0.0612812  0.75408036]\n",
      " After 0 training  step(s), loss on training batch is [0.06739613 0.75403225 0.75403225 0.75403225 0.75403225 0.06739613\n",
      " 0.06739613 0.06739613 0.75403225 0.06739613 0.06739613 0.06739613\n",
      " 0.06739613 0.06739613 0.75403225 0.75403225 0.75403225 0.06739613\n",
      " 0.75403225 0.75403225]\n",
      " After 0 training  step(s), loss on training batch is [0.75398874 0.75398874 0.75398874 0.75398874 0.06532745 0.06532745\n",
      " 0.06532745 0.75398874 0.75398874 0.06532745 0.75398874 0.75398874\n",
      " 0.06532745 0.06532745 0.75398874 0.06532745 0.06532745 0.75398874\n",
      " 0.06532745 0.75398874]\n",
      " After 0 training  step(s), loss on training batch is [0.06956326 0.75394356 0.06956326 0.75394356 0.75394356 0.06956326\n",
      " 0.06956326 0.06956326 0.75394356 0.06956326 0.75394356 0.75394356\n",
      " 0.06956326 0.75394356 0.75394356 0.75394356 0.06956325 0.75394356\n",
      " 0.06956325 0.75394356]\n",
      " After 0 training  step(s), loss on training batch is [0.06086933 0.75390077 0.06086933 0.75390077 0.06086933 0.06086933\n",
      " 0.06086933 0.75390077 0.06086933 0.75390077 0.06086933 0.06086933\n",
      " 0.06086933 0.06086933 0.06086933 0.06086933 0.06086933 0.06086933\n",
      " 0.06086933 0.06086933]\n",
      " After 0 training  step(s), loss on training batch is [0.7538525 0.0619687 0.0619687 0.0619687 0.0619687 0.7538525 0.7538525\n",
      " 0.0619687 0.7538525 0.7538525 0.0619687 0.0619687 0.7538525 0.0619687\n",
      " 0.7538525 0.7538525 0.0619687 0.7538525 0.7538525 0.7538525]\n",
      " After 0 training  step(s), loss on training batch is [0.06190178 0.06190178 0.7538052  0.06190178 0.7538052  0.7538052\n",
      " 0.06190178 0.06190178 0.7538052  0.7538052  0.06190178 0.06190178\n",
      " 0.06190178 0.7538052  0.06190178 0.06190178 0.06190178 0.7538052\n",
      " 0.06190178 0.06190178]\n",
      " After 0 training  step(s), loss on training batch is [0.0676423 0.0676423 0.0676423 0.0676423 0.7537585 0.0676423 0.7537585\n",
      " 0.0676423 0.7537585 0.7537585 0.7537585 0.7537585 0.7537585 0.7537585\n",
      " 0.0676423 0.0676423 0.7537585 0.0676423 0.7537585 0.7537585]\n",
      " After 0 training  step(s), loss on training batch is [0.753715   0.753715   0.06887852 0.06887852 0.753715   0.06887852\n",
      " 0.753715   0.06887852 0.06887852 0.753715   0.753715   0.753715\n",
      " 0.06887852 0.06887852 0.753715   0.06887852 0.753715   0.06887852\n",
      " 0.06887852 0.06887852]\n",
      " After 0 training  step(s), loss on training batch is [0.06083331 0.06083331 0.7536733  0.06083331 0.06083331 0.06083331\n",
      " 0.7536733  0.06083331 0.7536733  0.06083331 0.7536733  0.7536733\n",
      " 0.7536733  0.06083331 0.7536733  0.7536733  0.06083331 0.06083331\n",
      " 0.7536733  0.06083331]\n",
      " After 0 training  step(s), loss on training batch is [0.7536254  0.06100892 0.7536254  0.06100892 0.06100892 0.06100892\n",
      " 0.06100892 0.06100892 0.06100892 0.06100892 0.06100892 0.7536254\n",
      " 0.06100892 0.06100892 0.7536254  0.7536254  0.06100892 0.7536254\n",
      " 0.7536254  0.7536254 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7535778 0.0610717 0.7535778 0.0610717 0.0610717 0.7535778 0.0610717\n",
      " 0.0610717 0.0610717 0.7535778 0.7535778 0.7535778 0.0610717 0.7535778\n",
      " 0.7535778 0.7535778 0.0610717 0.0610717 0.0610717 0.7535778]\n",
      " After 0 training  step(s), loss on training batch is [0.7535303  0.7535303  0.06160608 0.7535303  0.7535303  0.06160608\n",
      " 0.06160608 0.06160608 0.7535303  0.06160608 0.7535303  0.06160608\n",
      " 0.7535303  0.06160608 0.06160608 0.7535303  0.06160608 0.06160608\n",
      " 0.06160608 0.06160608]\n",
      " After 0 training  step(s), loss on training batch is [0.06075306 0.06075306 0.06075306 0.7534837  0.7534837  0.06075306\n",
      " 0.06075306 0.06075306 0.06075306 0.7534837  0.06075306 0.06075306\n",
      " 0.7534837  0.06075306 0.06075306 0.7534837  0.06075306 0.7534837\n",
      " 0.06075306 0.06075306]\n",
      " After 0 training  step(s), loss on training batch is [0.06221729 0.06221729 0.06221729 0.06221729 0.7534362  0.06221729\n",
      " 0.7534362  0.06221729 0.06221729 0.06221729 0.06221729 0.7534362\n",
      " 0.06221729 0.06221729 0.7534362  0.06221729 0.7534362  0.06221729\n",
      " 0.7534362  0.06221729]\n",
      " After 0 training  step(s), loss on training batch is [0.06074322 0.7533908  0.06074322 0.7533908  0.7533908  0.06074322\n",
      " 0.06074322 0.06074322 0.7533908  0.06074322 0.7533908  0.06074322\n",
      " 0.06074322 0.06074322 0.06074322 0.7533908  0.06074322 0.06074322\n",
      " 0.06074322 0.06074322]\n",
      " After 0 training  step(s), loss on training batch is [0.7533435  0.06025465 0.06025465 0.06025465 0.06025465 0.06025465\n",
      " 0.06025465 0.06025465 0.06025465 0.7533435  0.7533435  0.06025465\n",
      " 0.7533435  0.06025465 0.06025465 0.7533435  0.06025465 0.06025465\n",
      " 0.06025465 0.06025465]\n",
      " After 0 training  step(s), loss on training batch is [0.07890371 0.07890371 0.75329554 0.75329554 0.75329554 0.75329554\n",
      " 0.75329554 0.75329554 0.07890371 0.07890371 0.07890371 0.75329554\n",
      " 0.07890371 0.75329554 0.75329554 0.75329554 0.75329554 0.75329554\n",
      " 0.07890371 0.75329554]\n",
      " After 0 training  step(s), loss on training batch is [0.06478351 0.753255   0.753255   0.06478351 0.06478351 0.06478351\n",
      " 0.753255   0.753255   0.753255   0.753255   0.06478351 0.06478351\n",
      " 0.06478351 0.753255   0.753255   0.06478351 0.753255   0.753255\n",
      " 0.06478351 0.06478351]\n",
      " After 0 training  step(s), loss on training batch is [0.08556239 0.75321096 0.08556239 0.08556239 0.75321096 0.75321096\n",
      " 0.75321096 0.08556239 0.75321096 0.75321096 0.75321096 0.75321096\n",
      " 0.08556239 0.75321096 0.75321096 0.75321096 0.75321096 0.75321096\n",
      " 0.08556239 0.75321096]\n",
      " After 0 training  step(s), loss on training batch is [0.75317085 0.08722231 0.75317085 0.75317085 0.75317085 0.08722231\n",
      " 0.75317085 0.75317085 0.75317085 0.75317085 0.08722231 0.08722231\n",
      " 0.08722231 0.75317085 0.75317085 0.75317085 0.75317085 0.08722231\n",
      " 0.75317085 0.08722231]\n",
      " After 0 training  step(s), loss on training batch is [0.0636209 0.7531324 0.7531324 0.0636209 0.7531324 0.7531324 0.0636209\n",
      " 0.0636209 0.7531324 0.0636209 0.0636209 0.7531324 0.0636209 0.7531324\n",
      " 0.7531324 0.7531324 0.0636209 0.0636209 0.7531324 0.7531324]\n",
      " After 0 training  step(s), loss on training batch is [0.06307787 0.7530874  0.7530874  0.7530874  0.7530874  0.06307787\n",
      " 0.06307787 0.06307787 0.7530874  0.06307787 0.06307787 0.06307787\n",
      " 0.7530874  0.06307787 0.06307787 0.7530874  0.7530874  0.06307787\n",
      " 0.7530874  0.7530874 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06004301 0.06004301 0.06004301 0.7530424  0.06004301 0.06004301\n",
      " 0.06004301 0.06004301 0.7530424  0.06004301 0.06004301 0.06004301\n",
      " 0.7530424  0.06004301 0.06004301 0.7530424  0.7530424  0.06004301\n",
      " 0.06004301 0.7530424 ]\n",
      " After 0 training  step(s), loss on training batch is [0.07705969 0.7529948  0.7529948  0.7529948  0.7529948  0.7529948\n",
      " 0.7529948  0.07705969 0.07705969 0.7529948  0.07705969 0.7529948\n",
      " 0.7529948  0.7529948  0.07705969 0.7529948  0.7529948  0.7529948\n",
      " 0.07705969 0.7529948 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06021815 0.75295305 0.75295305 0.06021815 0.06021815 0.06021815\n",
      " 0.06021815 0.06021815 0.06021815 0.06021815 0.75295305 0.06021815\n",
      " 0.75295305 0.06021815 0.06021815 0.06021815 0.06021815 0.75295305\n",
      " 0.75295305 0.75295305]\n",
      " After 0 training  step(s), loss on training batch is [0.75290596 0.75290596 0.75290596 0.75290596 0.75290596 0.75290596\n",
      " 0.08843733 0.08843733 0.08843733 0.08843733 0.08843733 0.75290596\n",
      " 0.75290596 0.75290596 0.08843733 0.75290596 0.75290596 0.75290596\n",
      " 0.75290596 0.75290596]\n",
      " After 0 training  step(s), loss on training batch is [0.06234273 0.06234273 0.06234273 0.75286657 0.06234273 0.06234273\n",
      " 0.75286657 0.75286657 0.75286657 0.06234273 0.06234273 0.75286657\n",
      " 0.06234273 0.75286657 0.75286657 0.06234273 0.75286657 0.06234273\n",
      " 0.75286657 0.06234273]\n",
      " After 0 training  step(s), loss on training batch is [0.7528216  0.08459706 0.7528216  0.7528216  0.7528216  0.7528216\n",
      " 0.7528216  0.7528216  0.08459706 0.7528216  0.7528216  0.7528216\n",
      " 0.7528216  0.7528216  0.7528216  0.7528216  0.08459706 0.08459706\n",
      " 0.7528216  0.7528216 ]\n",
      " After 0 training  step(s), loss on training batch is [0.75277907 0.75277907 0.75277907 0.06251936 0.06251936 0.06251936\n",
      " 0.06251936 0.75277907 0.06251936 0.06251936 0.75277907 0.75277907\n",
      " 0.75277907 0.75277907 0.75277907 0.75277907 0.75277907 0.06251936\n",
      " 0.06251936 0.75277907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.7527336  0.7527336  0.7527336  0.06068322 0.06068322 0.7527336\n",
      " 0.7527336  0.06068322 0.06068322 0.7527336  0.7527336  0.7527336\n",
      " 0.7527336  0.06068322 0.7527336  0.7527336  0.06068322 0.06068322\n",
      " 0.06068322 0.7527336 ]\n",
      " After 0 training  step(s), loss on training batch is [0.0608747 0.0608747 0.7526869 0.0608747 0.7526869 0.0608747 0.0608747\n",
      " 0.0608747 0.7526869 0.0608747 0.7526869 0.7526869 0.0608747 0.7526869\n",
      " 0.0608747 0.7526869 0.0608747 0.7526869 0.0608747 0.7526869]\n",
      " After 0 training  step(s), loss on training batch is [0.7526409  0.06129242 0.7526409  0.06129242 0.06129242 0.06129242\n",
      " 0.7526409  0.7526409  0.7526409  0.06129242 0.7526409  0.7526409\n",
      " 0.7526409  0.7526409  0.06129242 0.7526409  0.7526409  0.06129242\n",
      " 0.7526409  0.06129242]\n",
      " After 0 training  step(s), loss on training batch is [0.7525948  0.7525948  0.06959531 0.06959531 0.06959531 0.7525948\n",
      " 0.7525948  0.06959531 0.06959531 0.7525948  0.06959531 0.7525948\n",
      " 0.7525948  0.7525948  0.7525948  0.7525948  0.7525948  0.7525948\n",
      " 0.7525948  0.7525948 ]\n",
      " After 0 training  step(s), loss on training batch is [0.75255156 0.06562334 0.06562334 0.06562334 0.75255156 0.75255156\n",
      " 0.75255156 0.06562334 0.06562334 0.06562334 0.06562334 0.75255156\n",
      " 0.75255156 0.75255156 0.75255156 0.75255156 0.75255156 0.06562334\n",
      " 0.06562334 0.75255156]\n",
      " After 0 training  step(s), loss on training batch is [0.7525085  0.06355526 0.7525085  0.7525085  0.7525085  0.7525085\n",
      " 0.06355526 0.06355526 0.7525085  0.06355526 0.7525085  0.7525085\n",
      " 0.7525085  0.06355526 0.06355526 0.7525085  0.7525085  0.06355526\n",
      " 0.06355526 0.7525085 ]\n",
      " After 0 training  step(s), loss on training batch is [0.752464   0.06127052 0.06127052 0.06127052 0.06127052 0.752464\n",
      " 0.06127052 0.06127052 0.752464   0.752464   0.06127052 0.06127052\n",
      " 0.752464   0.06127052 0.752464   0.06127052 0.752464   0.752464\n",
      " 0.752464   0.06127052]\n",
      " After 0 training  step(s), loss on training batch is [0.75241876 0.06174557 0.06174557 0.75241876 0.06174557 0.06174557\n",
      " 0.06174557 0.06174557 0.75241876 0.75241876 0.75241876 0.06174557\n",
      " 0.06174557 0.75241876 0.06174557 0.75241876 0.75241876 0.75241876\n",
      " 0.75241876 0.06174557]\n",
      " After 0 training  step(s), loss on training batch is [0.06183103 0.06183103 0.06183103 0.7523738  0.06183103 0.7523738\n",
      " 0.7523738  0.06183103 0.06183103 0.7523738  0.06183103 0.06183103\n",
      " 0.06183103 0.7523738  0.7523738  0.06183103 0.7523738  0.06183103\n",
      " 0.7523738  0.7523738 ]\n",
      " After 0 training  step(s), loss on training batch is [0.0715582  0.75232923 0.0715582  0.0715582  0.75232923 0.75232923\n",
      " 0.0715582  0.0715582  0.0715582  0.75232923 0.0715582  0.75232923\n",
      " 0.0715582  0.75232923 0.75232923 0.75232923 0.75232923 0.75232923\n",
      " 0.75232923 0.75232923]\n",
      " After 0 training  step(s), loss on training batch is [0.7522884  0.7522884  0.06751023 0.7522884  0.06751023 0.7522884\n",
      " 0.7522884  0.06751023 0.06751023 0.7522884  0.06751023 0.7522884\n",
      " 0.06751023 0.06751023 0.7522884  0.7522884  0.7522884  0.7522884\n",
      " 0.7522884  0.7522884 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7522454  0.7522454  0.7522454  0.09277963 0.7522454  0.7522454\n",
      " 0.7522454  0.7522454  0.7522454  0.7522454  0.09277963 0.09277963\n",
      " 0.7522454  0.09277963 0.7522454  0.7522454  0.7522454  0.7522454\n",
      " 0.7522454  0.09277963]\n",
      " After 0 training  step(s), loss on training batch is [0.05923954 0.05923954 0.05923954 0.05923954 0.05923954 0.05923954\n",
      " 0.05923954 0.05923954 0.05923954 0.7522058  0.05923954 0.7522058\n",
      " 0.7522058  0.05923954 0.7522058  0.7522058  0.05923954 0.7522058\n",
      " 0.7522058  0.05923954]\n",
      " After 0 training  step(s), loss on training batch is [0.7521589  0.7521589  0.7521589  0.06421891 0.7521589  0.06421891\n",
      " 0.06421891 0.7521589  0.7521589  0.7521589  0.06421891 0.7521589\n",
      " 0.06421891 0.06421891 0.7521589  0.7521589  0.7521589  0.06421891\n",
      " 0.06421891 0.06421891]\n",
      " After 0 training  step(s), loss on training batch is [0.7521156  0.7521156  0.7521156  0.7521156  0.7521156  0.7521156\n",
      " 0.10012048 0.10012048 0.10012048 0.7521156  0.7521156  0.10012048\n",
      " 0.7521156  0.7521156  0.7521156  0.10012048 0.10012048 0.7521156\n",
      " 0.7521156  0.10012048]\n",
      " After 0 training  step(s), loss on training batch is [0.05940901 0.05940901 0.7520803  0.7520803  0.05940901 0.7520803\n",
      " 0.05940901 0.7520803  0.05940901 0.7520803  0.05940901 0.05940901\n",
      " 0.7520803  0.7520803  0.05940901 0.05940901 0.05940901 0.7520803\n",
      " 0.05940901 0.05940901]\n",
      " After 0 training  step(s), loss on training batch is [0.7520339 0.0599676 0.7520339 0.7520339 0.0599676 0.7520339 0.0599676\n",
      " 0.0599676 0.7520339 0.0599676 0.0599676 0.7520339 0.0599676 0.7520339\n",
      " 0.0599676 0.0599676 0.7520339 0.0599676 0.0599676 0.0599676]\n",
      " After 0 training  step(s), loss on training batch is [0.06260545 0.06260545 0.7519883  0.06260545 0.7519883  0.7519883\n",
      " 0.06260545 0.06260545 0.06260545 0.7519883  0.7519883  0.7519883\n",
      " 0.06260545 0.7519883  0.7519883  0.06260545 0.7519883  0.06260545\n",
      " 0.06260545 0.06260545]\n",
      " After 0 training  step(s), loss on training batch is [0.06365316 0.75194496 0.75194496 0.06365316 0.06365316 0.06365316\n",
      " 0.75194496 0.06365316 0.75194496 0.75194496 0.75194496 0.75194496\n",
      " 0.75194496 0.06365316 0.75194496 0.06365316 0.75194496 0.75194496\n",
      " 0.06365316 0.75194496]\n",
      " After 0 training  step(s), loss on training batch is [0.7519012  0.06077127 0.7519012  0.06077127 0.06077127 0.7519012\n",
      " 0.06077127 0.7519012  0.7519012  0.7519012  0.06077127 0.7519012\n",
      " 0.7519012  0.7519012  0.06077127 0.06077127 0.06077127 0.7519012\n",
      " 0.7519012  0.7519012 ]\n",
      " After 0 training  step(s), loss on training batch is [0.75185585 0.75185585 0.75185585 0.07829471 0.75185585 0.07829471\n",
      " 0.07829471 0.07829471 0.75185585 0.07829471 0.07829471 0.07829471\n",
      " 0.07829471 0.75185585 0.75185585 0.75185585 0.75185585 0.75185585\n",
      " 0.75185585 0.0782947 ]\n",
      " After 0 training  step(s), loss on training batch is [0.05979397 0.05979397 0.75181884 0.05979397 0.05979397 0.75181884\n",
      " 0.75181884 0.75181884 0.75181884 0.05979397 0.05979397 0.75181884\n",
      " 0.05979397 0.75181884 0.05979397 0.05979397 0.75181884 0.75181884\n",
      " 0.05979397 0.05979397]\n",
      " After 0 training  step(s), loss on training batch is [0.7517733  0.7517733  0.06448109 0.7517733  0.06448109 0.7517733\n",
      " 0.7517733  0.06448109 0.7517733  0.7517733  0.06448109 0.06448109\n",
      " 0.06448109 0.06448109 0.7517733  0.06448109 0.06448109 0.7517733\n",
      " 0.7517733  0.7517733 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7517307  0.06163892 0.7517307  0.06163892 0.7517307  0.7517307\n",
      " 0.06163892 0.7517307  0.06163892 0.06163892 0.7517307  0.06163892\n",
      " 0.06163892 0.06163892 0.7517307  0.7517307  0.7517307  0.06163892\n",
      " 0.06163892 0.7517307 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7516867  0.7516867  0.7516867  0.07528897 0.7516867  0.07528897\n",
      " 0.07528897 0.07528897 0.07528897 0.7516867  0.7516867  0.07528897\n",
      " 0.7516867  0.7516867  0.7516867  0.7516867  0.07528897 0.7516867\n",
      " 0.7516867  0.7516867 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7516468  0.7516468  0.06503984 0.7516468  0.06503984 0.06503984\n",
      " 0.7516468  0.7516468  0.7516468  0.7516468  0.06503984 0.7516468\n",
      " 0.7516468  0.06503984 0.06503984 0.06503984 0.7516468  0.7516468\n",
      " 0.7516468  0.06503984]\n",
      " After 0 training  step(s), loss on training batch is [0.75160414 0.06218945 0.75160414 0.06218945 0.75160414 0.75160414\n",
      " 0.75160414 0.75160414 0.06218945 0.75160414 0.06218945 0.75160414\n",
      " 0.06218945 0.06218945 0.75160414 0.75160414 0.75160414 0.06218945\n",
      " 0.06218945 0.06218945]\n",
      " After 0 training  step(s), loss on training batch is [0.05990542 0.05990542 0.7515604  0.05990542 0.7515604  0.05990542\n",
      " 0.05990542 0.05990542 0.7515604  0.05990542 0.7515604  0.7515604\n",
      " 0.05990542 0.7515604  0.05990542 0.7515604  0.7515604  0.05990542\n",
      " 0.05990542 0.7515604 ]\n",
      " After 0 training  step(s), loss on training batch is [0.75151545 0.09189437 0.75151545 0.09189437 0.75151545 0.75151545\n",
      " 0.75151545 0.75151545 0.75151545 0.75151545 0.09189437 0.09189437\n",
      " 0.75151545 0.75151545 0.09189437 0.09189437 0.75151545 0.75151545\n",
      " 0.75151545 0.75151545]\n",
      " After 0 training  step(s), loss on training batch is [0.05852718 0.05852718 0.75147784 0.05852718 0.05852718 0.05852718\n",
      " 0.05852718 0.75147784 0.75147784 0.75147784 0.75147784 0.05852718\n",
      " 0.05852718 0.05852718 0.05852718 0.05852718 0.75147784 0.05852718\n",
      " 0.05852718 0.05852718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.7514316  0.05861632 0.7514316  0.05861632 0.05861632 0.05861632\n",
      " 0.7514316  0.7514316  0.7514316  0.7514316  0.05861632 0.7514316\n",
      " 0.05861632 0.7514316  0.05861632 0.7514316  0.05861632 0.05861632\n",
      " 0.05861632 0.05861632]\n",
      " After 0 training  step(s), loss on training batch is [0.75138545 0.75138545 0.09840277 0.75138545 0.75138545 0.75138545\n",
      " 0.75138545 0.09840277 0.09840277 0.75138545 0.75138545 0.75138545\n",
      " 0.75138545 0.09840277 0.75138545 0.09840277 0.09840277 0.75138545\n",
      " 0.75138545 0.75138545]\n",
      " After 0 training  step(s), loss on training batch is [0.07552064 0.07552064 0.7513488  0.07552064 0.7513488  0.07552064\n",
      " 0.7513488  0.7513488  0.7513488  0.07552064 0.7513488  0.7513488\n",
      " 0.07552064 0.7513488  0.7513488  0.07552064 0.7513488  0.07552064\n",
      " 0.7513488  0.7513488 ]\n",
      " After 0 training  step(s), loss on training batch is [0.75131035 0.05896212 0.05896212 0.05896212 0.05896212 0.75131035\n",
      " 0.05896212 0.75131035 0.75131035 0.75131035 0.05896212 0.05896212\n",
      " 0.05896212 0.05896212 0.75131035 0.05896212 0.05896212 0.75131035\n",
      " 0.75131035 0.05896212]\n",
      " After 0 training  step(s), loss on training batch is [0.05957131 0.05957131 0.751265   0.751265   0.05957131 0.751265\n",
      " 0.05957131 0.751265   0.05957131 0.751265   0.751265   0.05957131\n",
      " 0.05957131 0.751265   0.751265   0.751265   0.05957131 0.751265\n",
      " 0.751265   0.05957131]\n",
      " After 0 training  step(s), loss on training batch is [0.75121987 0.75121987 0.75121987 0.06888546 0.75121987 0.75121987\n",
      " 0.06888546 0.06888546 0.06888546 0.06888546 0.06888546 0.06888546\n",
      " 0.75121987 0.75121987 0.75121987 0.06888546 0.06888546 0.75121987\n",
      " 0.75121987 0.06888546]\n",
      " After 0 training  step(s), loss on training batch is [0.05937858 0.05937858 0.75118077 0.75118077 0.75118077 0.05937858\n",
      " 0.05937858 0.05937858 0.05937858 0.05937858 0.05937858 0.05937858\n",
      " 0.75118077 0.05937858 0.75118077 0.75118077 0.05937858 0.75118077\n",
      " 0.05937858 0.75118077]\n",
      " After 0 training  step(s), loss on training batch is [0.05852801 0.05852801 0.05852801 0.05852801 0.05852801 0.05852801\n",
      " 0.05852801 0.05852801 0.7511362  0.7511362  0.05852801 0.05852801\n",
      " 0.05852801 0.7511362  0.7511362  0.05852801 0.7511362  0.05852801\n",
      " 0.05852801 0.7511362 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06448814 0.75109076 0.06448814 0.06448814 0.75109076 0.06448814\n",
      " 0.06448814 0.75109076 0.06448814 0.75109076 0.75109076 0.06448814\n",
      " 0.06448814 0.06448814 0.75109076 0.06448814 0.75109076 0.75109076\n",
      " 0.06448814 0.75109076]\n",
      " After 0 training  step(s), loss on training batch is [0.75105    0.75105    0.75105    0.07355057 0.75105    0.75105\n",
      " 0.75105    0.07355057 0.75105    0.07355057 0.75105    0.07355057\n",
      " 0.75105    0.75105    0.07355057 0.07355057 0.07355057 0.75105\n",
      " 0.75105    0.07355057]\n",
      " After 0 training  step(s), loss on training batch is [0.05847353 0.05847353 0.05847353 0.75101125 0.05847353 0.05847353\n",
      " 0.75101125 0.75101125 0.75101125 0.05847353 0.75101125 0.75101125\n",
      " 0.05847353 0.05847353 0.75101125 0.05847353 0.05847353 0.05847353\n",
      " 0.05847353 0.75101125]\n",
      " After 0 training  step(s), loss on training batch is [0.7509659  0.05969379 0.05969379 0.05969379 0.7509659  0.7509659\n",
      " 0.05969379 0.7509659  0.7509659  0.7509659  0.05969379 0.05969379\n",
      " 0.7509659  0.05969379 0.7509659  0.7509659  0.05969379 0.7509659\n",
      " 0.05969379 0.05969379]\n",
      " After 0 training  step(s), loss on training batch is [0.7509216  0.06911708 0.06911708 0.7509216  0.7509216  0.7509216\n",
      " 0.06911708 0.06911708 0.06911708 0.7509216  0.06911708 0.7509216\n",
      " 0.06911708 0.06911708 0.7509216  0.06911708 0.7509216  0.7509216\n",
      " 0.06911708 0.7509216 ]\n",
      " After 0 training  step(s), loss on training batch is [0.05874259 0.05874259 0.750883   0.750883   0.05874259 0.05874259\n",
      " 0.750883   0.750883   0.05874259 0.05874259 0.05874259 0.05874259\n",
      " 0.05874259 0.750883   0.750883   0.750883   0.05874259 0.750883\n",
      " 0.05874259 0.750883  ]\n",
      " After 0 training  step(s), loss on training batch is [0.05836013 0.05836013 0.75083804 0.75083804 0.05836013 0.05836013\n",
      " 0.05836013 0.75083804 0.05836013 0.05836013 0.75083804 0.75083804\n",
      " 0.05836013 0.75083804 0.05836013 0.05836013 0.05836013 0.75083804\n",
      " 0.05836013 0.75083804]\n",
      " After 0 training  step(s), loss on training batch is [0.7507929  0.7507929  0.7507929  0.7507929  0.05828627 0.05828627\n",
      " 0.05828627 0.7507929  0.7507929  0.05828627 0.05828627 0.05828627\n",
      " 0.7507929  0.05828627 0.05828627 0.7507929  0.05828627 0.05828627\n",
      " 0.7507929  0.7507929 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7507476  0.06007797 0.7507476  0.7507476  0.06007797 0.7507476\n",
      " 0.06007797 0.7507476  0.7507476  0.06007797 0.7507476  0.06007797\n",
      " 0.06007797 0.7507476  0.06007797 0.7507476  0.7507476  0.06007797\n",
      " 0.7507476  0.7507476 ]\n",
      " After 0 training  step(s), loss on training batch is [0.75070345 0.05911051 0.75070345 0.75070345 0.75070345 0.75070345\n",
      " 0.05911051 0.05911051 0.75070345 0.05911051 0.75070345 0.05911051\n",
      " 0.75070345 0.05911051 0.05911051 0.75070345 0.05911051 0.05911051\n",
      " 0.75070345 0.05911051]\n",
      " After 0 training  step(s), loss on training batch is [0.7506591  0.7506591  0.7506591  0.07730703 0.7506591  0.07730703\n",
      " 0.07730703 0.7506591  0.07730703 0.07730703 0.07730703 0.7506591\n",
      " 0.7506591  0.7506591  0.7506591  0.07730703 0.07730703 0.7506591\n",
      " 0.7506591  0.7506591 ]\n",
      " After 0 training  step(s), loss on training batch is [0.05856795 0.75062186 0.75062186 0.75062186 0.05856795 0.75062186\n",
      " 0.05856795 0.05856795 0.75062186 0.75062186 0.05856795 0.75062186\n",
      " 0.05856795 0.05856795 0.75062186 0.05856795 0.05856795 0.75062186\n",
      " 0.05856795 0.05856795]\n",
      " After 0 training  step(s), loss on training batch is [0.7505773  0.7505773  0.7505773  0.7505773  0.06788425 0.7505773\n",
      " 0.06788425 0.06788425 0.06788425 0.06788425 0.7505773  0.7505773\n",
      " 0.06788425 0.7505773  0.7505773  0.7505773  0.7505773  0.7505773\n",
      " 0.7505773  0.06788425]\n",
      " After 0 training  step(s), loss on training batch is [0.7505363  0.7505363  0.7505363  0.7505363  0.7505363  0.7505363\n",
      " 0.7505363  0.7505363  0.7505363  0.16150944 0.16150944 0.7505363\n",
      " 0.16150944 0.16150944 0.7505363  0.16150944 0.7505363  0.7505363\n",
      " 0.7505363  0.7505363 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7505013  0.05950649 0.7505013  0.05950649 0.05950649 0.05950649\n",
      " 0.05950649 0.05950649 0.05950649 0.7505013  0.05950649 0.7505013\n",
      " 0.7505013  0.7505013  0.7505013  0.7505013  0.7505013  0.7505013\n",
      " 0.05950649 0.7505013 ]\n",
      " After 0 training  step(s), loss on training batch is [0.75045735 0.75045735 0.06004811 0.75045735 0.75045735 0.06004811\n",
      " 0.06004811 0.06004811 0.06004811 0.75045735 0.06004811 0.06004811\n",
      " 0.75045735 0.06004811 0.06004811 0.75045735 0.75045735 0.06004811\n",
      " 0.75045735 0.06004811]\n",
      " After 0 training  step(s), loss on training batch is [0.05745672 0.05745672 0.7504144  0.05745672 0.05745672 0.7504144\n",
      " 0.7504144  0.05745672 0.7504144  0.05745672 0.7504144  0.7504144\n",
      " 0.7504144  0.05745672 0.05745672 0.05745672 0.05745672 0.05745672\n",
      " 0.05745672 0.7504144 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7503689  0.7503689  0.7503689  0.05814053 0.05814053 0.05814053\n",
      " 0.7503689  0.7503689  0.05814053 0.05814053 0.05814053 0.7503689\n",
      " 0.7503689  0.05814053 0.7503689  0.7503689  0.05814053 0.7503689\n",
      " 0.05814053 0.05814053]\n",
      " After 0 training  step(s), loss on training batch is [0.7503242  0.7503242  0.7503242  0.7503242  0.7503242  0.06599623\n",
      " 0.06599623 0.7503242  0.06599623 0.06599623 0.7503242  0.06599623\n",
      " 0.06599623 0.7503242  0.7503242  0.7503242  0.06599623 0.06599623\n",
      " 0.7503242  0.06599623]\n",
      " After 0 training  step(s), loss on training batch is [0.05764742 0.75028414 0.75028414 0.05764742 0.05764742 0.05764742\n",
      " 0.05764742 0.05764742 0.05764742 0.05764742 0.75028414 0.05764742\n",
      " 0.05764742 0.05764742 0.75028414 0.05764742 0.75028414 0.05764742\n",
      " 0.05764742 0.75028414]\n",
      " After 0 training  step(s), loss on training batch is [0.7502394  0.7502394  0.7502394  0.7502394  0.05788523 0.7502394\n",
      " 0.7502394  0.7502394  0.05788523 0.05788523 0.7502394  0.7502394\n",
      " 0.05788523 0.05788523 0.05788523 0.05788523 0.05788523 0.05788523\n",
      " 0.05788523 0.05788523]\n",
      " After 0 training  step(s), loss on training batch is [0.7501948  0.05879272 0.7501948  0.7501948  0.05879272 0.05879272\n",
      " 0.05879272 0.05879272 0.7501948  0.7501948  0.05879272 0.05879272\n",
      " 0.7501948  0.7501948  0.05879272 0.05879272 0.7501948  0.05879272\n",
      " 0.7501948  0.05879272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " After 0 training  step(s), loss on training batch is [0.06156818 0.75015116 0.06156818 0.75015116 0.75015116 0.06156818\n",
      " 0.75015116 0.75015116 0.75015116 0.75015116 0.06156818 0.06156818\n",
      " 0.75015116 0.75015116 0.75015116 0.75015116 0.06156818 0.06156818\n",
      " 0.06156818 0.06156818]\n",
      " After 0 training  step(s), loss on training batch is [0.750109   0.05759583 0.750109   0.750109   0.05759583 0.750109\n",
      " 0.05759583 0.750109   0.05759583 0.750109   0.750109   0.05759583\n",
      " 0.05759583 0.05759583 0.05759583 0.05759583 0.05759583 0.05759583\n",
      " 0.750109   0.750109  ]\n",
      " After 0 training  step(s), loss on training batch is [0.05704661 0.05704661 0.05704661 0.05704661 0.05704661 0.7500643\n",
      " 0.05704661 0.05704661 0.7500643  0.7500643  0.7500643  0.7500643\n",
      " 0.05704661 0.7500643  0.05704661 0.7500643  0.05704661 0.05704661\n",
      " 0.05704661 0.7500643 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7500191  0.06624888 0.7500191  0.7500191  0.7500191  0.06624888\n",
      " 0.06624888 0.7500191  0.06624888 0.7500191  0.06624888 0.7500191\n",
      " 0.7500191  0.06624888 0.06624888 0.06624888 0.7500191  0.7500191\n",
      " 0.7500191  0.06624888]\n",
      " After 0 training  step(s), loss on training batch is [0.06407303 0.06407303 0.7499795  0.06407303 0.7499795  0.06407303\n",
      " 0.06407303 0.7499795  0.7499795  0.7499795  0.7499795  0.06407303\n",
      " 0.7499795  0.7499795  0.7499795  0.06407303 0.06407303 0.7499795\n",
      " 0.7499795  0.06407303]\n",
      " After 0 training  step(s), loss on training batch is [0.05717626 0.74993896 0.05717626 0.74993896 0.05717626 0.05717626\n",
      " 0.05717626 0.05717626 0.05717626 0.05717626 0.74993896 0.05717626\n",
      " 0.74993896 0.05717626 0.05717626 0.05717626 0.74993896 0.05717626\n",
      " 0.05717626 0.74993896]\n",
      " After 0 training  step(s), loss on training batch is [0.06246549 0.06246549 0.06246549 0.06246549 0.74989426 0.74989426\n",
      " 0.06246549 0.74989426 0.74989426 0.06246549 0.74989426 0.74989426\n",
      " 0.06246549 0.74989426 0.74989426 0.74989426 0.74989426 0.06246549\n",
      " 0.74989426 0.74989426]\n",
      " After 0 training  step(s), loss on training batch is [0.05802115 0.05802115 0.05802115 0.74985254 0.05802115 0.74985254\n",
      " 0.74985254 0.74985254 0.05802115 0.05802115 0.05802115 0.05802115\n",
      " 0.74985254 0.05802115 0.74985254 0.05802115 0.74985254 0.05802115\n",
      " 0.74985254 0.74985254]\n",
      " After 0 training  step(s), loss on training batch is [0.7498088  0.7498088  0.05891674 0.7498088  0.05891674 0.7498088\n",
      " 0.7498088  0.7498088  0.05891674 0.05891674 0.05891674 0.7498088\n",
      " 0.7498088  0.05891674 0.7498088  0.7498088  0.05891674 0.7498088\n",
      " 0.05891674 0.7498088 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7497652  0.7497652  0.05689207 0.7497652  0.05689207 0.05689207\n",
      " 0.05689207 0.05689207 0.7497652  0.05689207 0.05689207 0.7497652\n",
      " 0.7497652  0.05689207 0.05689207 0.05689207 0.05689207 0.05689207\n",
      " 0.05689207 0.05689207]\n",
      " After 0 training  step(s), loss on training batch is [0.05702274 0.74972045 0.05702274 0.05702274 0.05702274 0.05702274\n",
      " 0.05702274 0.05702274 0.05702274 0.05702274 0.05702274 0.05702274\n",
      " 0.74972045 0.05702274 0.74972045 0.05702274 0.74972045 0.74972045\n",
      " 0.05702274 0.05702274]\n",
      " After 0 training  step(s), loss on training batch is [0.7496761  0.06972486 0.06972486 0.06972486 0.7496761  0.06972486\n",
      " 0.7496761  0.7496761  0.7496761  0.7496761  0.06972486 0.06972486\n",
      " 0.7496761  0.7496761  0.7496761  0.06972486 0.7496761  0.06972486\n",
      " 0.06972486 0.7496761 ]\n",
      " After 0 training  step(s), loss on training batch is [0.05690528 0.05690528 0.74963844 0.05690528 0.05690528 0.74963844\n",
      " 0.05690528 0.05690528 0.74963844 0.05690528 0.74963844 0.05690528\n",
      " 0.74963844 0.74963844 0.05690528 0.05690528 0.05690528 0.74963844\n",
      " 0.05690528 0.74963844]\n",
      " After 0 training  step(s), loss on training batch is [0.7495939  0.05855782 0.05855782 0.7495939  0.05855782 0.05855782\n",
      " 0.05855782 0.05855782 0.7495939  0.7495939  0.7495939  0.05855782\n",
      " 0.05855782 0.7495939  0.7495939  0.05855782 0.05855782 0.05855782\n",
      " 0.7495939  0.7495939 ]\n",
      " After 0 training  step(s), loss on training batch is [0.74955106 0.05918332 0.74955106 0.05918332 0.74955106 0.74955106\n",
      " 0.74955106 0.05918332 0.05918332 0.05918332 0.74955106 0.74955106\n",
      " 0.05918332 0.74955106 0.05918332 0.74955106 0.05918332 0.05918332\n",
      " 0.74955106 0.74955106]\n",
      " After 0 training  step(s), loss on training batch is [0.05719561 0.7495083  0.05719561 0.7495083  0.05719561 0.7495083\n",
      " 0.05719561 0.7495083  0.05719561 0.7495083  0.05719561 0.7495083\n",
      " 0.7495083  0.7495083  0.05719561 0.05719561 0.05719562 0.05719562\n",
      " 0.7495083  0.7495083 ]\n",
      " After 0 training  step(s), loss on training batch is [0.05779725 0.05779725 0.7494642  0.7494642  0.05779725 0.05779725\n",
      " 0.05779725 0.7494642  0.7494642  0.7494642  0.05779725 0.7494642\n",
      " 0.05779725 0.7494642  0.7494642  0.7494642  0.7494642  0.05779725\n",
      " 0.05779725 0.7494642 ]\n",
      " After 0 training  step(s), loss on training batch is [0.06022385 0.74942064 0.74942064 0.74942064 0.06022385 0.06022385\n",
      " 0.06022385 0.74942064 0.74942064 0.74942064 0.06022385 0.06022385\n",
      " 0.74942064 0.06022385 0.74942064 0.06022385 0.74942064 0.74942064\n",
      " 0.06022385 0.74942064]\n",
      " After 0 training  step(s), loss on training batch is [0.74937874 0.74937874 0.05875047 0.74937874 0.74937874 0.05875047\n",
      " 0.74937874 0.05875047 0.05875047 0.05875047 0.05875047 0.05875047\n",
      " 0.05875047 0.05875047 0.74937874 0.74937874 0.05875047 0.05875047\n",
      " 0.05875047 0.74937874]\n",
      " After 0 training  step(s), loss on training batch is [0.74933666 0.74933666 0.74933666 0.05729846 0.05729846 0.05729846\n",
      " 0.74933666 0.05729846 0.05729846 0.05729846 0.05729846 0.05729846\n",
      " 0.05729846 0.74933666 0.05729846 0.74933666 0.05729846 0.74933666\n",
      " 0.74933666 0.74933666]\n",
      " After 0 training  step(s), loss on training batch is [0.06415869 0.06415869 0.7492931  0.06415869 0.7492931  0.7492931\n",
      " 0.06415869 0.06415869 0.7492931  0.7492931  0.7492931  0.7492931\n",
      " 0.7492931  0.7492931  0.7492931  0.06415869 0.06415869 0.7492931\n",
      " 0.7492931  0.06415869]\n",
      " After 0 training  step(s), loss on training batch is [0.7492529  0.7492529  0.7492529  0.06232633 0.7492529  0.7492529\n",
      " 0.06232633 0.06232633 0.06232633 0.7492529  0.7492529  0.06232633\n",
      " 0.7492529  0.06232633 0.7492529  0.7492529  0.7492529  0.06232633\n",
      " 0.06232633 0.7492529 ]\n",
      " After 0 training  step(s), loss on training batch is [0.7492119  0.05720729 0.7492119  0.05720729 0.05720729 0.7492119\n",
      " 0.05720729 0.7492119  0.7492119  0.7492119  0.7492119  0.05720729\n",
      " 0.7492119  0.05720729 0.05720729 0.7492119  0.7492119  0.7492119\n",
      " 0.05720729 0.7492119 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-163ab8351b0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" After {step} training  step(s), loss on training batch is {predict_loss}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_SAVE_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_SAVE_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data0/sina_recmd_fe/.pyenv/versions/anaconda2-2019.07/lib/python2.7/site-packages/tensorflow/python/summary/writer/writer.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logdir, graph, max_queue, flush_secs, graph_def, filename_suffix, session)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data0/sina_recmd_fe/.pyenv/versions/anaconda2-2019.07/lib/python2.7/site-packages/tensorflow/python/summary/writer/writer.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, event_writer, graph, graph_def)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgraph_def\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;31m# Calling it with both graph and graph_def for backward compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m       \u001b[0;31m# Also export the meta_graph_def in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m       \u001b[0;31m# graph may itself be a graph_def due to positional arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data0/sina_recmd_fe/.pyenv/versions/anaconda2-2019.07/lib/python2.7/site-packages/tensorflow/python/summary/writer/writer.pyc\u001b[0m in \u001b[0;36madd_graph\u001b[0;34m(self, graph, global_step, graph_def)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;31m# Serialize the graph with additional info.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m       \u001b[0mtrue_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_plugin_assets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     elif (isinstance(graph, graph_pb2.GraphDef) or\n",
      "\u001b[0;32m/data0/sina_recmd_fe/.pyenv/versions/anaconda2-2019.07/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mas_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   3148\u001b[0m     \"\"\"\n\u001b[1;32m   3149\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3150\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3151\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data0/sina_recmd_fe/.pyenv/versions/anaconda2-2019.07/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_as_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   3106\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3107\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3108\u001b[0;31m         \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GraphToGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3109\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m       \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    global_step=tf.Variable(0,trainable=False)\n",
    "    trainx,trainy,train_field=gen_data()\n",
    "    \n",
    "    # \n",
    "    input_x=tf.placeholder(tf.float32,[input_x_size])\n",
    "    input_y=tf.placeholder(tf.float32)\n",
    "    \n",
    "    #\n",
    "    lambda_w=tf.constant(0.001,name='lambda_w')\n",
    "    lambda_v=tf.constant(0.001,name='lambda_v')\n",
    "    \n",
    "    zeroWeights=createZeroDimensionWeight()\n",
    "    oneDimWeights=createOneDimensionWeight(input_x_size)  # 创建二次项的权重变量\n",
    "    thirdWeight=createTwoDimensionWeight(input_x_size,\n",
    "                                        field_size,\n",
    "                                        vector_dimension)  # n * f * k\n",
    "    y_=inference(input_x,train_field,zeroWeights,oneDimWeights,thirdWeight)\n",
    "    \n",
    "    l2_norm=tf.reduce_sum(\n",
    "        tf.add(\n",
    "            tf.multiply(lambda_w,tf.pow(oneDimWeights,2)),\n",
    "            tf.reduce_sum(tf.multiply(lambda_v,tf.pow(thirdWeight,2)),axis=[1,2])          \n",
    "        )\n",
    "    )\n",
    "    \n",
    "    loss=tf.log(1+tf.exp(-input_y*y_))+l2_norm\n",
    "    \n",
    "    train_step=tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(loss)\n",
    "    \n",
    "    saver=tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(total_plan_train_steps):\n",
    "            for t in range(all_data_size):   \n",
    "                input_x_batch=trainx[t]\n",
    "                input_y_batch=trainy[t]\n",
    "                predict_loss,_,steps=sess.run([loss,train_step,global_step],\n",
    "                                             feed_dict={input_x:input_x_batch,input_y:input_x_batch})\n",
    "\n",
    "                print(\" After {step} training  step(s), loss on training batch is {predict_loss}\".format(step=steps,predict_loss=predict_loss))\n",
    "                saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step=steps)\n",
    "                writer=tf.summary.FileWriter(os.path.join(MODEL_SAVE_PATH,MODEL_NAME),tf.get_default_graph())\n",
    "                writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 0], [2, 1], [2, 2], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9]]\n"
     ]
    }
   ],
   "source": [
    "temp=[[2,m] for m in range(10)]\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 3, 1)\n",
      "(2, 3)\n",
      "Tensor(\"Squeeze_2:0\", shape=(2, 3), dtype=float32)\n",
      "[[1.  1.1 1.5]\n",
      " [3.1 3.2 3.3]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "a=tf.constant([[[[1],[1.1],[1.5]]],[[[3.1],[3.2],[3.3]]]])\n",
    "print(a.shape)\n",
    "b=tf.squeeze(a)\n",
    "print(b.shape)\n",
    "print(b)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, -1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, -1, 1, 1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, -1, -1, 1, -1, 1, -1, 1, 1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, -1, 1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, -1, 1, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, -1, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, -1, 1, -1, 1, 1, -1, 1, -1, -1, -1, 1, -1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "labels=[-1,1]\n",
    "y=[np.random.choice(labels,1)[0] for _ in range(all_data_size)]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
